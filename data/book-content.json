{
  "capstone-project\\autonomous-humanoid": {
    "title": "Autonomous Humanoid Project",
    "content": "\r\n# Autonomous Humanoid\r\n",
    "path": "capstone-project\\autonomous-humanoid.md",
    "description": ""
  },
  "capstone-project\\index": {
    "title": "Capstone Overview",
    "content": "\r\n# Capstone Project\r\n",
    "path": "capstone-project\\index.md",
    "description": ""
  },
  "intro": {
    "title": "Introduction",
    "content": "\n# Introduction to Physical AI & Humanoid Robotics\n\nWelcome to the Physical AI & Humanoid Robotics book! This comprehensive educational resource is designed to bridge the gap between digital intelligence and physical robotic bodies, teaching you how to create embodied AI systems.\n\n## What is Physical AI?\n\nPhysical AI represents the convergence of artificial intelligence and physical systems. Rather than AI existing purely in digital form, Physical AI brings intelligence into the physical world through robotic bodies. This creates a feedback loop where AI systems can perceive, interact with, and learn from the physical environment.\n\n## The Learning Pathway\n\nThis book follows a structured learning pathway that takes you from foundational concepts to advanced autonomous behaviors:\n\n1. **Robotic Middleware** - Understanding ROS 2 and communication patterns\n2. **Simulation & Digital Twins** - Creating virtual environments for testing\n3. **AI Perception & Navigation** - Enabling robots to understand their environment\n4. **Vision-Language-Action** - Natural interaction with robotic systems\n5. **Capstone Integration** - Bringing all concepts together\n\n## Target Audience\n\nThis book is designed for students with a computer science or AI background who want to:\n- Understand the fundamentals of humanoid robotics\n- Learn to integrate AI systems with physical robots\n- Develop skills in ROS 2, simulation environments, and AI frameworks\n- Apply embodied intelligence concepts in practical scenarios\n\n## Prerequisites\n\nBefore diving into this book, you should have:\n- Basic programming experience (Python preferred)\n- Understanding of fundamental AI concepts\n- Familiarity with Linux command line\n- Basic knowledge of robotics concepts (helpful but not required)\n\n## Technical Requirements\n\nTo follow along with the practical exercises, you'll need:\n- Ubuntu 22.04 LTS (or Windows with WSL2)\n- ROS 2 Humble Hawksbill or later\n- Gazebo Garden or compatible simulation environment\n- NVIDIA Isaac Sim (for AI perception modules)\n- Node.js 18+ for the book environment\n\n## How to Use This Book\n\nEach module builds upon the previous one, but is designed to be independently testable. You can:\n- Follow the modules sequentially for a complete learning experience\n- Focus on specific modules based on your interests\n- Complete the hands-on exercises to reinforce concepts\n- Use the capstone project to integrate all learned concepts\n\nLet's begin your journey into Physical AI and humanoid robotics!",
    "path": "intro.md",
    "description": ""
  },
  "module-1-ros2\\index": {
    "title": "Module 1 ROS-2",
    "content": "\n# Module 1: The Robotic Nervous System (ROS 2)\n\nWelcome to the first module of the Physical AI & Humanoid Robotics book! In this module, you'll learn about ROS 2 (Robot Operating System 2), the foundational middleware that enables communication and coordination in robotic systems.\n\n## Overview\n\nROS 2 serves as the \"nervous system\" of robotic applications, providing the infrastructure for different components to communicate with each other. In this module, we'll cover:\n\n- Core ROS 2 concepts: nodes, topics, services, and actions\n- Python programming with rclpy\n- URDF (Unified Robot Description Format) for robot modeling\n- Practical exercises to build your first ROS 2 applications\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n\n- Understand the architecture and communication patterns of ROS 2\n- Create and run basic ROS 2 nodes in Python\n- Work with topics, services, and actions for inter-component communication\n- Define robot models using URDF\n- Execute practical exercises with simulated humanoid robots\n\n## Prerequisites\n\nBefore starting this module, ensure you have:\n\n- ROS 2 Humble Hawksbill installed\n- Basic Python programming knowledge\n- Familiarity with command-line tools\n- Completed the book's introduction\n\n## Module Structure\n\nThis module is organized into the following sections:\n\n1. **Nodes, Topics, Services** - Understanding ROS 2 communication primitives\n2. **rclpy Basics** - Python client library for ROS 2\n3. **URDF Description** - Robot modeling and description format\n4. **Practical Exercises** - Hands-on applications with humanoid robots\n\n## Next Steps\n\nBegin with the next section to dive into ROS 2 communication patterns. Each section builds on the previous one, so follow the sequence for the best learning experience.",
    "path": "module-1-ros2\\index.md",
    "description": ""
  },
  "module-1-ros2\\installation-setup": {
    "title": "module-1-ros2\\installation-setup",
    "content": "# ROS 2 Installation and Setup\n\nThis section provides comprehensive instructions for installing and setting up ROS 2 (Humble Hawksbill) for use with humanoid robotics applications. Following these steps will prepare your development environment for the exercises in this book.\n\n## System Requirements\n\nBefore installing ROS 2, ensure your system meets the following requirements:\n\n- **Operating System**: Ubuntu 22.04 LTS (Jammy Jellyfish) - Recommended for this book\n- **Processor**: Multi-core processor (Intel i7 or equivalent recommended)\n- **Memory**: 8 GB RAM minimum, 16 GB recommended\n- **Storage**: 20 GB free disk space\n- **Internet**: Required for package installation\n\n> **Note**: While ROS 2 can run on other platforms (Windows with WSL2, macOS), this book focuses on Ubuntu 22.04 for consistency and optimal performance.\n\n## Installation Steps\n\n### 1. Set Locale\n\nEnsure your locale is set to UTF-8:\n\n```bash\nlocale  # Check current locale\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n```\n\n### 2. Add ROS 2 Repository\n\n```bash\nsudo apt update && sudo apt install -y locales\nsudo locale-gen en_US.UTF-8\nsudo apt install software-properties-common\nsudo add-apt-repository universe\nsudo apt update && sudo apt install curl -y\ncurl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\nsudo apt update\n```\n\n### 3. Install ROS 2 Humble\n\nInstall the desktop version which includes Gazebo and other simulation tools:\n\n```bash\nsudo apt install ros-humble-desktop\n```\n\n### 4. Install Additional Dependencies\n\n```bash\nsudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n```\n\n### 5. Initialize rosdep\n\n```bash\nsudo rosdep init\nrosdep update\n```\n\n### 6. Environment Setup\n\nAdd ROS 2 environment setup to your bashrc:\n\n```bash\necho \"source /opt/ros/humble/setup.bash\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\n## Verification\n\nTest that ROS 2 is installed correctly:\n\n```bash\n# Check ROS 2 version\nros2 --version\n\n# Test basic functionality\nros2 topic list\n\n# Run a simple demo\nsource /opt/ros/humble/setup.bash\nros2 run demo_nodes_cpp talker\n```\n\nIn another terminal:\n```bash\nsource /opt/ros/humble/setup.bash\nros2 run demo_nodes_py listener\n```\n\n## Python Package Dependencies\n\nInstall additional Python packages commonly used in robotics:\n\n```bash\npip3 install numpy matplotlib transforms3d pyquaternion\n```\n\n## Workspace Setup\n\nCreate a workspace for your humanoid robotics projects:\n\n```bash\n# Create workspace directory\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws\n\n# Source ROS 2 environment\nsource /opt/ros/humble/setup.bash\n\n# Build the workspace (initially empty)\ncolcon build\n```\n\nAdd workspace to your bashrc:\n\n```bash\necho \"source ~/ros2_ws/install/setup.bash\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\n## Common Issues and Solutions\n\n### Issue: Permission Denied Error\nIf you encounter permission errors, ensure you're sourcing the correct setup file:\n\n```bash\nsource /opt/ros/humble/setup.bash\n```\n\n### Issue: Python Import Errors\nIf Python packages cannot be imported, ensure your environment is properly set:\n\n```bash\n# Check if ROS 2 packages are found\npython3 -c \"import rclpy; print('rclpy imported successfully')\"\n```\n\n### Issue: Gazebo Not Found\nIf Gazebo is not available after installation:\n\n```bash\nsudo apt install ros-humble-gazebo-*\n```\n\n## Docker Alternative (Optional)\n\nFor a containerized development environment, you can use the official ROS 2 Docker image:\n\n```bash\n# Pull the ROS 2 Humble image\ndocker pull osrf/ros:humble-desktop\n\n# Run with GUI support (for visualization tools)\ndocker run -it \\\n  --env=\"DISPLAY\" \\\n  --env=\"QT_X11_NO_MITSHM=1\" \\\n  --volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n  --network=host \\\n  osrf/ros:humble-desktop\n```\n\n## Next Steps\n\nAfter completing the installation:\n\n1. Verify your installation by running the talker/listener demo\n2. Create your first ROS 2 package following the next section\n3. Set up your development environment with your preferred IDE\n4. Proceed to the next chapter to learn about ROS 2 concepts\n\n## Troubleshooting\n\n### If ROS 2 Commands Are Not Found\n\nEnsure your environment is properly sourced:\n\n```bash\n# Check if environment variables are set\necho $ROS_DISTRO\necho $ROS_VERSION\n\n# If not set, source the setup file manually\nsource /opt/ros/humble/setup.bash\n```\n\n### Network Configuration\n\nFor multi-machine ROS 2 communication, you may need to configure RMW (ROS Middleware):\n\n```bash\n# Check current RMW implementation\nprintenv | grep RMW\n\n# Set to Fast DDS (default)\nexport RMW_IMPLEMENTATION=rmw_fastrtps_cpp\n```\n\n## Resources\n\n- [Official ROS 2 Humble Installation Guide](https://docs.ros.org/en/humble/Installation.html)\n- [ROS 2 Tutorials](https://docs.ros.org/en/humble/Tutorials.html)\n- [ROS 2 Concepts](https://docs.ros.org/en/humble/Concepts.html)\n\nYour ROS 2 environment is now ready for developing humanoid robotics applications. In the next sections, we'll explore ROS 2 concepts and create your first robotic nodes.",
    "path": "module-1-ros2\\installation-setup.md",
    "description": ""
  },
  "module-1-ros2\\nodes-topics-services": {
    "title": "module-1-ros2\\nodes-topics-services",
    "content": "# Nodes, Topics, Services\n\nIn this section, we'll explore the fundamental communication patterns in ROS 2: nodes, topics, and services. These concepts form the backbone of robotic applications and enable different components to work together seamlessly.\n\n## Nodes\n\nA node is an executable process that works as part of a ROS 2 system. Nodes are the basic building blocks of a ROS 2 application and perform specific tasks. In a humanoid robot system, you might have nodes for:\n\n- Sensor processing\n- Motor control\n- Path planning\n- Perception\n- Behavior management\n\n### Creating a Node\n\nHere's a basic example of a ROS 2 node in Python:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\n\nclass MinimalPublisher(Node):\n\n    def __init__(self):\n        super().__init__('minimal_publisher')\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.i = 0\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello World: %d' % self.i\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n        self.i += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    minimal_publisher = MinimalPublisher()\n    rclpy.spin(minimal_publisher)\n    minimal_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Topics\n\nTopics enable asynchronous communication between nodes through a publish/subscribe model. Nodes can publish messages to topics and subscribe to topics to receive messages. This pattern is ideal for continuous data streams like sensor readings or motor commands.\n\n### Topic Communication Example\n\n```python\n# Publisher node\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass Talker(Node):\n\n    def __init__(self):\n        super().__init__('talker')\n        self.publisher = self.create_publisher(String, 'chatter', 10)\n        timer_period = 0.5\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello World'\n        self.publisher.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n\n# Subscriber node\nclass Listener(Node):\n\n    def __init__(self):\n        super().__init__('listener')\n        self.subscription = self.create_subscription(\n            String,\n            'chatter',\n            self.listener_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n\n    def listener_callback(self, msg):\n        self.get_logger().info('I heard: \"%s\"' % msg.data)\n```\n\n## Services\n\nServices provide synchronous request/response communication between nodes. Unlike topics, services are used for specific requests that require a response, such as requesting a specific action or configuration change.\n\n### Service Example\n\n```python\n# Service server\nfrom example_interfaces.srv import AddTwoInts\n\nclass AddTwoIntsServer(Node):\n\n    def __init__(self):\n        super().__init__('add_two_ints_server')\n        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\n\n    def add_two_ints_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b))\n        return response\n\n# Service client\nclass AddTwoIntsClient(Node):\n\n    def __init__(self):\n        super().__init__('add_two_ints_client')\n        self.cli = self.create_client(AddTwoInts, 'add_two_ints')\n        while not self.cli.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('service not available, waiting again...')\n        self.req = AddTwoInts.Request()\n\n    def send_request(self, a, b):\n        self.req.a = a\n        self.req.b = b\n        self.future = self.cli.call_async(self.req)\n        rclpy.spin_until_future_complete(self, self.future)\n        return self.future.result()\n```\n\n## Actions\n\nActions are used for long-running tasks that may take time to complete and provide feedback during execution. They're particularly useful for tasks like navigation or manipulation that require ongoing communication about progress.\n\n## Best Practices\n\n- Use topics for continuous data streams (sensor data, motor commands)\n- Use services for specific requests that need immediate responses\n- Use actions for long-running tasks that require feedback\n- Keep message types consistent across your system\n- Use appropriate Quality of Service (QoS) settings for your use case\n\n## Exercise\n\nCreate a simple ROS 2 package with a publisher node that publishes joint positions for a humanoid robot and a subscriber node that logs these positions to the console.",
    "path": "module-1-ros2\\nodes-topics-services.md",
    "description": ""
  },
  "module-1-ros2\\practical-exercises": {
    "title": "module-1-ros2\\practical-exercises",
    "content": "# Practical Exercises - ROS 2 Fundamentals\n\nThis section provides hands-on exercises to reinforce the concepts learned in the ROS 2 module. Complete these exercises to gain practical experience with ROS 2 nodes, topics, services, and URDF.\n\n## Exercise 1: Basic Publisher and Subscriber\n\n### Objective\nCreate a simple publisher that sends messages containing the current joint positions of a humanoid robot and a subscriber that logs these positions.\n\n### Steps\n1. Create a new ROS 2 package called `humanoid_control`\n2. Create a publisher node that publishes joint positions as a `sensor_msgs/JointState` message\n3. Create a subscriber node that receives and logs the joint positions\n4. Use appropriate message types and topic names\n5. Test the communication between nodes\n\n### Code Template\n```python\n# publisher_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nimport math\n\nclass JointStatePublisher(Node):\n    def __init__(self):\n        super().__init__('joint_state_publisher')\n        self.publisher = self.create_publisher(JointState, 'joint_states', 10)\n        self.timer = self.create_timer(0.1, self.publish_joint_state)\n        self.time = 0.0\n\n    def publish_joint_state(self):\n        msg = JointState()\n        msg.name = ['hip_joint', 'knee_joint', 'ankle_joint', 'shoulder_joint', 'elbow_joint']\n        msg.position = [\n            math.sin(self.time),           # hip\n            math.cos(self.time) * 0.5,     # knee\n            math.sin(self.time * 0.5),     # ankle\n            math.cos(self.time * 0.3),     # shoulder\n            math.sin(self.time * 0.7)      # elbow\n        ]\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = 'base_link'\n\n        self.publisher.publish(msg)\n        self.time += 0.1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = JointStatePublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n### Expected Output\nThe subscriber should log joint positions at 10Hz, showing oscillating values for each joint.\n\n## Exercise 2: Service for Robot Control\n\n### Objective\nCreate a service that accepts a target position for a humanoid robot and returns whether the position is reachable.\n\n### Steps\n1. Define a custom service message for position validation\n2. Create a service server that checks if a position is within the robot's reach\n3. Create a client that calls the service with different positions\n4. Test with both reachable and unreachable positions\n\n### Service Definition (`srv/ValidatePosition.srv`)\n```\ngeometry_msgs/Point target_position\n---\nbool is_reachable\nstring reason\n```\n\n## Exercise 3: URDF Robot Model\n\n### Objective\nCreate a complete URDF model of a simple humanoid robot with proper kinematic chains.\n\n### Steps\n1. Create a URDF file defining a humanoid with body, head, arms, and legs\n2. Include proper joint types and limits\n3. Add visual and collision properties\n4. Use Xacro to create macros for similar limbs\n5. Validate the URDF using `check_urdf`\n\n### Template Structure\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"simple_humanoid\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Define properties -->\n  <xacro:property name=\"M_PI\" value=\"3.1415926535897931\" />\n\n  <!-- Base body -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.2 0.5\"/>\n      </geometry>\n      <material name=\"gray\">\n        <color rgba=\"0.5 0.5 0.5 1\"/>\n      </material>\n    </visual>\n    <!-- Add collision and inertial properties -->\n  </link>\n\n  <!-- Use Xacro macros for limbs -->\n  <xacro:macro name=\"arm\" params=\"name parent xyz\">\n    <!-- Define arm structure with joints -->\n  </xacro:macro>\n\n  <!-- Create arms using macros -->\n  <xacro:arm name=\"left\" parent=\"base_link\" xyz=\"0.15 0 0.1\"/>\n  <xacro:arm name=\"right\" parent=\"base_link\" xyz=\"0.15 0 0.1\"/>\n</robot>\n```\n\n## Exercise 4: Robot State Publisher Integration\n\n### Objective\nIntegrate your URDF model with the robot_state_publisher to visualize joint transformations.\n\n### Steps\n1. Create a launch file that starts your joint state publisher and robot_state_publisher\n2. Use the `--ros-args` to pass the URDF to robot_state_publisher\n3. Visualize the robot in RViz2\n4. Verify that joint transformations are properly published\n\n### Launch File Template\n```python\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import Command, LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    use_sim_time = LaunchConfiguration('use_sim_time')\n\n    urdf_file = get_package_share_directory('humanoid_control') + '/urdf/humanoid.urdf'\n\n    robot_state_publisher_node = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'robot_description': Command(['xacro ', urdf_file])\n        }]\n    )\n\n    joint_state_publisher_node = Node(\n        package='humanoid_control',\n        executable='joint_state_publisher',\n        name='joint_state_publisher'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use sim time if true'),\n        robot_state_publisher_node,\n        joint_state_publisher_node\n    ])\n```\n\n## Exercise 5: Complete Control Node\n\n### Objective\nCreate a complete control node that integrates all concepts: publishers, subscribers, services, and parameters.\n\n### Steps\n1. Create a node that subscribes to desired joint positions\n2. Publish current joint states\n3. Provide a service to validate positions\n4. Use parameters to configure control behavior\n5. Add proper logging and error handling\n\n### Template\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom std_srvs.srv import SetBool  # or your custom service\nimport numpy as np\n\nclass HumanoidController(Node):\n    def __init__(self):\n        super().__init__('humanoid_controller')\n\n        # Declare parameters\n        self.declare_parameter('control_frequency', 50)\n        self.declare_parameter('max_velocity', 2.0)\n\n        # Create publishers and subscribers\n        self.joint_state_pub = self.create_publisher(JointState, 'joint_states', 10)\n        self.joint_command_sub = self.create_subscription(\n            JointState, 'joint_commands', self.joint_command_callback, 10)\n\n        # Create services\n        self.validate_srv = self.create_service(\n            ValidatePosition, 'validate_position', self.validate_position_callback)\n\n        # Create timer\n        freq = self.get_parameter('control_frequency').value\n        self.timer = self.create_timer(1.0/freq, self.control_loop)\n\n        # Initialize joint states\n        self.current_joints = JointState()\n\n    def joint_command_callback(self, msg):\n        # Process joint commands\n        pass\n\n    def validate_position_callback(self, request, response):\n        # Validate if position is reachable\n        pass\n\n    def control_loop(self):\n        # Main control loop\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = HumanoidController()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n```\n\n## Validation and Testing\n\nTo verify your implementations:\n\n1. **Check URDF validity**:\n   ```bash\n   check_urdf path/to/your/robot.urdf\n   ```\n\n2. **List ROS 2 topics**:\n   ```bash\n   ros2 topic list\n   ```\n\n3. **Echo topics**:\n   ```bash\n   ros2 topic echo /joint_states\n   ```\n\n4. **Call services**:\n   ```bash\n   ros2 service call /validate_position your_robot_msgs/srv/ValidatePosition \"{target_position: {x: 1.0, y: 0.0, z: 0.0}}\"\n   ```\n\n## Troubleshooting Tips\n\n- Ensure all packages are properly sourced: `source /opt/ros/humble/setup.bash`\n- Check that your package has the correct dependencies in `package.xml`\n- Verify that your CMakeLists.txt includes all necessary libraries\n- Use `rqt_graph` to visualize the ROS graph and verify connections\n- Check that joint names in your messages match those in your URDF\n\n## Extension Challenges\n\n1. Add a simple PID controller to your joint control node\n2. Implement a basic trajectory execution interface\n3. Create a simple GUI using rqt to control the robot\n4. Add collision checking between robot links\n5. Implement forward and inverse kinematics for simple movements\n\nComplete these exercises to solidify your understanding of ROS 2 fundamentals before moving to the next module.",
    "path": "module-1-ros2\\practical-exercises.md",
    "description": ""
  },
  "module-1-ros2\\rclpy-basics": {
    "title": "module-1-ros2\\rclpy-basics",
    "content": "# rclpy Basics\n\nrclpy is the Python client library for ROS 2. It provides the Python API for developing ROS 2 applications, allowing you to create nodes, publish/subscribe to topics, provide/call services, and interact with other ROS 2 concepts.\n\n## Installation and Setup\n\nrclpy is typically installed as part of the ROS 2 Python development packages:\n\n```bash\n# If you installed ROS 2 from packages, rclpy should already be available\n# Otherwise, install the Python development packages:\nsudo apt install python3-ros-dev-tools\n```\n\n## Basic Node Structure\n\nEvery ROS 2 Python node follows a similar structure:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\n\nclass MyNode(Node):\n    def __init__(self):\n        super().__init__('node_name')\n        # Node initialization code here\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MyNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Creating Publishers and Subscribers\n\n### Publisher\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass PublisherNode(Node):\n    def __init__(self):\n        super().__init__('publisher_node')\n\n        # Create a publisher\n        self.publisher = self.create_publisher(String, 'topic_name', 10)\n\n        # Create a timer to periodically publish messages\n        timer_period = 1.0  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello from publisher'\n        self.publisher.publish(msg)\n        self.get_logger().info(f'Publishing: {msg.data}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PublisherNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n### Subscriber\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass SubscriberNode(Node):\n    def __init__(self):\n        super().__init__('subscriber_node')\n\n        # Create a subscriber\n        self.subscription = self.create_subscription(\n            String,\n            'topic_name',\n            self.listener_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n\n    def listener_callback(self, msg):\n        self.get_logger().info(f'I heard: {msg.data}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SubscriberNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Creating Services and Clients\n\n### Service Server\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import AddTwoInts\n\nclass ServiceServer(Node):\n    def __init__(self):\n        super().__init__('service_server')\n\n        # Create a service\n        self.service = self.create_service(\n            AddTwoInts,\n            'add_two_ints',\n            self.add_two_ints_callback)\n\n    def add_two_ints_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info(f'Request received: {request.a} + {request.b} = {response.sum}')\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ServiceServer()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Parameters\n\nROS 2 nodes can use parameters for configuration:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\n\nclass ParameterNode(Node):\n    def __init__(self):\n        super().__init__('parameter_node')\n\n        # Declare parameters with default values\n        self.declare_parameter('robot_name', 'humanoid_robot')\n        self.declare_parameter('max_velocity', 1.0)\n\n        # Get parameter values\n        self.robot_name = self.get_parameter('robot_name').value\n        self.max_velocity = self.get_parameter('max_velocity').value\n\n        self.get_logger().info(f'Robot: {self.robot_name}, Max Velocity: {self.max_velocity}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ParameterNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Quality of Service (QoS)\n\nQoS profiles control the behavior of publishers and subscribers:\n\n```python\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy\n\n# Create a QoS profile for reliable communication\nqos_profile = QoSProfile(\n    depth=10,\n    reliability=ReliabilityPolicy.RELIABLE,\n    durability=DurabilityPolicy.VOLATILE\n)\n\n# Use the QoS profile when creating a publisher\npublisher = self.create_publisher(String, 'topic_name', qos_profile)\n```\n\n## Working with Custom Messages\n\nTo use custom message types, you need to create them in a package and import them:\n\n```python\n# Assuming you have a custom message package called 'my_robot_msgs'\nfrom my_robot_msgs.msg import JointState\n\nclass CustomMessageNode(Node):\n    def __init__(self):\n        super().__init__('custom_message_node')\n        self.publisher = self.create_publisher(JointState, 'joint_states', 10)\n\n    def publish_joint_state(self, positions):\n        msg = JointState()\n        msg.position = positions\n        self.publisher.publish(msg)\n```\n\n## Error Handling and Logging\n\nROS 2 provides different logging levels:\n\n```python\nclass LoggingNode(Node):\n    def __init__(self):\n        super().__init__('logging_node')\n\n        # Different logging levels\n        self.get_logger().debug('Debug message')\n        self.get_logger().info('Info message')\n        self.get_logger().warn('Warning message')\n        self.get_logger().error('Error message')\n        self.get_logger().fatal('Fatal message')\n```\n\n## Exercise\n\nCreate a simple rclpy node that:\n1. Publishes joint position commands to control a humanoid robot\n2. Subscribes to sensor feedback from the robot\n3. Uses parameters to configure the control behavior\n4. Implements proper error handling and logging",
    "path": "module-1-ros2\\rclpy-basics.md",
    "description": ""
  },
  "module-1-ros2\\urdf-description": {
    "title": "module-1-ros2\\urdf-description",
    "content": "# URDF Description\n\nURDF (Unified Robot Description Format) is an XML format used to describe robot models in ROS. It defines the physical and visual properties of a robot, including its links, joints, and other components. URDF is essential for simulating robots in Gazebo and visualizing them in RViz.\n\n## URDF Basics\n\nA URDF file describes a robot as a collection of rigid bodies (links) connected by joints. The structure typically includes:\n\n- **Links**: Rigid bodies that make up the robot structure\n- **Joints**: Connections between links with specific degrees of freedom\n- **Visual**: How the link appears in simulation and visualization\n- **Collision**: Collision properties for physics simulation\n- **Inertial**: Mass, center of mass, and inertia properties\n\n## Basic URDF Structure\n\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"simple_robot\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Base link -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.6\" radius=\"0.2\"/>\n      </geometry>\n      <material name=\"blue\">\n        <color rgba=\"0 0 0.8 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.6\" radius=\"0.2\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"10\"/>\n      <inertia ixx=\"1.0\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"1.0\" iyz=\"0.0\" izz=\"1.0\"/>\n    </inertial>\n  </link>\n\n  <!-- Child link connected via joint -->\n  <joint name=\"base_to_top\" type=\"fixed\">\n    <parent link=\"base_link\"/>\n    <child link=\"top_link\"/>\n    <origin xyz=\"0 0 0.3\"/>\n  </joint>\n\n  <link name=\"top_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.1 0.1 0.1\"/>\n      </geometry>\n      <material name=\"red\">\n        <color rgba=\"0.8 0 0 1\"/>\n      </material>\n    </visual>\n  </link>\n</robot>\n```\n\n## Links\n\nLinks represent rigid bodies in the robot. Each link can have:\n\n- **Visual**: How the link looks in visualization\n- **Collision**: How the link interacts in physics simulation\n- **Inertial**: Physical properties for dynamics simulation\n\n### Visual Properties\n\n```xml\n<link name=\"link_name\">\n  <visual>\n    <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n    <geometry>\n      <!-- Choose one geometry type -->\n      <box size=\"0.1 0.1 0.1\"/>\n      <!-- <cylinder radius=\"0.1\" length=\"0.1\"/> -->\n      <!-- <sphere radius=\"0.1\"/> -->\n      <!-- <mesh filename=\"package://path/to/mesh.stl\"/> -->\n    </geometry>\n    <material name=\"material_name\">\n      <color rgba=\"0.8 0.2 0.2 1.0\"/>\n    </material>\n  </visual>\n</link>\n```\n\n### Collision Properties\n\n```xml\n<collision>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n  <geometry>\n    <box size=\"0.1 0.1 0.1\"/>\n  </geometry>\n</collision>\n```\n\n### Inertial Properties\n\n```xml\n<inertial>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n  <mass value=\"0.1\"/>\n  <inertia ixx=\"0.01\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.01\" iyz=\"0.0\" izz=\"0.01\"/>\n</inertial>\n```\n\n## Joints\n\nJoints connect links and define how they can move relative to each other:\n\n```xml\n<joint name=\"joint_name\" type=\"joint_type\">\n  <parent link=\"parent_link_name\"/>\n  <child link=\"child_link_name\"/>\n  <origin xyz=\"0 0 0\" rpy=\"0 0 0\"/>\n\n  <!-- For revolute joints -->\n  <axis xyz=\"0 0 1\"/>\n  <limit lower=\"-3.14\" upper=\"3.14\" effort=\"100\" velocity=\"1\"/>\n\n  <!-- For continuous joints (like wheels) -->\n  <!-- <mimic joint=\"other_joint\" multiplier=\"1.0\" offset=\"0.0\"/> -->\n</joint>\n```\n\n### Joint Types\n\n- **fixed**: No movement allowed\n- **revolute**: Rotational movement around an axis (limited)\n- **continuous**: Rotational movement around an axis (unlimited)\n- **prismatic**: Linear movement along an axis (limited)\n- **floating**: 6 DOF movement (rarely used)\n- **planar**: Movement in a plane (rarely used)\n\n## Complete Humanoid Robot Example\n\nHere's a simplified example of a humanoid robot with basic body parts:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"simple_humanoid\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Base body -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.2 0.5\"/>\n      </geometry>\n      <material name=\"gray\">\n        <color rgba=\"0.5 0.5 0.5 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.3 0.2 0.5\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"5.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.1\" iyz=\"0.0\" izz=\"0.1\"/>\n    </inertial>\n  </link>\n\n  <!-- Head -->\n  <joint name=\"neck_joint\" type=\"revolute\">\n    <parent link=\"base_link\"/>\n    <child link=\"head_link\"/>\n    <origin xyz=\"0 0 0.35\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"2\"/>\n  </joint>\n\n  <link name=\"head_link\">\n    <visual>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n      <material name=\"skin\">\n        <color rgba=\"0.8 0.6 0.4 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.001\" iyz=\"0.0\" izz=\"0.001\"/>\n    </inertial>\n  </link>\n\n  <!-- Left Arm -->\n  <joint name=\"left_shoulder_joint\" type=\"revolute\">\n    <parent link=\"base_link\"/>\n    <child link=\"left_upper_arm\"/>\n    <origin xyz=\"0.2 0 0.1\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"2\"/>\n  </joint>\n\n  <link name=\"left_upper_arm\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.3\" radius=\"0.05\"/>\n      </geometry>\n      <material name=\"gray\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.3\" radius=\"0.05\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.0005\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"left_elbow_joint\" type=\"revolute\">\n    <parent link=\"left_upper_arm\"/>\n    <child link=\"left_lower_arm\"/>\n    <origin xyz=\"0 0 -0.3\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"2\"/>\n  </joint>\n\n  <link name=\"left_lower_arm\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.3\" radius=\"0.05\"/>\n      </geometry>\n      <material name=\"gray\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.3\" radius=\"0.05\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.3\"/>\n      <inertia ixx=\"0.003\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.003\" iyz=\"0.0\" izz=\"0.0003\"/>\n    </inertial>\n  </link>\n</robot>\n```\n\n## Xacro for Complex Models\n\nXacro is an XML macro language that makes URDF more manageable for complex robots:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"humanoid_with_xacro\">\n  <!-- Define properties -->\n  <xacro:property name=\"M_PI\" value=\"3.1415926535897931\" />\n  <xacro:property name=\"body_width\" value=\"0.3\" />\n  <xacro:property name=\"body_depth\" value=\"0.2\" />\n  <xacro:property name=\"body_height\" value=\"0.5\" />\n\n  <!-- Macro for creating limbs -->\n  <xacro:macro name=\"simple_arm\" params=\"name parent *origin\">\n    <joint name=\"${name}_shoulder_joint\" type=\"revolute\">\n      <xacro:insert_block name=\"origin\"/>\n      <parent link=\"${parent}\"/>\n      <child link=\"${name}_upper_arm\"/>\n      <axis xyz=\"0 1 0\"/>\n      <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"2\"/>\n    </joint>\n\n    <link name=\"${name}_upper_arm\">\n      <visual>\n        <geometry>\n          <cylinder length=\"0.3\" radius=\"0.05\"/>\n        </geometry>\n        <material name=\"gray\">\n          <color rgba=\"0.5 0.5 0.5 1\"/>\n        </material>\n      </visual>\n    </link>\n  </xacro:macro>\n\n  <!-- Use the macro -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"${body_width} ${body_depth} ${body_height}\"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <xacro:simple_arm name=\"left\" parent=\"base_link\">\n    <origin xyz=\"${body_width/2} 0 ${body_height/4}\" rpy=\"0 0 0\"/>\n  </xacro:simple_arm>\n</robot>\n```\n\n## Working with URDF in ROS 2\n\nTo use URDF in ROS 2 applications:\n\n1. **Robot State Publisher**: Publishes the robot's joint states and transforms\n2. **TF2**: Handles coordinate transformations between links\n3. **Gazebo Integration**: Use the robot in simulation\n\n### Launching with Robot State Publisher\n\n```python\nfrom launch import LaunchDescription\nfrom launch.substitutions import Command\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    pkg_share = get_package_share_directory('your_robot_description')\n    urdf_file = os.path.join(pkg_share, 'urdf', 'your_robot.urdf')\n\n    robot_state_publisher_node = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{'robot_description': Command(['xacro ', urdf_file])}]\n    )\n\n    return LaunchDescription([robot_state_publisher_node])\n```\n\n## Best Practices\n\n- Use consistent naming conventions for links and joints\n- Keep visual and collision geometries as simple as possible for performance\n- Use Xacro for complex robots to reduce redundancy\n- Include proper inertial properties for accurate simulation\n- Validate URDF files using tools like `check_urdf`\n- Use appropriate joint limits based on physical constraints\n\n## Exercise\n\nCreate a URDF file for a simple humanoid robot with:\n1. A body, head, and 4 limbs (arms and legs)\n2. Proper joint connections with realistic movement ranges\n3. Visual and collision properties for each link\n4. Use Xacro macros to avoid redundancy in similar limbs",
    "path": "module-1-ros2\\urdf-description.md",
    "description": ""
  },
  "module-2-digital-twin\\gazebo-simulation": {
    "title": "module-2-digital-twin\\gazebo-simulation",
    "content": "# Gazebo Simulation\n\nGazebo is the standard simulation environment for ROS and provides a powerful physics engine for realistic robot simulation. This section covers setting up Gazebo, creating simulation worlds, and integrating with ROS 2.\n\n## Introduction to Gazebo\n\nGazebo provides:\n- Realistic physics simulation using ODE, Bullet, or Simbody\n- High-quality 3D graphics rendering\n- Sensor simulation (cameras, LIDAR, IMU, etc.)\n- Multiple robot support\n- Plugin architecture for custom functionality\n\n## Installation\n\nIf not already installed during ROS 2 setup:\n\n```bash\nsudo apt install ros-humble-gazebo-ros-pkgs ros-humble-gazebo-ros2-control ros-humble-gazebo-dev\n```\n\n## Basic Gazebo Concepts\n\n### Worlds\nGazebo worlds define the environment where simulation takes place. They include:\n- Physics properties (gravity, air density)\n- Models (robots, objects, obstacles)\n- Lighting and visual properties\n- Plugins for additional functionality\n\nExample world file:\n```xml\n<?xml version=\"1.0\" ?>\n<sdf version=\"1.7\">\n  <world name=\"simple_world\">\n    <!-- Physics properties -->\n    <physics type=\"ode\">\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n    <!-- Lighting -->\n    <light name=\"sun\" type=\"directional\">\n      <cast_shadows>true</cast_shadows>\n      <pose>0 0 10 0 0 0</pose>\n      <diffuse>0.8 0.8 0.8 1</diffuse>\n      <specular>0.2 0.2 0.2 1</specular>\n      <attenuation>\n        <range>1000</range>\n        <constant>0.9</constant>\n        <linear>0.01</linear>\n        <quadratic>0.001</quadratic>\n      </attenuation>\n    </light>\n\n    <!-- Ground plane -->\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <!-- Sky -->\n    <include>\n      <uri>model://sun</uri>\n    </include>\n  </world>\n</sdf>\n```\n\n### Models\nModels represent objects in the simulation. They include:\n- Visual properties (appearance)\n- Collision properties (physics interaction)\n- Inertial properties (mass, center of mass)\n- Joints (connections between parts)\n\n## ROS 2 Integration\n\n### Gazebo ROS Packages\nThe `gazebo_ros_pkgs` provide ROS 2 interfaces for Gazebo:\n- `gazebo_ros`: Core ROS 2 interface\n- `gazebo_plugins`: Various simulation plugins\n- `gazebo_msgs`: ROS 2 messages for Gazebo\n\n### Launching Gazebo with ROS 2\n```bash\n# Launch empty world with ROS 2 interface\nros2 launch gazebo_ros empty_world.launch.py\n\n# Launch with a specific world file\nros2 launch gazebo_ros empty_world.launch.py world:=/path/to/world.sdf\n```\n\n## Controlling Robots in Gazebo\n\n### Joint State Publisher\nGazebo publishes joint states that can be used with ROS 2:\n```bash\n# View joint states from simulated robot\nros2 topic echo /joint_states\n```\n\n### Robot Control\nUse ROS 2 controllers to command robot joints:\n```xml\n<!-- In your robot's URDF or SDF -->\n<gazebo>\n  <plugin name=\"gazebo_ros_control\" filename=\"libgazebo_ros_control.so\">\n    <robotNamespace>/your_robot</robotNamespace>\n  </plugin>\n</gazebo>\n```\n\n## Creating a Simple Simulation\n\n### 1. Create a World File\nCreate `simple_room.sdf`:\n```xml\n<?xml version=\"1.0\" ?>\n<sdf version=\"1.7\">\n  <world name=\"simple_room\">\n    <physics type=\"ode\">\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n    <light name=\"sun\" type=\"directional\">\n      <pose>0 0 10 0 0 0</pose>\n      <diffuse>0.8 0.8 0.8 1</diffuse>\n      <specular>0.2 0.2 0.2 1</specular>\n    </light>\n\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    <!-- Simple box obstacle -->\n    <model name=\"box\">\n      <pose>2 0 0.5 0 0 0</pose>\n      <link name=\"link\">\n        <collision name=\"collision\">\n          <geometry>\n            <box>\n              <size>1 1 1</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name=\"visual\">\n          <geometry>\n            <box>\n              <size>1 1 1</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.5 0.5 0.5 1</ambient>\n            <diffuse>0.8 0.3 0.3 1</diffuse>\n          </material>\n        </visual>\n        <inertial>\n          <mass>1.0</mass>\n          <inertia>\n            <ixx>0.166667</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>0.166667</iyy>\n            <iyz>0</iyz>\n            <izz>0.166667</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n  </world>\n</sdf>\n```\n\n### 2. Launch the Simulation\n```bash\ngz sim simple_room.sdf\n```\n\n## Robot Simulation in Gazebo\n\n### URDF Integration\nTo use your URDF robot in Gazebo, add Gazebo-specific tags:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"humanoid_robot\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Your URDF model -->\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.2 0.5\"/>\n      </geometry>\n    </visual>\n    <collision>\n      <geometry>\n        <box size=\"0.3 0.2 0.5\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"5.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.1\" iyz=\"0.0\" izz=\"0.1\"/>\n    </inertial>\n  </link>\n\n  <!-- Gazebo-specific tags -->\n  <gazebo reference=\"base_link\">\n    <material>Gazebo/Blue</material>\n    <mu1>0.2</mu1>\n    <mu2>0.2</mu2>\n  </gazebo>\n\n  <!-- Controller plugin -->\n  <gazebo>\n    <plugin filename=\"libgazebo_ros2_control.so\" name=\"gazebo_ros2_control\">\n      <parameters>$(find your_robot_description)/config/controllers.yaml</parameters>\n    </plugin>\n  </gazebo>\n</robot>\n```\n\n### Controllers Configuration\nCreate `controllers.yaml`:\n```yaml\ncontroller_manager:\n  ros__parameters:\n    update_rate: 100  # Hz\n\n    joint_state_broadcaster:\n      type: joint_state_broadcaster/JointStateBroadcaster\n\n    velocity_controller:\n      type: velocity_controllers/JointGroupVelocityController\n\n    position_controller:\n      type: position_controllers/JointGroupPositionController\n```\n\n## Sensor Simulation\n\nGazebo can simulate various sensors:\n\n### Camera\n```xml\n<gazebo reference=\"camera_link\">\n  <sensor name=\"camera\" type=\"camera\">\n    <camera name=\"head\">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n    <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n      <frame_name>camera_link</frame_name>\n      <topic_name>image_raw</topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n### LIDAR\n```xml\n<gazebo reference=\"lidar_link\">\n  <sensor name=\"lidar\" type=\"ray\">\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-1.570796</min_angle>\n          <max_angle>1.570796</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <namespace>laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n## Debugging and Visualization\n\n### Gazebo GUI\nLaunch with GUI for visualization:\n```bash\ngz sim -g your_world.sdf\n```\n\n### Model States\nMonitor model states:\n```bash\nros2 topic echo /model_states\n```\n\n### TF Tree\nVisualize transforms:\n```bash\nros2 run tf2_tools view_frames\n```\n\n## Best Practices\n\n- Start with simple models and gradually add complexity\n- Use appropriate physics properties (mass, friction) for realistic behavior\n- Test controllers in simulation before deploying to real robots\n- Use collision checking to prevent interpenetration\n- Optimize update rates for performance vs. accuracy trade-offs\n\n## Exercise\n\nCreate a simple simulation with your humanoid robot navigating around obstacles in Gazebo. Implement a basic controller that allows the robot to move forward, turn, and avoid obstacles using simulated sensors.",
    "path": "module-2-digital-twin\\gazebo-simulation.md",
    "description": ""
  },
  "module-2-digital-twin\\gazebo-world-setup": {
    "title": "module-2-digital-twin\\gazebo-world-setup",
    "content": "# Gazebo World Setup and Configuration\n\nSetting up realistic and functional simulation environments is crucial for effective digital twin development. This section covers creating and configuring Gazebo worlds for humanoid robotics applications.\n\n## World File Structure\n\nA Gazebo world file is an SDF (Simulation Description Format) file that defines the complete simulation environment. The basic structure includes:\n\n```xml\n<?xml version=\"1.0\" ?>\n<sdf version=\"1.7\">\n  <world name=\"humanoid_test_world\">\n    <!-- Physics properties -->\n    <physics type=\"ode\">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n    <!-- Scene properties -->\n    <scene>\n      <ambient>0.4 0.4 0.4 1</ambient>\n      <background>0.7 0.7 0.7 1</background>\n      <shadows>true</shadows>\n    </scene>\n\n    <!-- Lighting -->\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    <!-- Ground plane -->\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <!-- Models and objects -->\n    <!-- Robot, obstacles, furniture, etc. -->\n\n    <!-- Plugins -->\n    <!-- Additional functionality -->\n\n  </world>\n</sdf>\n```\n\n## Physics Configuration\n\n### Physics Engine Selection\nChoose the appropriate physics engine based on your simulation requirements:\n\n```xml\n<!-- ODE (Open Dynamics Engine) - Default, good balance -->\n<physics type=\"ode\">\n  <max_step_size>0.001</max_step_size>\n  <real_time_factor>1</real_time_factor>\n  <real_time_update_rate>1000</real_time_update_rate>\n  <gravity>0 0 -9.8</gravity>\n  <ode>\n    <solver>\n      <type>quick</type>  <!-- quick or pgs -->\n      <iters>10</iters>\n      <sor>1.3</sor>\n    </solver>\n    <constraints>\n      <cfm>0</cfm>\n      <erp>0.2</erp>\n      <contact_max_correcting_vel>100</contact_max_correcting_vel>\n      <contact_surface_layer>0.001</contact_surface_layer>\n    </constraints>\n  </ode>\n</physics>\n\n<!-- Bullet - Faster, good for complex interactions -->\n<physics type=\"bullet\">\n  <max_step_size>0.001</max_step_size>\n  <real_time_factor>1</real_time_factor>\n  <real_time_update_rate>1000</real_time_update_rate>\n  <gravity>0 0 -9.8</gravity>\n  <bullet>\n    <solver>\n      <type>sequential_impulse</type>\n      <iteration_count>50</iteration_count>\n    </solver>\n    <constraints>\n      <cfm>0.0</cfm>\n      <erp>0.2</erp>\n    </constraints>\n  </bullet>\n</physics>\n```\n\n### Performance vs. Accuracy Trade-offs\nAdjust physics parameters based on your needs:\n\n```xml\n<!-- For real-time simulation (faster, less accurate) -->\n<physics type=\"ode\">\n  <max_step_size>0.01</max_step_size>  <!-- Larger time steps -->\n  <real_time_update_rate>100</real_time_update_rate>  <!-- Lower update rate -->\n  <ode>\n    <solver>\n      <iters>10</iters>  <!-- Fewer iterations -->\n    </solver>\n  </ode>\n</physics>\n\n<!-- For accurate simulation (slower, more accurate) -->\n<physics type=\"ode\">\n  <max_step_size>0.0001</max_step_size>  <!-- Smaller time steps -->\n  <real_time_update_rate>10000</real_time_update_rate>  <!-- Higher update rate -->\n  <ode>\n    <solver>\n      <iters>100</iters>  <!-- More iterations -->\n    </solver>\n  </ode>\n</physics>\n```\n\n## Lighting and Visual Environment\n\n### Dynamic Lighting\nCreate realistic lighting conditions:\n\n```xml\n<!-- Directional light (sun) -->\n<light name=\"sun\" type=\"directional\">\n  <cast_shadows>true</cast_shadows>\n  <pose>0 0 10 0 0 0</pose>\n  <diffuse>0.8 0.8 0.8 1</diffuse>\n  <specular>0.2 0.2 0.2 1</specular>\n  <attenuation>\n    <range>1000</range>\n    <constant>0.9</constant>\n    <linear>0.01</linear>\n    <quadratic>0.001</quadratic>\n  </attenuation>\n  <direction>-0.3 -0.3 -1</direction>\n</light>\n\n<!-- Point lights for indoor environments -->\n<light name=\"room_light\" type=\"point\">\n  <pose>0 0 3 0 0 0</pose>\n  <diffuse>1 1 1 1</diffuse>\n  <specular>0.5 0.5 0.5 1</specular>\n  <attenuation>\n    <range>10</range>\n    <constant>0.2</constant>\n    <linear>0.5</linear>\n    <quadratic>0.01</quadratic>\n  </attenuation>\n</light>\n\n<!-- Spot lights for focused illumination -->\n<light name=\"spot_light\" type=\"spot\">\n  <pose>2 2 3 0 0 0</pose>\n  <diffuse>1 0.8 0.5 1</diffuse>\n  <specular>1 1 1 1</specular>\n  <attenuation>\n    <range>5</range>\n    <constant>0.2</constant>\n    <linear>0.5</linear>\n    <quadratic>0.01</quadratic>\n  </attenuation>\n  <direction>-0.5 -0.5 -1</direction>\n  <spot>\n    <inner_angle>0.1</inner_angle>\n    <outer_angle>0.5</outer_angle>\n    <falloff>1</falloff>\n  </spot>\n</light>\n```\n\n### Environment and Sky\nConfigure the visual environment:\n\n```xml\n<scene>\n  <ambient>0.3 0.3 0.3 1</ambient>\n  <background>0.6 0.7 0.9 1</background>\n  <shadows>true</shadows>\n  <!-- Enable HDR rendering -->\n  <grid>false</grid>\n  <origin_visual>false</origin_visual>\n  <!-- Sky properties -->\n  <sky>\n    <time>14:00</time>\n    <sun_direction>0.7 -0.7 -0.7</sun_direction>\n    <clouds>\n      <speed>0.1</speed>\n      <direction>0.3 0.7</direction>\n      <humidity>0.5</humidity>\n      <mean_size>0.5</mean_size>\n    </clouds>\n  </sky>\n</scene>\n```\n\n## Model Placement and Configuration\n\n### Including Standard Models\nUse Gazebo's built-in models:\n\n```xml\n<!-- Ground plane -->\n<include>\n  <uri>model://ground_plane</uri>\n</include>\n\n<!-- Sun light -->\n<include>\n  <uri>model://sun</uri>\n</include>\n\n<!-- Custom models from model database -->\n<include>\n  <uri>model://cylinder</uri>\n  <pose>2 0 1 0 0 0</pose>\n</include>\n\n<include>\n  <uri>model://box</uri>\n  <pose>-2 0 1 0 0 0</pose>\n  <name>obstacle_box</name>\n</include>\n```\n\n### Creating Custom Models\nDefine custom objects in your world:\n\n```xml\n<!-- Simple box obstacle -->\n<model name=\"table\">\n  <pose>1 1 0.5 0 0 0</pose>\n  <link name=\"table_base\">\n    <pose>0 0 0.4 0 0 0</pose>\n    <collision name=\"collision\">\n      <geometry>\n        <box>\n          <size>1.0 0.8 0.8</size>\n        </box>\n      </geometry>\n    </collision>\n    <visual name=\"visual\">\n      <geometry>\n        <box>\n          <size>1.0 0.8 0.8</size>\n        </box>\n      </geometry>\n      <material>\n        <ambient>0.8 0.6 0.4 1</ambient>\n        <diffuse>0.8 0.6 0.4 1</diffuse>\n        <specular>0.2 0.2 0.2 1</specular>\n      </material>\n    </visual>\n    <inertial>\n      <mass>50.0</mass>\n      <inertia>\n        <ixx>6.8667</ixx>\n        <ixy>0</ixy>\n        <ixz>0</ixz>\n        <iyy>8.5333</iyy>\n        <iyz>0</iyz>\n        <izz>2.6667</izz>\n      </inertia>\n    </inertial>\n  </link>\n\n  <link name=\"table_top\">\n    <pose>0 0 0.85 0 0 0</pose>\n    <collision name=\"collision\">\n      <geometry>\n        <box>\n          <size>1.2 1.0 0.05</size>\n        </box>\n      </geometry>\n    </collision>\n    <visual name=\"visual\">\n      <geometry>\n        <box>\n          <size>1.2 1.0 0.05</size>\n        </box>\n      </geometry>\n      <material>\n        <ambient>0.9 0.9 0.9 1</ambient>\n        <diffuse>0.9 0.9 0.9 1</diffuse>\n        <specular>0.3 0.3 0.3 1</specular>\n      </material>\n    </visual>\n    <inertial>\n      <mass>10.0</mass>\n      <inertia>\n        <ixx>1.0417</ixx>\n        <ixy>0</ixy>\n        <ixz>0</ixz>\n        <iyy>1.45</iyy>\n        <iyz>0</iyz>\n        <izz>0.4083</izz>\n      </inertia>\n    </inertial>\n  </link>\n\n  <joint name=\"top_to_base\" type=\"fixed\">\n    <parent>table_base</parent>\n    <child>table_top</child>\n  </joint>\n</model>\n```\n\n## Robot Integration\n\n### Spawning Robots in Worlds\nInclude your robot model directly in the world file:\n\n```xml\n<!-- Include your robot URDF -->\n<include>\n  <uri>model://humanoid_robot</uri>\n  <pose>0 0 1 0 0 0</pose>\n  <name>humanoid_1</name>\n</include>\n```\n\n### Robot-Specific World Configuration\nConfigure world properties specifically for your robot:\n\n```xml\n<world name=\"humanoid_navigaton_world\">\n  <!-- Physics tuned for humanoid robot -->\n  <physics type=\"ode\">\n    <max_step_size>0.001</max_step_size>\n    <real_time_factor>1</real_time_factor>\n    <real_time_update_rate>1000</real_time_update_rate>\n    <gravity>0 0 -9.8</gravity>\n    <ode>\n      <solver>\n        <type>quick</type>\n        <iters>20</iters>  <!-- Higher iterations for stability -->\n        <sor>1.3</sor>\n      </solver>\n      <constraints>\n        <cfm>1e-5</cfm>  <!-- Lower CFM for better contact stability -->\n        <erp>0.2</erp>\n        <contact_max_correcting_vel>10</contact_max_correcting_vel>\n        <contact_surface_layer>0.001</contact_surface_layer>\n      </constraints>\n    </ode>\n  </physics>\n\n  <!-- Humanoid-specific objects -->\n  <include>\n    <uri>model://ground_plane</uri>\n  </include>\n\n  <!-- Navigation obstacles -->\n  <model name=\"obstacle_1\">\n    <pose>2 0 0.5 0 0 0</pose>\n    <link name=\"link\">\n      <collision name=\"collision\">\n        <geometry>\n          <box><size>0.5 0.5 1.0</size></box>\n        </geometry>\n      </collision>\n      <visual name=\"visual\">\n        <geometry>\n          <box><size>0.5 0.5 1.0</size></box>\n        </geometry>\n        <material>\n          <ambient>0.8 0.3 0.3 1</ambient>\n          <diffuse>0.8 0.3 0.3 1</diffuse>\n        </material>\n      </visual>\n      <inertial>\n        <mass>10.0</mass>\n        <inertia>\n          <ixx>1.0417</ixx>\n          <ixy>0</ixy>\n          <ixz>0</ixz>\n          <iyy>1.0417</iyy>\n          <iyz>0</iyz>\n          <izz>0.4167</izz>\n        </inertia>\n      </inertial>\n    </link>\n  </model>\n\n  <!-- Add robot -->\n  <include>\n    <uri>model://humanoid_robot</uri>\n    <pose>0 0 1 0 0 0</pose>\n    <name>test_humanoid</name>\n  </include>\n</world>\n```\n\n## Plugin Configuration\n\n### ROS Integration Plugins\nAdd plugins for ROS communication:\n\n```xml\n<!-- World plugin for ROS integration -->\n<plugin name=\"world_plugin\" filename=\"libgazebo_ros_init.so\">\n  <ros>\n    <namespace>gazebo</namespace>\n  </ros>\n  <update_rate>1000</update_rate>\n</plugin>\n\n<!-- Physics reset plugin -->\n<plugin name=\"physics_reset\" filename=\"libgazebo_ros_pubslish_odometry.so\">\n  <ros>\n    <namespace>gazebo</namespace>\n  </ros>\n</plugin>\n```\n\n### Custom Plugins\nCreate custom world plugins for specific functionality:\n\n```xml\n<!-- Example: Plugin to spawn objects at random locations -->\n<plugin name=\"random_spawner\" filename=\"librandom_spawner.so\">\n  <spawn_count>5</spawn_count>\n  <spawn_area>\n    <min_x>-5</min_x>\n    <max_x>5</max_x>\n    <min_y>-5</min_y>\n    <max_y>5</max_y>\n  </spawn_area>\n  <object_types>\n    <object>box</object>\n    <object>sphere</object>\n    <object>cylinder</object>\n  </object_types>\n</plugin>\n```\n\n## Advanced World Features\n\n### Heightmaps for Terrain\nCreate realistic outdoor environments:\n\n```xml\n<model name=\"terrain\">\n  <static>true</static>\n  <link name=\"link\">\n    <collision name=\"collision\">\n      <geometry>\n        <heightmap>\n          <uri>model://my_terrain/heightmap.png</uri>\n          <size>100 100 20</size>\n          <pos>0 0 0</pos>\n        </heightmap>\n      </geometry>\n    </collision>\n    <visual name=\"visual\">\n      <geometry>\n        <heightmap>\n          <uri>model://my_terrain/heightmap.png</uri>\n          <size>100 100 20</size>\n          <pos>0 0 0</pos>\n        </heightmap>\n      </geometry>\n      <material>\n        <script>\n          <uri>file://media/materials/scripts/gazebo.material</uri>\n          <name>Gazebo/Ground</name>\n        </script>\n      </material>\n    </visual>\n  </link>\n</model>\n```\n\n### Wind Effects\nSimulate environmental forces:\n\n```xml\n<world name=\"windy_world\">\n  <!-- Add wind plugin -->\n  <plugin name=\"wind\" filename=\"libgazebo_ros_wind.so\">\n    <always_on>true</always_on>\n    <pub_topic>/wind</pub_topic>\n    <wind_direction>1 0 0</wind_direction>\n    <wind_force>0.5 0 0</wind_force>\n    <wind_gust_duration>0</wind_gust_duration>\n    <wind_gust_start>0</wind_gust_start>\n    <wind_gust_final>0</wind_gust_final>\n  </plugin>\n</world>\n```\n\n## World Launch Configuration\n\n### ROS 2 Launch Files\nCreate launch files to easily load your worlds:\n\n```python\n# launch/humanoid_world.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    world_arg = DeclareLaunchArgument(\n        'world',\n        default_value='humanoid_test_world.sdf',\n        description='Choose one of the world files from `/my_robot_gazebo/worlds`'\n    )\n\n    world_path = PathJoinSubstitution([\n        get_package_share_directory('my_robot_gazebo'),\n        'worlds',\n        LaunchConfiguration('world')\n    ])\n\n    # Launch Gazebo with world\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            get_package_share_directory('gazebo_ros'),\n            '/launch/gazebo.launch.py'\n        ]),\n        launch_arguments={\n            'world': world_path,\n            'verbose': 'true'\n        }.items()\n    )\n\n    return LaunchDescription([\n        world_arg,\n        gazebo\n    ])\n```\n\n## Performance Optimization\n\n### Level of Detail (LOD)\nOptimize complex models for performance:\n\n```xml\n<model name=\"detailed_object\">\n  <link name=\"link\">\n    <visual name=\"visual\">\n      <geometry>\n        <mesh>\n          <uri>model://object/meshes/complex_model.dae</uri>\n        </mesh>\n      </geometry>\n      <!-- Simplified collision geometry -->\n      <collision name=\"collision\">\n        <geometry>\n          <box><size>1 1 1</size></box>\n        </geometry>\n      </collision>\n    </visual>\n  </link>\n</model>\n```\n\n### Dynamic Loading\nUse plugins to load/unload objects dynamically:\n\n```xml\n<!-- Plugin to dynamically add/remove objects -->\n<plugin name=\"dynamic_objects\" filename=\"libdynamic_objects.so\">\n  <max_objects>20</max_objects>\n  <object_types>\n    <type>\n      <name>box</name>\n      <uri>model://box</uri>\n      <max_count>10</max_count>\n    </type>\n    <type>\n      <name>sphere</name>\n      <uri>model://sphere</uri>\n      <max_count>10</max_count>\n    </type>\n  </object_types>\n</plugin>\n```\n\n## Debugging World Files\n\n### Common Issues and Solutions\n\n#### 1. Robot Falls Through Ground\n- Check collision geometry is properly defined\n- Verify mass and inertia properties\n- Adjust physics parameters (CFM, ERP)\n\n#### 2. Objects Interpenetrate\n- Increase constraint solver iterations\n- Adjust contact properties\n- Verify collision geometry overlaps\n\n#### 3. Performance Issues\n- Simplify collision geometry\n- Reduce physics update rate\n- Use static models where possible\n\n### Validation Commands\n```bash\n# Check SDF validity\ngz sdf -k /path/to/your/world.sdf\n\n# View world in Gazebo\ngz sim /path/to/your/world.sdf\n\n# Debug with verbose output\ngz sim -v 4 /path/to/your/world.sdf\n```\n\n## Best Practices\n\n1. **Start Simple**: Begin with basic worlds and add complexity gradually\n2. **Use Standard Models**: Leverage Gazebo's built-in models when possible\n3. **Optimize for Performance**: Balance visual quality with simulation speed\n4. **Validate Physics**: Test that objects behave realistically\n5. **Document Configuration**: Keep comments explaining parameter choices\n6. **Test Robot Integration**: Verify your robot works in the world\n7. **Version Control**: Track world file changes with your robot code\n\n## Exercise\n\nCreate a complete Gazebo world for humanoid robot testing that includes:\n1. A physics configuration optimized for humanoid robots\n2. Appropriate lighting for vision sensors\n3. Various obstacles and navigation challenges\n4. A robot spawn point with proper initial configuration\n5. ROS integration plugins for communication\n6. Performance optimizations for real-time simulation\n\nTest your world by launching it with your humanoid robot model and verifying that the robot can navigate and interact with the environment appropriately.",
    "path": "module-2-digital-twin\\gazebo-world-setup.md",
    "description": ""
  },
  "module-2-digital-twin\\index": {
    "title": "Module 2 Digital Twin",
    "content": "\n# Module 2: The Digital Twin (Gazebo & Unity)\n\nWelcome to the Digital Twin module! This module focuses on creating and validating virtual representations of physical robots, enabling safe testing and iterative development without requiring physical hardware.\n\n## Overview\n\nDigital twins are virtual models that accurately represent physical systems. In robotics, digital twins allow us to:\n- Test robot behaviors in safe virtual environments\n- Validate control algorithms before deployment\n- Simulate sensors and their data\n- Train AI models with synthetic data\n- Iterate on designs without physical constraints\n\nThis module covers both Gazebo (the standard ROS simulation environment) and Unity (a powerful game engine for high-fidelity simulation).\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n\n- Create accurate digital twin models of humanoid robots\n- Set up simulation environments in both Gazebo and Unity\n- Validate that digital twins behave like their physical counterparts\n- Simulate various sensors and their outputs\n- Understand the physics and collision properties needed for realistic simulation\n\n## Prerequisites\n\nBefore starting this module, ensure you have:\n\n- Completed Module 1 (ROS 2 fundamentals)\n- Basic understanding of physics concepts (mass, friction, collision)\n- ROS 2 Humble installed with Gazebo packages\n- Basic familiarity with 3D concepts\n\n## Module Structure\n\nThis module is organized into the following sections:\n\n1. **Gazebo Simulation** - The standard ROS simulation environment\n2. **Unity Visualization** - High-fidelity simulation with Unity\n3. **Physics and Collisions** - Understanding realistic physical interactions\n4. **Sensor Simulation** - Modeling sensors in virtual environments\n5. **Practical Exercises** - Hands-on applications with digital twins\n\n## Digital Twin Benefits\n\nDigital twins provide several advantages in robotics development:\n\n- **Safety**: Test dangerous maneuvers in simulation first\n- **Cost**: Reduce wear on physical robots and hardware\n- **Speed**: Rapid iteration without physical setup time\n- **Repeatability**: Exactly repeat experiments with identical conditions\n- **Data Generation**: Create large datasets for AI training\n\n## Next Steps\n\nBegin with the next section to dive into Gazebo simulation. Each section builds on the previous one, so follow the sequence for the best learning experience.",
    "path": "module-2-digital-twin\\index.md",
    "description": ""
  },
  "module-2-digital-twin\\physics-collisions": {
    "title": "module-2-digital-twin\\physics-collisions",
    "content": "# Physics and Collisions\n\nUnderstanding physics and collision properties is crucial for creating realistic digital twins. This section covers the principles of physics simulation and collision detection in both Gazebo and Unity environments.\n\n## Physics Simulation Fundamentals\n\nPhysics simulation in robotics environments involves modeling real-world physical behaviors:\n- **Rigid Body Dynamics**: Movement and interaction of solid objects\n- **Collision Detection**: Identifying when objects intersect\n- **Contact Response**: Calculating forces when objects touch\n- **Constraints**: Joints and connections between parts\n\n## Physics in Gazebo\n\n### Physics Engines\nGazebo supports multiple physics engines:\n- **ODE (Open Dynamics Engine)**: Default, good balance of speed and accuracy\n- **Bullet**: Fast and robust, good for complex interactions\n- **Simbody**: Highly accurate, suitable for biomechanics\n- **DART**: Advanced constraint handling\n\n### Physics Configuration\nIn world files, configure physics properties:\n\n```xml\n<physics type=\"ode\">\n  <max_step_size>0.001</max_step_size>  <!-- Time step for simulation -->\n  <real_time_factor>1</real_time_factor> <!-- Simulation speed vs real time -->\n  <real_time_update_rate>1000</real_time_update_rate> <!-- Hz -->\n\n  <!-- Gravity -->\n  <gravity>0 0 -9.8</gravity>\n\n  <!-- ODE-specific parameters -->\n  <ode>\n    <solver>\n      <type>quick</type>  <!-- or \"pgs\" -->\n      <iters>10</iters>\n      <sor>1.3</sor>\n    </solver>\n    <constraints>\n      <cfm>0</cfm>\n      <erp>0.2</erp>\n      <contact_max_correcting_vel>100</contact_max_correcting_vel>\n      <contact_surface_layer>0.001</contact_surface_layer>\n    </constraints>\n  </ode>\n</physics>\n```\n\n### Material Properties\nDefine material properties for realistic interactions:\n\n```xml\n<link name=\"link_with_materials\">\n  <collision name=\"collision\">\n    <geometry>\n      <box><size>0.1 0.1 0.1</size></box>\n    </geometry>\n    <surface>\n      <friction>\n        <ode>\n          <mu>0.5</mu>      <!-- Static friction coefficient -->\n          <mu2>0.5</mu2>    <!-- Secondary friction coefficient -->\n          <slip1>0.0</slip1> <!-- Primary slip coefficient -->\n          <slip2>0.0</slip2> <!-- Secondary slip coefficient -->\n        </ode>\n      </friction>\n      <bounce>\n        <restitution_coefficient>0.1</restitution_coefficient>\n        <threshold>100000</threshold>\n      </bounce>\n      <contact>\n        <ode>\n          <max_vel>100</max_vel>\n          <min_depth>0.001</min_depth>\n        </ode>\n      </contact>\n    </surface>\n  </collision>\n</link>\n```\n\n### Inertial Properties\nProper inertial properties are essential for realistic simulation:\n\n```xml\n<inertial>\n  <mass>1.0</mass>\n  <inertia>\n    <ixx>0.0833333</ixx>\n    <ixy>0.0</ixy>\n    <ixz>0.0</ixz>\n    <iyy>0.0833333</iyy>\n    <iyz>0.0</iyz>\n    <izz>0.0833333</izz>\n  </inertia>\n</inertial>\n```\n\nFor a box with mass m and dimensions (x, y, z):\n- ixx = m*(y + z)/12\n- iyy = m*(x + z)/12\n- izz = m*(x + y)/12\n\n## Collision Detection in Gazebo\n\n### Collision Geometries\nGazebo supports various collision geometries:\n- **Box**: Rectangular prism\n- **Cylinder**: Cylindrical shape\n- **Sphere**: Spherical shape\n- **Mesh**: Complex shapes from 3D models\n- **Plane**: Infinite flat surface\n\n```xml\n<collision name=\"collision_box\">\n  <geometry>\n    <box><size>0.1 0.2 0.3</size></box>\n  </geometry>\n</collision>\n\n<collision name=\"collision_cylinder\">\n  <geometry>\n    <cylinder>\n      <radius>0.05</radius>\n      <length>0.1</length>\n    </cylinder>\n  </geometry>\n</collision>\n\n<collision name=\"collision_sphere\">\n  <geometry>\n    <sphere><radius>0.05</radius></sphere>\n  </geometry>\n</collision>\n```\n\n### Collision Filtering\nUse collision groups and masks to control which objects can collide:\n\n```xml\n<collision name=\"collision_with_filter\">\n  <surface>\n    <contact>\n      <collide_without_contact>false</collide_without_contact>\n    </contact>\n  </surface>\n</collision>\n```\n\n## Physics in Unity\n\n### Physics Engine\nUnity uses NVIDIA PhysX for physics simulation, which provides:\n- Advanced collision detection\n- Realistic contact response\n- Vehicle dynamics\n- Cloth simulation\n- Soft body dynamics\n\n### Rigidbody Configuration\nEach physical object needs a Rigidbody component:\n\n```csharp\nusing UnityEngine;\n\npublic class PhysicsObject : MonoBehaviour\n{\n    [Header(\"Physics Properties\")]\n    public float mass = 1.0f;\n    public Vector3 centerOfMass = Vector3.zero;\n    public Vector3 inertiaTensor = Vector3.one;\n    public float drag = 0.0f;\n    public float angularDrag = 0.05f;\n\n    [Header(\"Collision Properties\")]\n    public bool useGravity = true;\n    public bool isKinematic = false;\n\n    private Rigidbody rb;\n\n    void Start()\n    {\n        rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            rb.mass = mass;\n            rb.centerOfMass = centerOfMass;\n            rb.inertiaTensor = inertiaTensor;\n            rb.drag = drag;\n            rb.angularDrag = angularDrag;\n            rb.useGravity = useGravity;\n            rb.isKinematic = isKinematic;\n        }\n    }\n}\n```\n\n### Collider Configuration\nUnity supports various collider types:\n\n```csharp\n// Box Collider\n[RequireComponent(typeof(Rigidbody))]\npublic class BoxColliderSetup : MonoBehaviour\n{\n    [SerializeField] private Vector3 size = Vector3.one;\n    [SerializeField] private Vector3 center = Vector3.zero;\n    [SerializeField] private bool isTrigger = false;\n\n    void Start()\n    {\n        BoxCollider boxCollider = gameObject.AddComponent<BoxCollider>();\n        boxCollider.size = size;\n        boxCollider.center = center;\n        boxCollider.isTrigger = isTrigger;\n    }\n}\n\n// Mesh Collider\n[RequireComponent(typeof(Rigidbody))]\npublic class MeshColliderSetup : MonoBehaviour\n{\n    [SerializeField] private bool convex = false;\n    [SerializeField] private bool inflateMesh = false;\n\n    void Start()\n    {\n        MeshCollider meshCollider = gameObject.AddComponent<MeshCollider>();\n        meshCollider.convex = convex;\n        meshCollider.inflateMesh = inflateMesh;\n\n        // Use the mesh from a MeshFilter component\n        MeshFilter meshFilter = GetComponent<MeshFilter>();\n        if (meshFilter != null)\n        {\n            meshCollider.sharedMesh = meshFilter.sharedMesh;\n        }\n    }\n}\n```\n\n### Joint Configuration\nUnity provides various joint types for connecting rigidbodies:\n\n```csharp\n// Hinge Joint Example\npublic class HingeJointSetup : MonoBehaviour\n{\n    [SerializeField] private float motorForce = 10f;\n    [SerializeField] private float targetVelocity = 0f;\n    [SerializeField] private bool useMotor = false;\n\n    void Start()\n    {\n        HingeJoint hinge = GetComponent<HingeJoint>();\n\n        // Configure motor\n        JointMotor motor = hinge.motor;\n        motor.force = motorForce;\n        motor.targetVelocity = targetVelocity;\n        motor.freeSpin = false;\n        hinge.motor = motor;\n        hinge.useMotor = useMotor;\n\n        // Configure limits\n        JointLimits limits = hinge.limits;\n        limits.min = -90f;  // Minimum angle in degrees\n        limits.max = 90f;   // Maximum angle in degrees\n        hinge.limits = limits;\n        hinge.useLimits = true;\n    }\n}\n```\n\n### Physics Materials\nCreate realistic surface interactions:\n\n```csharp\n// Create a physics material in code\npublic class PhysicsMaterialSetup : MonoBehaviour\n{\n    [SerializeField] private float staticFriction = 0.5f;\n    [SerializeField] private float dynamicFriction = 0.4f;\n    [SerializeField] private float bounciness = 0.1f;\n    [SerializeField] private PhysicMaterialCombine frictionCombine = PhysicMaterialCombine.Average;\n    [SerializeField] private PhysicMaterialCombine bounceCombine = PhysicMaterialCombine.Average;\n\n    void Start()\n    {\n        PhysicMaterial material = new PhysicMaterial(\"CustomMaterial\");\n        material.staticFriction = staticFriction;\n        material.dynamicFriction = dynamicFriction;\n        material.bounciness = bounciness;\n        material.frictionCombine = frictionCombine;\n        material.bounceCombine = bounceCombine;\n\n        Collider col = GetComponent<Collider>();\n        if (col != null)\n        {\n            col.material = material;\n        }\n    }\n}\n```\n\n## Collision Events\n\n### Unity Collision Detection\nHandle collision events in Unity:\n\n```csharp\nusing UnityEngine;\n\npublic class CollisionHandler : MonoBehaviour\n{\n    void OnCollisionEnter(Collision collision)\n    {\n        Debug.Log($\"Collision with {collision.gameObject.name}\");\n\n        foreach (ContactPoint contact in collision.contacts)\n        {\n            Debug.DrawRay(contact.point, contact.normal, Color.white);\n            Debug.Log($\"Contact point: {contact.point}\");\n            Debug.Log($\"Contact force: {collision.impulse}\");\n        }\n    }\n\n    void OnCollisionStay(Collision collision)\n    {\n        // Called each frame while colliding\n        foreach (ContactPoint contact in collision.contacts)\n        {\n            Debug.Log($\"Contact force: {contact.force}\");\n        }\n    }\n\n    void OnCollisionExit(Collision collision)\n    {\n        Debug.Log($\"Collision ended with {collision.gameObject.name}\");\n    }\n\n    // Trigger events (for isTrigger colliders)\n    void OnTriggerEnter(Collider other)\n    {\n        Debug.Log($\"Trigger entered: {other.name}\");\n    }\n}\n```\n\n## Advanced Physics Concepts\n\n### Continuous Collision Detection\nFor fast-moving objects to prevent tunneling:\n\n```csharp\nvoid Start()\n{\n    Rigidbody rb = GetComponent<Rigidbody>();\n    if (rb != null)\n    {\n        // Use continuous collision detection for fast objects\n        rb.collisionDetectionMode = CollisionDetectionMode.Continuous;\n    }\n}\n```\n\n### Layer-based Collision Matrix\nControl which layers can collide with each other in Unity:\n- Edit  Project Settings  Physics\n- Configure the collision matrix for different layers\n\n### Force Application\nApply forces to simulate realistic interactions:\n\n```csharp\npublic class ForceApplication : MonoBehaviour\n{\n    [SerializeField] private float forceMagnitude = 10f;\n    [SerializeField] private ForceMode forceMode = ForceMode.Force;\n\n    public void ApplyForce(Vector3 direction)\n    {\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            rb.AddForce(direction * forceMagnitude, forceMode);\n        }\n    }\n\n    public void ApplyTorque(Vector3 torque)\n    {\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            rb.AddTorque(torque * forceMagnitude, forceMode);\n        }\n    }\n}\n```\n\n## Performance Considerations\n\n### Gazebo Optimization\n- Use simpler collision geometries when possible\n- Adjust physics update rates for performance vs accuracy\n- Use fixed joints instead of complex constraint systems\n- Limit the number of active objects in simulation\n\n### Unity Optimization\n- Use convex mesh colliders for dynamic objects\n- Use compound colliders for complex shapes\n- Adjust solver iteration counts for performance\n- Use object pooling for frequently created/destroyed objects\n\n## Debugging Physics\n\n### Visualization Tools\n- Enable physics visualization in both Gazebo and Unity\n- Use debug drawing to visualize collision shapes\n- Monitor simulation statistics and performance metrics\n\n### Common Issues\n- **Tunneling**: Objects passing through each other (fix with CCD)\n- **Jittering**: Unstable contact points (fix with proper mass ratios)\n- **Penetration**: Objects sinking into each other (fix with proper solver settings)\n\n## Best Practices\n\n- Use realistic mass properties (don't make everything 1kg)\n- Configure friction coefficients based on real materials\n- Test with extreme values to ensure stability\n- Balance accuracy with performance requirements\n- Validate simulation results against real-world data when possible\n\n## Exercise\n\nCreate a simulation scene with multiple objects of different materials and masses. Implement collision detection that triggers different behaviors based on collision forces. Test the scene with different physics configurations to observe the effects on stability and realism.",
    "path": "module-2-digital-twin\\physics-collisions.md",
    "description": ""
  },
  "module-2-digital-twin\\practical-exercises": {
    "title": "module-2-digital-twin\\practical-exercises",
    "content": "# Practical Exercises - Digital Twin Simulation\n\nThis section provides hands-on exercises to reinforce the concepts learned in the Digital Twin module. Complete these exercises to gain practical experience with Gazebo and Unity simulation environments.\n\n## Exercise 1: Basic Robot in Gazebo\n\n### Objective\nCreate a simple robot model and simulate it in Gazebo with basic movement capabilities.\n\n### Steps\n1. Create a URDF file for a simple wheeled robot with:\n   - A base link\n   - Two wheels as child links\n   - Revolute joints connecting wheels to base\n   - Proper inertial properties for each link\n\n2. Launch the robot in Gazebo:\n   ```bash\n   # Create a launch file to spawn your robot\n   ros2 launch your_robot_description spawn_robot.launch.py\n   ```\n\n3. Control the robot using ROS 2 topics:\n   ```bash\n   # Send velocity commands to move the robot\n   ros2 topic pub /cmd_vel geometry_msgs/Twist \"{linear: {x: 0.5}, angular: {z: 0.2}}\"\n   ```\n\n### Expected Outcome\nA robot that moves forward and turns in the Gazebo simulation environment when velocity commands are sent.\n\n### Solution Template\n```xml\n<!-- simple_robot.urdf -->\n<?xml version=\"1.0\"?>\n<robot name=\"simple_robot\">\n  <link name=\"base_link\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.2\" radius=\"0.2\"/>\n      </geometry>\n      <material name=\"blue\">\n        <color rgba=\"0 0 0.8 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.2\" radius=\"0.2\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"5.0\"/>\n      <inertia ixx=\"0.1\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.1\" iyz=\"0.0\" izz=\"0.2\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"wheel_left_joint\" type=\"continuous\">\n    <parent link=\"base_link\"/>\n    <child link=\"wheel_left\"/>\n    <origin xyz=\"-0.1 0.2 0\" rpy=\"1.5708 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n  </joint>\n\n  <link name=\"wheel_left\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.05\" radius=\"0.1\"/>\n      </geometry>\n      <material name=\"black\">\n        <color rgba=\"0 0 0 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.05\" radius=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.001\" iyz=\"0.0\" izz=\"0.002\"/>\n    </inertial>\n  </link>\n\n  <joint name=\"wheel_right_joint\" type=\"continuous\">\n    <parent link=\"base_link\"/>\n    <child link=\"wheel_right\"/>\n    <origin xyz=\"-0.1 -0.2 0\" rpy=\"1.5708 0 0\"/>\n    <axis xyz=\"0 0 1\"/>\n  </joint>\n\n  <link name=\"wheel_right\">\n    <visual>\n      <geometry>\n        <cylinder length=\"0.05\" radius=\"0.1\"/>\n      </geometry>\n      <material name=\"black\"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length=\"0.05\" radius=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"0.5\"/>\n      <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.001\" iyz=\"0.0\" izz=\"0.002\"/>\n    </inertial>\n  </link>\n</robot>\n```\n\n## Exercise 2: Sensor Integration in Gazebo\n\n### Objective\nAdd a camera sensor to your robot and visualize the sensor data.\n\n### Steps\n1. Add a camera sensor to your robot's URDF:\n   ```xml\n   <joint name=\"camera_joint\" type=\"fixed\">\n     <parent link=\"base_link\"/>\n     <child link=\"camera_link\"/>\n     <origin xyz=\"0.2 0 0.1\" rpy=\"0 0 0\"/>\n   </joint>\n\n   <link name=\"camera_link\">\n     <visual>\n       <geometry>\n         <box size=\"0.05 0.05 0.05\"/>\n       </geometry>\n     </visual>\n     <collision>\n       <geometry>\n         <box size=\"0.05 0.05 0.05\"/>\n       </geometry>\n     </collision>\n   </link>\n   ```\n\n2. Add the camera sensor with Gazebo plugin:\n   ```xml\n   <gazebo reference=\"camera_link\">\n     <sensor name=\"camera\" type=\"camera\">\n       <update_rate>30</update_rate>\n       <camera name=\"head\">\n         <horizontal_fov>1.047</horizontal_fov>\n         <image>\n           <width>640</width>\n           <height>480</height>\n           <format>R8G8B8</format>\n         </image>\n         <clip>\n           <near>0.1</near>\n           <far>100</far>\n         </clip>\n       </camera>\n       <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n         <frame_name>camera_link</frame_name>\n         <topic_name>image_raw</topic_name>\n       </plugin>\n     </sensor>\n   </gazebo>\n   ```\n\n3. Visualize the camera feed:\n   ```bash\n   # Install image tools if not already installed\n   sudo apt install ros-humble-image-view\n\n   # View the camera feed\n   ros2 run image_view image_view __ns:=/your_robot_namespace\n   ```\n\n### Expected Outcome\nA robot with a camera that publishes image data to a ROS topic, viewable in an image viewer.\n\n## Exercise 3: Unity Robot Setup\n\n### Objective\nImport a robot model into Unity and set up basic physics properties.\n\n### Steps\n1. Install the URDF Importer package in Unity:\n   - Window  Package Manager\n   - Click the + button  Add package from git URL\n   - Enter: `com.unity.robotics.urdf-importer`\n\n2. Import your URDF file:\n   - Assets  Import Robot from URDF\n   - Select your URDF file\n   - Configure import settings (scale, materials, etc.)\n\n3. Add physics properties to imported robot:\n   ```csharp\n   using UnityEngine;\n\n   public class UnityRobotSetup : MonoBehaviour\n   {\n       [Header(\"Physics Configuration\")]\n       [SerializeField] private float massMultiplier = 1.0f;\n       [SerializeField] private PhysicMaterial robotMaterial;\n\n       void Start()\n       {\n           ConfigureRobotPhysics();\n       }\n\n       void ConfigureRobotPhysics()\n       {\n           // Add Rigidbody to each link\n           var links = GetComponentsInChildren<Transform>();\n           foreach (Transform link in links)\n           {\n               if (link.TryGetComponent(out MeshCollider collider))\n               {\n                   // Add Rigidbody if not present\n                   if (!link.GetComponent<Rigidbody>())\n                   {\n                       Rigidbody rb = link.gameObject.AddComponent<Rigidbody>();\n                       rb.mass = CalculateMass(link.name) * massMultiplier;\n                       rb.drag = 0.1f;\n                       rb.angularDrag = 0.1f;\n                       rb.interpolation = RigidbodyInterpolation.Interpolate;\n                   }\n\n                   // Apply material\n                   if (robotMaterial != null)\n                   {\n                       collider.material = robotMaterial;\n                   }\n               }\n           }\n       }\n\n       float CalculateMass(string linkName)\n       {\n           // Return approximate mass based on link name\n           switch (linkName.ToLower())\n           {\n               case \"base_link\":\n                   return 10.0f;\n               case \"wheel_left\":\n               case \"wheel_right\":\n                   return 1.0f;\n               default:\n                   return 2.0f;\n           }\n       }\n   }\n   ```\n\n4. Test the robot in Unity physics simulation.\n\n### Expected Outcome\nA robot imported from URDF with proper physics properties and realistic behavior in Unity's physics engine.\n\n## Exercise 4: Unity Sensor Simulation\n\n### Objective\nCreate a simulated camera sensor in Unity that publishes data to ROS.\n\n### Steps\n1. Create a camera sensor script:\n   ```csharp\n   using UnityEngine;\n   using Unity.Robotics.ROSTCPConnector;\n   using RosMessageTypes.Sensor;\n   using Unity.Robotics.ROSTCPConnector.MessageGeneration;\n\n   public class UnityCameraSensor : MonoBehaviour\n   {\n       [Header(\"Camera Configuration\")]\n       [SerializeField] private int width = 640;\n       [SerializeField] private int height = 480;\n       [SerializeField] private float fieldOfView = 60f;\n       [SerializeField] private string topicName = \"/unity_camera/image_raw\";\n\n       private Camera cam;\n       private RenderTexture renderTexture;\n       private ROSConnection ros;\n       private Texture2D texture2D;\n\n       void Start()\n       {\n           ros = ROSConnection.GetOrCreateInstance();\n\n           // Set up camera\n           cam = GetComponent<Camera>();\n           if (cam == null)\n           {\n               cam = gameObject.AddComponent<Camera>();\n           }\n           cam.fieldOfView = fieldOfView;\n\n           // Create render texture\n           renderTexture = new RenderTexture(width, height, 24);\n           cam.targetTexture = renderTexture;\n\n           // Create texture for reading pixels\n           texture2D = new Texture2D(width, height, TextureFormat.RGB24, false);\n       }\n\n       void Update()\n       {\n           // Capture image and publish to ROS\n           PublishCameraImage();\n       }\n\n       void PublishCameraImage()\n       {\n           // Set the active render texture and read pixels\n           RenderTexture.active = renderTexture;\n           texture2D.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n           texture2D.Apply();\n\n           // Convert texture to byte array (simplified - in practice you'd handle image encoding)\n           byte[] imageData = texture2D.EncodeToPNG();\n\n           // Create and publish ROS message\n           var imageMsg = new Sensor_msgs.ImageMsg();\n           imageMsg.header = new Std_msgs.HeaderMsg();\n           imageMsg.header.frame_id = transform.name;\n           imageMsg.header.stamp = new Builtin_interfaces.TimeMsg();\n           imageMsg.height = (uint)height;\n           imageMsg.width = (uint)width;\n           imageMsg.encoding = \"rgb8\";\n           imageMsg.is_bigendian = 0;\n           imageMsg.step = (uint)(width * 3); // 3 bytes per pixel for RGB\n           imageMsg.data = imageData;\n\n           ros.Publish(topicName, imageMsg);\n       }\n   }\n   ```\n\n2. Attach the script to a camera in your Unity scene\n3. Configure the ROS connection settings\n4. Test that the camera publishes images to ROS\n\n### Expected Outcome\nA Unity camera that captures images and publishes them to a ROS topic for use in robotics applications.\n\n## Exercise 5: Digital Twin Integration\n\n### Objective\nCreate a complete digital twin scenario with both Gazebo and Unity representations of the same robot.\n\n### Steps\n1. Create a humanoid robot URDF with multiple joints and sensors\n2. Set up a Gazebo world with the robot and obstacles\n3. Create a Unity scene with the same robot model\n4. Implement ROS communication to synchronize state between both simulations\n5. Create a control system that works in both environments\n\n### Unity Synchronization Script\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class RobotStateSynchronizer : MonoBehaviour\n{\n    [Header(\"ROS Configuration\")]\n    [SerializeField] private string jointStateTopic = \"/joint_states\";\n    [SerializeField] private string cmdVelTopic = \"/cmd_vel\";\n\n    private ROSConnection ros;\n    private ArticulationBody[] joints;\n    private string[] jointNames;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe<JointStateMsg>(jointStateTopic, JointStateCallback);\n\n        // Find all articulation bodies (Unity's joint equivalent)\n        joints = GetComponentsInChildren<ArticulationBody>();\n        jointNames = new string[joints.Length];\n\n        for (int i = 0; i < joints.Length; i++)\n        {\n            jointNames[i] = joints[i].name;\n        }\n    }\n\n    void JointStateCallback(JointStateMsg jointState)\n    {\n        for (int i = 0; i < jointState.name.Length; i++)\n        {\n            int jointIndex = System.Array.IndexOf(jointNames, jointState.name[i]);\n            if (jointIndex >= 0 && jointIndex < joints.Length)\n            {\n                // Update joint position in Unity\n                ArticulationDrive drive = joints[jointIndex].jointDrive;\n                drive.target = jointState.position[i] * Mathf.Rad2Deg; // Convert radians to degrees\n                joints[jointIndex].jointDrive = drive;\n            }\n        }\n    }\n}\n```\n\n### Expected Outcome\nA synchronized digital twin where robot movements in Gazebo are reflected in Unity and vice versa, with both simulations representing the same physical system.\n\n## Exercise 6: Performance Optimization Challenge\n\n### Objective\nOptimize your simulation for real-time performance while maintaining accuracy.\n\n### Steps\n1. Profile your simulation using Unity's Profiler or Gazebo's built-in tools\n2. Identify performance bottlenecks (physics, rendering, ROS communication)\n3. Implement optimization techniques:\n   - Reduce polygon count for visual meshes\n   - Use simpler collision geometries\n   - Adjust physics solver settings\n   - Optimize ROS message frequency\n   - Implement level-of-detail (LOD) systems\n\n### Expected Outcome\nA simulation that runs at real-time speed (30+ FPS) while maintaining the necessary accuracy for robotics applications.\n\n## Troubleshooting Tips\n\n### Gazebo Issues\n- If the robot falls through the ground: Check collision geometry and mass properties\n- If joints are unstable: Increase physics solver iterations or adjust joint damping\n- If simulation is slow: Simplify collision meshes or reduce physics update rate\n\n### Unity Issues\n- If physics are unstable: Check mass ratios and adjust solver iterations\n- If ROS connection fails: Verify IP address and port settings\n- If sensors don't work: Check layer masks and collision settings\n\n## Extension Challenges\n\n1. **Advanced Navigation**: Implement path planning and obstacle avoidance in both simulators\n2. **Multi-Robot Simulation**: Create a scenario with multiple robots coordinating\n3. **Dynamic Environments**: Add moving obstacles or changing environmental conditions\n4. **Machine Learning Integration**: Use the simulation for training RL agents\n5. **Haptic Feedback**: Implement force feedback for human-in-the-loop simulation\n\nComplete these exercises to solidify your understanding of digital twin simulation before moving to the next module.",
    "path": "module-2-digital-twin\\practical-exercises.md",
    "description": ""
  },
  "module-2-digital-twin\\sensor-simulation": {
    "title": "module-2-digital-twin\\sensor-simulation",
    "content": "# Sensor Simulation\n\nSensor simulation is a critical component of digital twin technology, enabling robots to perceive their virtual environment just as they would in the real world. This section covers simulating various types of sensors in both Gazebo and Unity environments.\n\n## Overview of Sensor Simulation\n\nRobotic sensors provide the robot with information about its environment and internal state. In simulation, we must accurately model:\n- **Physical sensing**: How the sensor would detect real-world phenomena\n- **Noise characteristics**: Real sensors have inherent noise and uncertainty\n- **Latency**: Processing delays in real sensor systems\n- **Field of view**: Physical limitations of sensor coverage\n- **Range limitations**: Maximum and minimum detection distances\n\n## Types of Sensors\n\n### Range Sensors\n- **LIDAR**: 2D/3D laser range finders\n- **Ultrasonic**: Sound-based distance measurement\n- **Infrared**: Infrared-based proximity detection\n\n### Vision Sensors\n- **Cameras**: RGB, depth, thermal imaging\n- **Stereo cameras**: 3D vision capabilities\n- **Event cameras**: High-speed dynamic vision\n\n### Inertial Sensors\n- **IMU**: Inertial measurement units\n- **Accelerometers**: Linear acceleration\n- **Gyroscopes**: Angular velocity\n- **Magnetometers**: Magnetic field detection\n\n### Force/Torque Sensors\n- **Force sensors**: Linear forces\n- **Torque sensors**: Rotational forces\n- **FT sensors**: Combined force/torque measurement\n\n## Sensor Simulation in Gazebo\n\n### Camera Simulation\nGazebo provides realistic camera simulation through the gazebo_ros_camera plugin:\n\n```xml\n<link name=\"camera_link\">\n  <visual>\n    <geometry>\n      <box size=\"0.05 0.05 0.05\"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <box size=\"0.05 0.05 0.05\"/>\n    </geometry>\n  </collision>\n</link>\n\n<gazebo reference=\"camera_link\">\n  <sensor name=\"camera\" type=\"camera\">\n    <update_rate>30</update_rate>\n    <camera name=\"head\">\n      <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\">\n      <frame_name>camera_link</frame_name>\n      <topic_name>image_raw</topic_name>\n      <camera_info_topic_name>camera_info</camera_info_topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n### LIDAR Simulation\nSimulate 2D and 3D LIDAR sensors:\n\n```xml\n<gazebo reference=\"lidar_link\">\n  <sensor name=\"lidar\" type=\"ray\">\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle> <!-- - radians -->\n          <max_angle>3.14159</max_angle>   <!--  radians -->\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name=\"lidar_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n      <ros>\n        <namespace>laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n### Depth Camera Simulation\n```xml\n<gazebo reference=\"depth_camera_link\">\n  <sensor name=\"depth_camera\" type=\"depth\">\n    <update_rate>30</update_rate>\n    <camera name=\"depth_cam\">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10</far>\n      </clip>\n    </camera>\n    <plugin name=\"depth_camera_controller\" filename=\"libgazebo_ros_openni_kinect.so\">\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>10.0</updateRate>\n      <cameraName>depth_camera</cameraName>\n      <imageTopicName>/rgb/image_raw</imageTopicName>\n      <depthImageTopicName>/depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>/depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>/rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>/depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>depth_camera_optical_frame</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.5</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n    </plugin>\n  </sensor>\n</gazebo>\n```\n\n### IMU Simulation\n```xml\n<gazebo reference=\"imu_link\">\n  <sensor name=\"imu_sensor\" type=\"imu\">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type=\"gaussian\">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n  </sensor>\n</gazebo>\n```\n\n## Sensor Simulation in Unity\n\n### Camera Simulation with Perception Package\nUnity's Perception package provides advanced camera simulation:\n\n```csharp\nusing UnityEngine;\nusing Unity.Perception.GroundTruth;\nusing Unity.Simulation;\n\npublic class CameraSensorSetup : MonoBehaviour\n{\n    [Header(\"Camera Properties\")]\n    [SerializeField] private int width = 640;\n    [SerializeField] private int height = 480;\n    [SerializeField] private float fieldOfView = 60f;\n    [SerializeField] private float nearClip = 0.1f;\n    [SerializeField] private float farClip = 100f;\n\n    [Header(\"Sensor Properties\")]\n    [SerializeField] private string sensorId = \"camera_0\";\n    [SerializeField] private string rosTopic = \"/camera/image_raw\";\n\n    private Camera cam;\n    private SyntheticCameraData syntheticCamera;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        if (cam == null)\n        {\n            cam = gameObject.AddComponent<Camera>();\n        }\n\n        ConfigureCamera();\n        SetupSyntheticCamera();\n    }\n\n    void ConfigureCamera()\n    {\n        cam.fieldOfView = fieldOfView;\n        cam.nearClipPlane = nearClip;\n        cam.farClipPlane = farClip;\n        cam.targetTexture = new RenderTexture(width, height, 24);\n    }\n\n    void SetupSyntheticCamera()\n    {\n        syntheticCamera = GetComponent<SyntheticCameraData>();\n        if (syntheticCamera == null)\n        {\n            syntheticCamera = gameObject.AddComponent<SyntheticCameraData>();\n        }\n\n        syntheticCamera.camera = cam;\n        syntheticCamera.sensorId = sensorId;\n    }\n}\n```\n\n### LIDAR Simulation in Unity\nCreate a LIDAR sensor using raycasting:\n\n```csharp\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class UnityLidarSensor : MonoBehaviour\n{\n    [Header(\"LIDAR Properties\")]\n    [SerializeField] private int horizontalRays = 360;\n    [SerializeField] private int verticalRays = 1;\n    [SerializeField] private float maxDistance = 10f;\n    [SerializeField] private float minDistance = 0.1f;\n    [SerializeField] private string topicName = \"/scan\";\n    [SerializeField] private float updateRate = 10f; // Hz\n\n    private float[] ranges;\n    private ROSConnection ros;\n    private float updateInterval;\n    private float lastUpdateTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        updateInterval = 1.0f / updateRate;\n        lastUpdateTime = 0;\n\n        // Initialize ranges array\n        ranges = new float[horizontalRays * verticalRays];\n    }\n\n    void Update()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            UpdateLidarScan();\n            PublishScan();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    void UpdateLidarScan()\n    {\n        for (int v = 0; v < verticalRays; v++)\n        {\n            float vAngle = (v - (verticalRays - 1) / 2.0f) * 0.1f; // Vertical spread\n\n            for (int h = 0; h < horizontalRays; h++)\n            {\n                float hAngle = (h * 2 * Mathf.PI) / horizontalRays;\n\n                Vector3 direction = new Vector3(\n                    Mathf.Cos(vAngle) * Mathf.Cos(hAngle),\n                    Mathf.Sin(vAngle),\n                    Mathf.Cos(vAngle) * Mathf.Sin(hAngle)\n                );\n\n                if (Physics.Raycast(transform.position, direction, out RaycastHit hit, maxDistance))\n                {\n                    ranges[v * horizontalRays + h] = hit.distance;\n                }\n                else\n                {\n                    ranges[v * horizontalRays + h] = float.PositiveInfinity;\n                }\n            }\n        }\n    }\n\n    void PublishScan()\n    {\n        var laserScan = new LaserScanMsg\n        {\n            header = new std_msgs.HeaderMsg\n            {\n                stamp = new builtin_interfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = transform.name\n            },\n            angle_min = -Mathf.PI,\n            angle_max = Mathf.PI,\n            angle_increment = (2 * Mathf.PI) / horizontalRays,\n            time_increment = 0,\n            scan_time = 1.0f / updateRate,\n            range_min = minDistance,\n            range_max = maxDistance,\n            ranges = ranges\n        };\n\n        ros.Publish(topicName, laserScan);\n    }\n}\n```\n\n### IMU Simulation in Unity\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class UnityImuSensor : MonoBehaviour\n{\n    [Header(\"IMU Properties\")]\n    [SerializeField] private string topicName = \"/imu/data\";\n    [SerializeField] private float updateRate = 100f; // Hz\n    [SerializeField] private float noiseLevel = 0.01f;\n\n    private ROSConnection ros;\n    private float updateInterval;\n    private float lastUpdateTime;\n    private Rigidbody attachedRigidbody;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        updateInterval = 1.0f / updateRate;\n        lastUpdateTime = 0;\n\n        // Try to find attached rigidbody\n        attachedRigidbody = GetComponent<Rigidbody>();\n        if (attachedRigidbody == null)\n        {\n            attachedRigidbody = GetComponentInParent<Rigidbody>();\n        }\n    }\n\n    void Update()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            PublishImuData();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    void PublishImuData()\n    {\n        var imuMsg = new ImuMsg\n        {\n            header = new std_msgs.HeaderMsg\n            {\n                stamp = new builtin_interfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = transform.name\n            }\n        };\n\n        // Set orientation (from Unity rotation to quaternion)\n        Quaternion unityRotation = transform.rotation;\n        // Convert Unity coordinate system to ROS coordinate system\n        imuMsg.orientation = new geometry_msgs.QuaternionMsg\n        {\n            x = unityRotation.x,\n            y = unityRotation.y,\n            z = unityRotation.z,\n            w = unityRotation.w\n        };\n\n        // Set angular velocity\n        if (attachedRigidbody != null)\n        {\n            Vector3 angularVel = attachedRigidbody.angularVelocity;\n            imuMsg.angular_velocity = new geometry_msgs.Vector3Msg\n            {\n                x = angularVel.x + Random.Range(-noiseLevel, noiseLevel),\n                y = angularVel.y + Random.Range(-noiseLevel, noiseLevel),\n                z = angularVel.z + Random.Range(-noiseLevel, noiseLevel)\n            };\n\n            // Set linear acceleration\n            Vector3 linearAcc = attachedRigidbody.velocity / Time.fixedDeltaTime;\n            imuMsg.linear_acceleration = new geometry_msgs.Vector3Msg\n            {\n                x = linearAcc.x + Random.Range(-noiseLevel, noiseLevel),\n                y = linearAcc.y + Random.Range(-noiseLevel, noiseLevel),\n                z = linearAcc.z + Random.Range(-noiseLevel, noiseLevel)\n            };\n        }\n\n        ros.Publish(topicName, imuMsg);\n    }\n}\n```\n\n## Sensor Fusion and Calibration\n\n### Multi-Sensor Integration\nCombine data from multiple sensors for enhanced perception:\n\n```csharp\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class SensorFusion : MonoBehaviour\n{\n    [SerializeField] private List<GameObject> sensors;\n    private Dictionary<string, object> sensorData;\n\n    void Start()\n    {\n        sensorData = new Dictionary<string, object>();\n    }\n\n    void Update()\n    {\n        // Collect data from all sensors\n        foreach (var sensor in sensors)\n        {\n            ISensorDataProvider provider = sensor.GetComponent<ISensorDataProvider>();\n            if (provider != null)\n            {\n                sensorData[provider.GetSensorId()] = provider.GetData();\n            }\n        }\n\n        // Process fused sensor data\n        ProcessFusedData();\n    }\n\n    void ProcessFusedData()\n    {\n        // Implement sensor fusion algorithms (Kalman filters, particle filters, etc.)\n        // Combine data from different sensors for better accuracy\n    }\n}\n\npublic interface ISensorDataProvider\n{\n    string GetSensorId();\n    object GetData();\n}\n```\n\n### Sensor Calibration\nSimulate sensor calibration procedures:\n\n```csharp\nusing UnityEngine;\n\npublic class SensorCalibration : MonoBehaviour\n{\n    [Header(\"Calibration Parameters\")]\n    [SerializeField] private float calibrationDuration = 5.0f;\n    [SerializeField] private float calibrationInterval = 0.1f;\n\n    private bool isCalibrating = false;\n    private float calibrationStartTime = 0f;\n\n    public void StartCalibration()\n    {\n        isCalibrating = true;\n        calibrationStartTime = Time.time;\n    }\n\n    void Update()\n    {\n        if (isCalibrating)\n        {\n            if (Time.time - calibrationStartTime >= calibrationDuration)\n            {\n                CompleteCalibration();\n            }\n            else\n            {\n                PerformCalibrationStep();\n            }\n        }\n    }\n\n    void PerformCalibrationStep()\n    {\n        // Perform calibration calculations\n        // Adjust sensor parameters based on calibration data\n    }\n\n    void CompleteCalibration()\n    {\n        isCalibrating = false;\n        Debug.Log(\"Calibration completed\");\n    }\n}\n```\n\n## Sensor Noise and Uncertainty\n\n### Adding Realistic Noise\nReal sensors have inherent noise and uncertainty:\n\n```csharp\nusing UnityEngine;\n\npublic class SensorNoise : MonoBehaviour\n{\n    [Header(\"Noise Parameters\")]\n    [SerializeField] private float gaussianNoiseStdDev = 0.01f;\n    [SerializeField] private float bias = 0.0f;\n    [SerializeField] private float driftRate = 0.001f;\n\n    private float currentBias = 0f;\n\n    public float ApplyNoise(float rawValue)\n    {\n        // Add Gaussian noise\n        float gaussianNoise = RandomGaussian() * gaussianNoiseStdDev;\n\n        // Add bias\n        float biasedValue = rawValue + bias + currentBias;\n\n        // Add noise\n        float noisyValue = biasedValue + gaussianNoise;\n\n        // Update drift\n        currentBias += Random.Range(-driftRate, driftRate) * Time.deltaTime;\n\n        return noisyValue;\n    }\n\n    float RandomGaussian()\n    {\n        // Box-Muller transform for Gaussian random numbers\n        float u1 = Random.value;\n        float u2 = Random.value;\n        return Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Cos(2.0f * Mathf.PI * u2);\n    }\n}\n```\n\n## Best Practices for Sensor Simulation\n\n### Performance Optimization\n- Use appropriate update rates for each sensor type\n- Implement level-of-detail for sensor processing\n- Use occlusion culling for vision sensors\n- Cache expensive calculations when possible\n\n### Realism vs. Performance\n- Balance physical accuracy with simulation speed\n- Use simplified models for distant objects\n- Implement adaptive resolution based on importance\n\n### Validation\n- Compare simulation results with real sensor data\n- Validate noise characteristics match real sensors\n- Test edge cases and failure modes\n\n## Troubleshooting Common Issues\n\n### Sensor Data Quality\n- Check coordinate frame alignment between sensors\n- Verify proper TF transforms are published\n- Ensure sensor mounting positions are accurate\n\n### Performance Issues\n- Reduce ray count for LIDAR sensors if performance is poor\n- Use lower resolution cameras for faster processing\n- Implement sensor frustum culling\n\n## Exercise\n\nCreate a complete sensor simulation setup for your humanoid robot that includes:\n1. A RGB camera with realistic noise characteristics\n2. A 2D LIDAR sensor for navigation\n3. An IMU for orientation and motion sensing\n4. A fusion algorithm that combines sensor data for improved accuracy\n\nTest the sensor setup in various environments and validate that the simulated data matches expected real-world sensor behavior.",
    "path": "module-2-digital-twin\\sensor-simulation.md",
    "description": ""
  },
  "module-2-digital-twin\\unity-scene-setup": {
    "title": "module-2-digital-twin\\unity-scene-setup",
    "content": "# Unity Scene Setup and Physics Configuration\n\nUnity provides a powerful environment for creating high-fidelity digital twins with advanced graphics and physics capabilities. This section covers setting up Unity scenes for humanoid robotics simulation with proper physics configuration.\n\n## Unity Project Setup for Robotics\n\n### Initial Project Configuration\nWhen creating a Unity project for robotics applications:\n\n1. **Project Template**: Start with 3D template\n2. **Target Platform**: Consider your deployment needs (PC, VR, mobile)\n3. **Scripting Runtime**: Use .NET Standard 2.1 or higher\n4. **API Compatibility**: .NET Framework for Windows applications\n\n### Required Packages\nInstall these packages via the Unity Package Manager (Window  Package Manager):\n\n```csharp\n// Key packages for robotics simulation\n- com.unity.robotics.ros-tcp-connector: ROS communication\n- com.unity.robotics.urdf-importer: URDF model import\n- com.unity.perception: Synthetic data generation\n- com.unity.physics: Advanced physics simulation\n- com.unity.xr: If using VR/AR\n```\n\n### Project Settings Configuration\nConfigure Project Settings for robotics simulation:\n\n```csharp\n// Edit  Project Settings  Time\n- Fixed Timestep: 0.02 (50 Hz) for physics simulation\n- Maximum Allowed Timestep: 0.333 (30 FPS cap)\n- Maximum Particle Timestep: 0.03\n\n// Edit  Project Settings  Physics\n- Gravity: (0, -9.81, 0) for Earth-like gravity\n- Default Material: Create custom material with appropriate friction\n- Bounce Threshold: 2 (velocity threshold for bounce)\n- Contact Offset: 0.01 (contact distance threshold)\n- Solver Iterations: 6 (accuracy vs performance)\n- Solver Velocity Iterations: 1 (velocity accuracy)\n```\n\n## Scene Architecture\n\n### Basic Scene Structure\nCreate a well-organized scene hierarchy:\n\n```\nScene Root\n Environment\n    Ground\n    Walls\n    Obstacles\n    Lighting\n Robots\n    HumanoidRobot\n       BaseLink\n       Links (joints and bodies)\n       Sensors\n    OtherRobots...\n Controllers\n    ROSConnection\n    RobotController\n    SceneController\n Managers\n     PhysicsManager\n     VisualizationManager\n     CommunicationManager\n```\n\n### Environment Setup\n\n#### Ground Plane\nCreate a realistic ground plane with proper physics properties:\n\n```csharp\nusing UnityEngine;\n\npublic class GroundPlaneSetup : MonoBehaviour\n{\n    [Header(\"Ground Properties\")]\n    [SerializeField] private PhysicMaterial groundMaterial;\n    [SerializeField] private float size = 100f;\n    [SerializeField] private float friction = 0.5f;\n    [SerializeField] private float bounciness = 0.1f;\n\n    void Start()\n    {\n        // Create ground plane\n        GameObject ground = GameObject.CreatePrimitive(PrimitiveType.Plane);\n        ground.transform.SetParent(transform);\n        ground.transform.localPosition = Vector3.zero;\n        ground.transform.localScale = Vector3.one * (size / 10f); // Plane is 10 units by default\n\n        // Configure collider\n        var collider = ground.GetComponent<Collider>();\n        if (collider != null)\n        {\n            if (groundMaterial == null)\n            {\n                groundMaterial = new PhysicMaterial(\"GroundMaterial\");\n                groundMaterial.staticFriction = friction;\n                groundMaterial.dynamicFriction = friction;\n                groundMaterial.bounciness = bounciness;\n            }\n            collider.material = groundMaterial;\n        }\n\n        // Add visual material\n        Renderer renderer = ground.GetComponent<Renderer>();\n        if (renderer != null)\n        {\n            renderer.material = CreateGroundMaterial();\n        }\n    }\n\n    Material CreateGroundMaterial()\n    {\n        Material groundMat = new Material(Shader.Find(\"Standard\"));\n        groundMat.color = Color.gray;\n        groundMat.SetFloat(\"_Metallic\", 0.1f);\n        groundMat.SetFloat(\"_Smoothness\", 0.2f);\n        return groundMat;\n    }\n}\n```\n\n#### Lighting Configuration\nSet up realistic lighting for the scene:\n\n```csharp\nusing UnityEngine;\n\npublic class LightingSetup : MonoBehaviour\n{\n    [Header(\"Lighting Configuration\")]\n    [SerializeField] private LightType lightType = LightType.Directional;\n    [SerializeField] private Color lightColor = Color.white;\n    [SerializeField] private float intensity = 1.0f;\n    [SerializeField] private Vector3 lightDirection = new Vector3(-0.5f, -1f, -0.5f);\n\n    void Start()\n    {\n        // Create main light\n        GameObject lightObj = new GameObject(\"Main Light\");\n        lightObj.transform.SetParent(transform);\n        lightObj.transform.position = new Vector3(0, 10, 0);\n\n        Light light = lightObj.AddComponent<Light>();\n        light.type = lightType;\n        light.color = lightColor;\n        light.intensity = intensity;\n        light.transform.rotation = Quaternion.LookRotation(lightDirection);\n\n        // Add ambient lighting\n        RenderSettings.ambientLight = new Color(0.2f, 0.2f, 0.2f, 1f);\n        RenderSettings.ambientMode = UnityEngine.Rendering.AmbientMode.Trilight;\n    }\n}\n```\n\n## Physics Configuration\n\n### Physics Manager Settings\nConfigure the Physics Manager for robotics simulation:\n\n```csharp\n// This would be configured through Edit  Project Settings  Physics\n/*\nPhysics Settings for Robotics:\n- Gravity: (0, -9.81, 0) m/s\n- Default Material: Custom material with realistic friction\n- Bounce Threshold: 2 m/s\n- Sleep Threshold: 0.005 (keep objects from sleeping too early for control)\n- Default Contact Offset: 0.01\n- Solver Iteration Count: 6-10 (balance accuracy and performance)\n- Solver Velocity Iteration Count: 1\n- Queries Hit Backfaces: False (for sensors)\n- Queries Hit Triggers: True (for detection zones)\n- Enable Adaptive Force: False (for consistent control)\n- Layer Collision Matrix: Configure for robot self-collision if needed\n*/\n```\n\n### Custom Physics Material\nCreate realistic physics materials for robot components:\n\n```csharp\nusing UnityEngine;\n\n[CreateAssetMenu(fileName = \"RobotPhysicsMaterial\", menuName = \"Robotics/Physics Material\")]\npublic class RobotPhysicsMaterial : ScriptableObject\n{\n    [Header(\"Friction Properties\")]\n    public float staticFriction = 0.5f;\n    public float dynamicFriction = 0.4f;\n    [Range(0, 1)] public float frictionCombine = 0; // 0=Average, 1=Minimum, 2=Maximum, 3=Multiply\n\n    [Header(\"Bounce Properties\")]\n    [Range(0, 1)] public float bounciness = 0.1f;\n    [Range(0, 1)] public float bounceCombine = 0; // Same options as frictionCombine\n\n    [Header(\"Simulation Properties\")]\n    public bool enableAdaptiveForce = false;\n    public float solverIterations = 6f;\n    public float solverVelocityIterations = 1f;\n\n    public PhysicMaterial CreatePhysicMaterial(string name = \"RobotMaterial\")\n    {\n        PhysicMaterial material = new PhysicMaterial(name);\n        material.staticFriction = staticFriction;\n        material.dynamicFriction = dynamicFriction;\n        material.frictionCombine = GetCombineMode(frictionCombine);\n        material.bounciness = bounciness;\n        material.bounceCombine = GetCombineMode(bounceCombine);\n\n        return material;\n    }\n\n    PhysicMaterialCombine GetCombineMode(float value)\n    {\n        if (value < 0.33f) return PhysicMaterialCombine.Average;\n        if (value < 0.66f) return PhysicMaterialCombine.Minimum;\n        if (value < 1.0f) return PhysicMaterialCombine.Maximum;\n        return PhysicMaterialCombine.Multiply;\n    }\n}\n```\n\n## Robot Integration\n\n### Robot Prefab Structure\nCreate a well-structured robot prefab:\n\n```csharp\nusing UnityEngine;\n\npublic class RobotStructure : MonoBehaviour\n{\n    [Header(\"Robot Configuration\")]\n    public string robotName = \"HumanoidRobot\";\n    public float massMultiplier = 1.0f;\n\n    [Header(\"Joint Configuration\")]\n    public float defaultDamping = 0.1f;\n    public float defaultSpring = 10000f;\n    public float defaultMaxForce = 100f;\n\n    [Header(\"Sensor Configuration\")]\n    public Transform[] sensorPoints;\n\n    void Start()\n    {\n        ConfigureRobotPhysics();\n        ConfigureJoints();\n    }\n\n    void ConfigureRobotPhysics()\n    {\n        // Apply consistent physics properties to all rigidbodies\n        Rigidbody[] rigidbodies = GetComponentsInChildren<Rigidbody>();\n        foreach (Rigidbody rb in rigidbodies)\n        {\n            rb.mass *= massMultiplier;\n            rb.drag = 0.01f;\n            rb.angularDrag = 0.05f;\n            rb.sleepThreshold = 0.005f;\n            rb.interpolation = RigidbodyInterpolation.Interpolate;\n        }\n    }\n\n    void ConfigureJoints()\n    {\n        // Configure all joints with realistic properties\n        ConfigurableJoint[] joints = GetComponentsInChildren<ConfigurableJoint>();\n        foreach (ConfigurableJoint joint in joints)\n        {\n            ConfigureJoint(joint);\n        }\n    }\n\n    void ConfigureJoint(ConfigurableJoint joint)\n    {\n        // Configure linear limits\n        SoftJointLimit linearLimit = joint.linearLimit;\n        linearLimit.limit = 0.0f; // Fixed position constraint\n        joint.linearLimit = linearLimit;\n\n        // Configure angular limits\n        SoftJointLimit lowAngularXLimit = joint.lowAngularXLimit;\n        lowAngularXLimit.limit = -45f * Mathf.Deg2Rad; // Convert to radians\n        joint.lowAngularXLimit = lowAngularXLimit;\n\n        SoftJointLimit highAngularXLimit = joint.highAngularXLimit;\n        highAngularXLimit.limit = 45f * Mathf.Deg2Rad;\n        joint.highAngularXLimit = highAngularXLimit;\n\n        // Configure drive for control\n        JointDrive angularXDrive = joint.angularXDrive;\n        angularXDrive.mode = JointDriveMode.PositionAndVelocity;\n        angularXDrive.positionSpring = defaultSpring;\n        angularXDrive.positionDamper = defaultDamping;\n        angularXDrive.maximumForce = defaultMaxForce;\n        joint.angularXDrive = angularXDrive;\n    }\n}\n```\n\n### Sensor Integration\nIntegrate sensors with proper physics configuration:\n\n```csharp\nusing UnityEngine;\n\npublic class RobotSensorSetup : MonoBehaviour\n{\n    [Header(\"Camera Sensors\")]\n    public Camera[] cameras;\n    [SerializeField] private string cameraTopicPrefix = \"/camera\";\n\n    [Header(\"LIDAR Sensors\")]\n    public Transform[] lidarPoints;\n    [SerializeField] private string lidarTopicPrefix = \"/lidar\";\n\n    [Header(\"IMU Sensors\")]\n    public Transform[] imuPoints;\n    [SerializeField] private string imuTopicPrefix = \"/imu\";\n\n    void Start()\n    {\n        ConfigureCameras();\n        ConfigureLIDAR();\n        ConfigureIMUs();\n    }\n\n    void ConfigureCameras()\n    {\n        for (int i = 0; i < cameras.Length; i++)\n        {\n            Camera cam = cameras[i];\n            if (cam != null)\n            {\n                // Configure camera for realistic vision\n                cam.allowMSAA = true;\n                cam.allowDynamicResolution = true;\n\n                // Add perception components if using Unity Perception package\n                var syntheticCamera = cam.gameObject.AddComponent<Unity.Perception.GroundTruth.SyntheticCameraData>();\n                syntheticCamera.sensorId = $\"{cameraTopicPrefix}_{i}\";\n            }\n        }\n    }\n\n    void ConfigureLIDAR()\n    {\n        // LIDAR typically uses raycasting, configure collision layers\n        foreach (Transform lidarPoint in lidarPoints)\n        {\n            // Configure the lidar point's collision detection\n            // Set up appropriate layers for raycasting\n        }\n    }\n\n    void ConfigureIMUs()\n    {\n        // Configure IMU sensors with appropriate physics properties\n        foreach (Transform imuPoint in imuPoints)\n        {\n            // Add IMU-specific components\n            var imuSensor = imuPoint.gameObject.AddComponent<IMUMockup>();\n            imuSensor.Configure(this, imuPoint.name);\n        }\n    }\n}\n\n// Mock IMU component for demonstration\npublic class IMUMockup : MonoBehaviour\n{\n    private RobotSensorSetup robot;\n    private string sensorName;\n\n    public void Configure(RobotSensorSetup robotSetup, string name)\n    {\n        robot = robotSetup;\n        sensorName = name;\n    }\n\n    void Update()\n    {\n        // Simulate IMU data\n        // This would typically interface with ROS through the TCP connector\n    }\n}\n```\n\n## Scene Management\n\n### Scene Controller\nCreate a scene controller for managing simulation state:\n\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class SceneController : MonoBehaviour\n{\n    [Header(\"Simulation Control\")]\n    [SerializeField] private float simulationSpeed = 1.0f;\n    [SerializeField] private bool useFixedTimeStep = true;\n    [SerializeField] private float fixedTimeStep = 0.02f; // 50 Hz\n\n    [Header(\"ROS Connection\")]\n    [SerializeField] private string rosIPAddress = \"127.0.0.1\";\n    [SerializeField] private int rosPort = 10000;\n\n    private ROSConnection rosConnection;\n    private bool isSimulationRunning = false;\n\n    void Start()\n    {\n        SetupROSLifecycle();\n        ConfigureTimeStep();\n    }\n\n    void SetupROSLifecycle()\n    {\n        // Initialize ROS connection\n        rosConnection = ROSConnection.GetOrCreateInstance();\n        rosConnection.rosIPAddress = rosIPAddress;\n        rosConnection.rosPort = rosPort;\n\n        // Register shutdown callback\n        Application.quitting += OnApplicationQuit;\n    }\n\n    void ConfigureTimeStep()\n    {\n        if (useFixedTimeStep)\n        {\n            Time.fixedDeltaTime = fixedTimeStep;\n        }\n    }\n\n    void OnApplicationQuit()\n    {\n        // Clean up ROS connection\n        if (rosConnection != null)\n        {\n            rosConnection.Close();\n        }\n    }\n\n    public void StartSimulation()\n    {\n        isSimulationRunning = true;\n        // Publish simulation start message to ROS\n    }\n\n    public void StopSimulation()\n    {\n        isSimulationRunning = false;\n        // Publish simulation stop message to ROS\n    }\n\n    public void ResetSimulation()\n    {\n        // Reset robot positions, clear data, etc.\n        ResetRobots();\n    }\n\n    void ResetRobots()\n    {\n        RobotStructure[] robots = FindObjectsOfType<RobotStructure>();\n        foreach (RobotStructure robot in robots)\n        {\n            // Reset robot to initial configuration\n            ResetRobotToInitialPosition(robot);\n        }\n    }\n\n    void ResetRobotToInitialPosition(RobotStructure robot)\n    {\n        // Store initial positions and reset to them\n        Transform robotTransform = robot.transform;\n        robotTransform.position = robotTransform.position; // Or use stored initial position\n        robotTransform.rotation = robotTransform.rotation; // Or use stored initial rotation\n\n        // Reset all joint positions\n        ConfigurableJoint[] joints = robot.GetComponentsInChildren<ConfigurableJoint>();\n        foreach (ConfigurableJoint joint in joints)\n        {\n            // Reset joint to initial configuration\n        }\n    }\n}\n```\n\n## Performance Optimization\n\n### Physics Optimization\nConfigure physics for optimal performance:\n\n```csharp\nusing UnityEngine;\n\npublic class PhysicsOptimizer : MonoBehaviour\n{\n    [Header(\"Performance Settings\")]\n    [SerializeField] private int solverIterationCount = 6;\n    [SerializeField] private int solverVelocityIterationCount = 1;\n    [SerializeField] private float contactOffset = 0.01f;\n    [SerializeField] private float sleepThreshold = 0.005f;\n\n    [Header(\"Simulation Quality\")]\n    [SerializeField] private bool enableAdaptiveForce = false;\n    [SerializeField] private bool enableContinuousCollisionDetection = false;\n    [SerializeField] private CollisionDetectionMode collisionDetectionMode = CollisionDetectionMode.Discrete;\n\n    void Start()\n    {\n        OptimizePhysicsSettings();\n    }\n\n    void OptimizePhysicsSettings()\n    {\n        Physics.defaultSolverIterations = solverIterationCount;\n        Physics.defaultSolverVelocityIterations = solverVelocityIterationCount;\n        Physics.defaultContactOffset = contactOffset;\n        Physics.sleepThreshold = sleepThreshold;\n        Physics.defaultUseAdaptiveForce = enableAdaptiveForce;\n        Physics.defaultSolverContactOffset = contactOffset;\n\n        // Apply settings to all rigidbodies in the scene\n        Rigidbody[] allRigidbodies = FindObjectsOfType<Rigidbody>();\n        foreach (Rigidbody rb in allRigidbodies)\n        {\n            rb.collisionDetectionMode = enableContinuousCollisionDetection ?\n                CollisionDetectionMode.Continuous : collisionDetectionMode;\n        }\n    }\n}\n```\n\n### Level of Detail (LOD) for Complex Robots\nImplement LOD for complex robot models:\n\n```csharp\nusing UnityEngine;\n\npublic class RobotLODManager : MonoBehaviour\n{\n    [System.Serializable]\n    public class LODLevel\n    {\n        public string name;\n        public float distance;\n        public GameObject[] objects;\n    }\n\n    [SerializeField] private LODLevel[] lodLevels;\n    [SerializeField] private Transform cameraTransform;\n\n    private int currentLOD = 0;\n\n    void Start()\n    {\n        if (cameraTransform == null)\n        {\n            cameraTransform = Camera.main.transform;\n        }\n    }\n\n    void Update()\n    {\n        UpdateLOD();\n    }\n\n    void UpdateLOD()\n    {\n        float distance = Vector3.Distance(transform.position, cameraTransform.position);\n\n        int newLOD = 0;\n        for (int i = 0; i < lodLevels.Length; i++)\n        {\n            if (distance > lodLevels[i].distance)\n            {\n                newLOD = i;\n            }\n            else\n            {\n                break;\n            }\n        }\n\n        if (newLOD != currentLOD)\n        {\n            SetLOD(newLOD);\n            currentLOD = newLOD;\n        }\n    }\n\n    void SetLOD(int lodIndex)\n    {\n        for (int i = 0; i < lodLevels.Length; i++)\n        {\n            bool isActive = i == lodIndex;\n            foreach (GameObject obj in lodLevels[i].objects)\n            {\n                if (obj != null)\n                {\n                    obj.SetActive(isActive);\n                }\n            }\n        }\n    }\n}\n```\n\n## Debugging and Visualization\n\n### Physics Debugging Tools\nCreate tools for debugging physics simulation:\n\n```csharp\nusing UnityEngine;\n\npublic class PhysicsDebugger : MonoBehaviour\n{\n    [Header(\"Debug Settings\")]\n    [SerializeField] private bool showColliders = true;\n    [SerializeField] private bool showJoints = true;\n    [SerializeField] private bool showForces = true;\n    [SerializeField] private Color colliderColor = Color.blue;\n    [SerializeField] private Color jointColor = Color.green;\n\n    void OnDrawGizmos()\n    {\n        if (showColliders)\n        {\n            DrawColliders();\n        }\n\n        if (showJoints)\n        {\n            DrawJoints();\n        }\n\n        if (showForces)\n        {\n            DrawForces();\n        }\n    }\n\n    void DrawColliders()\n    {\n        Collider[] colliders = GetComponentsInChildren<Collider>();\n        foreach (Collider col in colliders)\n        {\n            Gizmos.color = colliderColor;\n            if (col is BoxCollider)\n            {\n                BoxCollider box = (BoxCollider)col;\n                Gizmos.matrix = Matrix4x4.TRS(box.transform.position, box.transform.rotation, Vector3.one);\n                Gizmos.DrawWireCube(box.center, box.size);\n            }\n            else if (col is SphereCollider)\n            {\n                SphereCollider sphere = (SphereCollider)col;\n                Gizmos.matrix = Matrix4x4.TRS(box.transform.position, box.transform.rotation, Vector3.one);\n                Gizmos.DrawWireSphere(sphere.center, sphere.radius);\n            }\n            // Add other collider types as needed\n        }\n    }\n\n    void DrawJoints()\n    {\n        ConfigurableJoint[] joints = GetComponentsInChildren<ConfigurableJoint>();\n        foreach (ConfigurableJoint joint in joints)\n        {\n            Gizmos.color = jointColor;\n            Gizmos.DrawWireSphere(joint.transform.position, 0.1f);\n\n            if (joint.connectedBody != null)\n            {\n                Gizmos.DrawLine(joint.transform.position, joint.connectedBody.transform.position);\n            }\n        }\n    }\n\n    void DrawForces()\n    {\n        Rigidbody[] rigidbodies = GetComponentsInChildren<Rigidbody>();\n        foreach (Rigidbody rb in rigidbodies)\n        {\n            Vector3 force = rb.velocity * rb.mass; // Approximate force\n            Gizmos.color = Color.red;\n            Gizmos.DrawRay(rb.position, force.normalized * 0.5f);\n        }\n    }\n}\n```\n\n## Best Practices\n\n### 1. Consistent Units\nAlways use SI units (meters, kilograms, seconds) to match ROS conventions:\n\n```csharp\n// Configuration constants using SI units\npublic static class RobotConstants\n{\n    public const float Gravity = 9.81f; // m/s\n    public const float MassMultiplier = 1.0f; // kg\n    public const float LengthScale = 1.0f; // meters\n    public const float VelocityScale = 1.0f; // m/s\n    public const float TorqueScale = 1.0f; // Nm\n}\n```\n\n### 2. Modularity\nCreate modular components that can be reused:\n\n```csharp\n// Generic sensor base class\npublic abstract class RobotSensor : MonoBehaviour\n{\n    [SerializeField] protected string topicName;\n    [SerializeField] protected string frameId;\n    [SerializeField] protected float updateRate = 30f; // Hz\n\n    protected float updateInterval;\n    protected float lastUpdateTime;\n\n    protected virtual void Start()\n    {\n        updateInterval = 1.0f / updateRate;\n        lastUpdateTime = 0;\n    }\n\n    protected virtual void Update()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            UpdateSensorData();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    protected abstract void UpdateSensorData();\n}\n```\n\n### 3. Error Handling\nImplement robust error handling for ROS connections:\n\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class RobustROSConnection : MonoBehaviour\n{\n    [SerializeField] private string rosIPAddress = \"127.0.0.1\";\n    [SerializeField] private int rosPort = 10000;\n    [SerializeField] private float connectionRetryDelay = 5f;\n\n    private ROSConnection rosConnection;\n    private bool isConnected = false;\n    private float lastConnectionAttempt = 0f;\n\n    void Start()\n    {\n        InitializeROSConnection();\n    }\n\n    void InitializeROSConnection()\n    {\n        try\n        {\n            rosConnection = ROSConnection.GetOrCreateInstance();\n            rosConnection.rosIPAddress = rosIPAddress;\n            rosConnection.rosPort = rosPort;\n            rosConnection.OnConnected += OnConnected;\n            rosConnection.OnDisconnected += OnDisconnected;\n        }\n        catch (System.Exception e)\n        {\n            Debug.LogError($\"Failed to initialize ROS connection: {e.Message}\");\n        }\n    }\n\n    void OnConnected()\n    {\n        isConnected = true;\n        Debug.Log(\"Successfully connected to ROS\");\n        OnROSConnected();\n    }\n\n    void OnDisconnected()\n    {\n        isConnected = false;\n        Debug.LogWarning(\"Disconnected from ROS\");\n        AttemptReconnection();\n    }\n\n    void AttemptReconnection()\n    {\n        if (Time.time - lastConnectionAttempt > connectionRetryDelay)\n        {\n            lastConnectionAttempt = Time.time;\n            InitializeROSConnection();\n        }\n    }\n\n    protected virtual void OnROSConnected()\n    {\n        // Override in derived classes\n    }\n}\n```\n\n## Exercise\n\nCreate a complete Unity scene for humanoid robot simulation that includes:\n1. A well-structured scene hierarchy with proper organization\n2. Physics configuration optimized for humanoid robot simulation\n3. A robot prefab with realistic physical properties\n4. Proper lighting and environment setup\n5. Sensor configurations for cameras, LIDAR, and IMU\n6. Performance optimization techniques\n7. Debugging visualization tools\n\nTest your scene by ensuring the robot behaves realistically with proper physics interactions and can communicate with ROS systems.",
    "path": "module-2-digital-twin\\unity-scene-setup.md",
    "description": ""
  },
  "module-2-digital-twin\\unity-visualization": {
    "title": "module-2-digital-twin\\unity-visualization",
    "content": "# Unity Visualization\n\nUnity provides a powerful game engine environment for high-fidelity robot simulation and visualization. While Gazebo is the standard for ROS simulation, Unity offers advanced graphics capabilities and a rich ecosystem for creating detailed digital twins.\n\n## Introduction to Unity for Robotics\n\nUnity for Robotics provides:\n- High-fidelity visual rendering\n- Advanced physics simulation\n- Realistic lighting and materials\n- VR/AR support for immersive development\n- Large asset store with pre-built components\n- Cross-platform deployment capabilities\n\n## Unity Robotics Setup\n\n### Installation\n1. Download Unity Hub from the Unity website\n2. Install Unity 2022.3 LTS (Long Term Support) version\n3. Install the Unity Robotics packages through the Package Manager\n\n### Required Packages\n- **Unity Robotics Hub**: Central access point for robotics tools\n- **Unity Robotics Package**: Core ROS integration\n- **Unity Perception Package**: Synthetic data generation\n- **Unity Simulation Package**: Cloud-based simulation\n\n## ROS-TCP-Connector\n\nUnity communicates with ROS 2 through the ROS-TCP-Connector:\n\n### Installation\n```bash\n# Clone the ROS-TCP-Connector repository\ngit clone https://github.com/Unity-Technologies/ROS-TCP-Connector.git\n```\n\n### Setup in Unity\n1. Create a new Unity project\n2. Import the ROS-TCP-Connector package\n3. Add the ROSConnection prefab to your scene\n4. Configure the IP address and port to match your ROS 2 setup\n\n## Basic Unity Robot Integration\n\n### Robot Model Setup\n```csharp\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class RobotController : MonoBehaviour\n{\n    [SerializeField]\n    private string topicName = \"/joint_commands\";\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<JointStateMsg>(topicName);\n    }\n\n    void Update()\n    {\n        // Send joint state messages to ROS\n        var jointState = new JointStateMsg();\n        jointState.name = new string[] { \"joint1\", \"joint2\", \"joint3\" };\n        jointState.position = new double[] { 0.1, 0.2, 0.3 };\n\n        ros.Publish(topicName, jointState);\n    }\n}\n```\n\n### Receiving ROS Messages\n```csharp\nusing Unity.Robotics.ROSTCPConnector.ROSGeometry;\nusing RosMessageTypes.Std;\n\npublic class RobotReceiver : MonoBehaviour\n{\n    [SerializeField]\n    private string topicName = \"/joint_states\";\n\n    void Start()\n    {\n        ROSConnection.GetOrCreateInstance().Subscribe<JointStateMsg>(topicName, JointStateCallback);\n    }\n\n    void JointStateCallback(JointStateMsg jointState)\n    {\n        // Update robot joints based on received state\n        for (int i = 0; i < jointState.name.Length; i++)\n        {\n            UpdateJoint(jointState.name[i], jointState.position[i]);\n        }\n    }\n\n    void UpdateJoint(string jointName, double position)\n    {\n        // Apply position to corresponding joint in Unity\n        Transform joint = transform.Find(jointName);\n        if (joint != null)\n        {\n            joint.localRotation = Quaternion.Euler(0, (float)position * Mathf.Rad2Deg, 0);\n        }\n    }\n}\n```\n\n## NVIDIA Isaac Integration\n\nUnity works closely with NVIDIA Isaac for advanced robotics simulation:\n\n### Isaac Unity Plugin\n- **Isaac Unity Robotics Package**: Direct integration with Isaac Sim\n- **Synthetic Data Generation**: Create training data for AI models\n- **Photorealistic Simulation**: High-quality rendering for computer vision\n\n### Setting up Isaac Sim with Unity\n1. Install NVIDIA Isaac Sim\n2. Configure Unity to work with Isaac Sim\n3. Use Omniverse for collaborative simulation\n\n## Creating a Robot in Unity\n\n### 1. Import Robot Model\n- Import your URDF as an FBX file\n- Or create the robot model directly in Unity\n- Set up the kinematic hierarchy with proper joint connections\n\n### 2. Physics Configuration\n```csharp\n// Configure Rigidbody for each link\npublic class RobotLink : MonoBehaviour\n{\n    [Header(\"Physics Properties\")]\n    public float mass = 1.0f;\n    public Vector3 centerOfMass = Vector3.zero;\n\n    void Start()\n    {\n        Rigidbody rb = GetComponent<Rigidbody>();\n        if (rb != null)\n        {\n            rb.mass = mass;\n            rb.centerOfMass = centerOfMass;\n        }\n    }\n}\n```\n\n### 3. Joint Configuration\nUnity supports various joint types:\n- **Hinge Joint**: Rotational movement around one axis\n- **Configurable Joint**: Custom joint with multiple degrees of freedom\n- **Fixed Joint**: Rigid connection between bodies\n\n## Sensor Simulation in Unity\n\n### Camera Simulation\n```csharp\nusing Unity.Robotics.Sensors;\n\npublic class UnityCamera : MonoBehaviour\n{\n    [SerializeField] private int width = 640;\n    [SerializeField] private int height = 480;\n    [SerializeField] private float fov = 60f;\n\n    private UnityCameraSensor cameraSensor;\n\n    void Start()\n    {\n        cameraSensor = new UnityCameraSensor(\n            transform,\n            width, height, fov,\n            \"camera_topic\",\n            \"camera_frame\"\n        );\n    }\n\n    void Update()\n    {\n        cameraSensor.PublishCameraImage();\n    }\n}\n```\n\n### LIDAR Simulation\nUnity can simulate LIDAR using raycasting:\n\n```csharp\nusing UnityEngine;\n\npublic class UnityLidar : MonoBehaviour\n{\n    [SerializeField] private int numRays = 360;\n    [SerializeField] private float maxDistance = 10f;\n    [SerializeField] private string topicName = \"/scan\";\n\n    private float[] ranges;\n\n    void Start()\n    {\n        ranges = new float[numRays];\n    }\n\n    void Update()\n    {\n        for (int i = 0; i < numRays; i++)\n        {\n            float angle = (i * 360f / numRays) * Mathf.Deg2Rad;\n            Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\n\n            if (Physics.Raycast(transform.position, direction, out RaycastHit hit, maxDistance))\n            {\n                ranges[i] = hit.distance;\n            }\n            else\n            {\n                ranges[i] = maxDistance;\n            }\n        }\n\n        // Publish ranges to ROS\n        PublishScan(ranges);\n    }\n\n    void PublishScan(float[] ranges)\n    {\n        // Implementation to publish LaserScan message to ROS\n    }\n}\n```\n\n## Unity Package Manager Setup\n\n### Adding Robotics Packages\n1. Open Unity Package Manager (Window  Package Manager)\n2. Add packages by name or Git URL:\n   - `com.unity.robotics.ros-tcp-connector`\n   - `com.unity.robotics.urdf-importer`\n   - `com.unity.perception`\n\n### URDF Importer\nThe URDF Importer package allows direct import of ROS URDF files:\n```csharp\n// Import URDF through the UI or script\nusing Unity.Robotics.UrdfImporter;\n\npublic class UrdfLoader : MonoBehaviour\n{\n    public string urdfPath;\n\n    void Start()\n    {\n        // Load URDF file and create Unity robot\n        GameObject robot = UrdfRobotExtensions.Create(urdfPath);\n        robot.transform.SetParent(transform);\n    }\n}\n```\n\n## Simulation Scenarios\n\n### Indoor Navigation\nCreate realistic indoor environments:\n- Office buildings with furniture\n- Warehouses with obstacles\n- Multi-floor structures with elevators\n\n### Outdoor Environments\n- Urban environments with traffic\n- Natural terrains with vegetation\n- Weather effects and lighting conditions\n\n### Multi-Robot Simulation\nUnity supports multiple robots in the same scene:\n- Coordinate multiple robot behaviors\n- Implement communication between robots\n- Simulate robot swarms or teams\n\n## Performance Optimization\n\n### Rendering Optimization\n- Use Level of Detail (LOD) for complex models\n- Implement occlusion culling for large environments\n- Use occlusion areas to optimize rendering\n\n### Physics Optimization\n- Adjust fixed timestep for physics simulation\n- Use appropriate collision detection methods\n- Optimize joint configurations for performance\n\n## Debugging and Visualization\n\n### Scene View Tools\n- Use Gizmos to visualize transforms and bounds\n- Enable physics visualization in Scene view\n- Use the Profiler to identify performance bottlenecks\n\n### ROS Integration Debugging\n- Monitor ROS topics in real-time\n- Visualize TF transforms in Unity\n- Log messages between Unity and ROS\n\n## Best Practices\n\n- Start with simple scenes and gradually add complexity\n- Use prefabs for reusable robot components\n- Implement proper error handling for ROS connections\n- Optimize assets for real-time performance\n- Use version control for Unity projects (Git LFS for large files)\n\n## Exercise\n\nCreate a Unity scene with your humanoid robot model imported from URDF. Implement basic joint control through ROS 2 communication and set up a camera sensor to publish images to ROS. Create a simple environment with obstacles and implement a basic navigation task.\n\n## Resources\n\n- [Unity Robotics Hub Documentation](https://github.com/Unity-Technologies/Unity-Robotics-Hub)\n- [ROS-TCP-Connector](https://github.com/Unity-Technologies/ROS-TCP-Connector)\n- [Unity URDF Importer](https://github.com/Unity-Technologies/URDF-Importer)\n- [NVIDIA Isaac Sim Documentation](https://docs.omniverse.nvidia.com/isaacsim/latest/overview.html)",
    "path": "module-2-digital-twin\\unity-visualization.md",
    "description": ""
  },
  "module-2-digital-twin\\urdf-validation": {
    "title": "module-2-digital-twin\\urdf-validation",
    "content": "# URDF Model Creation and Validation\n\nCreating accurate URDF models is fundamental to successful digital twin implementation. This section covers best practices for creating URDF models and validating them for use in simulation environments.\n\n## URDF Model Best Practices\n\n### 1. Proper Kinematic Structure\nA well-structured URDF model should have a clear kinematic chain:\n\n```xml\n<?xml version=\"1.0\"?>\n<robot name=\"humanoid_with_proper_structure\" xmlns:xacro=\"http://www.ros.org/wiki/xacro\">\n  <!-- Base link (required - no geometry needed for simple base) -->\n  <link name=\"base_link\">\n    <visual>\n      <origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n      <geometry>\n        <box size=\"0.3 0.3 0.5\"/>\n      </geometry>\n      <material name=\"light_gray\">\n        <color rgba=\"0.7 0.7 0.7 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n      <geometry>\n        <box size=\"0.3 0.3 0.5\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"10.0\"/>\n      <origin xyz=\"0 0 0.25\" rpy=\"0 0 0\"/>\n      <inertia ixx=\"0.2\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.2\" iyz=\"0.0\" izz=\"0.1\"/>\n    </inertial>\n  </link>\n\n  <!-- Head -->\n  <joint name=\"neck_joint\" type=\"revolute\">\n    <parent link=\"base_link\"/>\n    <child link=\"head_link\"/>\n    <origin xyz=\"0 0 0.5\" rpy=\"0 0 0\"/>\n    <axis xyz=\"0 1 0\"/>\n    <limit lower=\"-0.5\" upper=\"0.5\" effort=\"10\" velocity=\"2\"/>\n    <dynamics damping=\"0.5\" friction=\"0.1\"/>\n  </joint>\n\n  <link name=\"head_link\">\n    <visual>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n      <material name=\"skin\">\n        <color rgba=\"0.8 0.6 0.4 1\"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius=\"0.1\"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value=\"2.0\"/>\n      <inertia ixx=\"0.004\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.004\" iyz=\"0.0\" izz=\"0.004\"/>\n    </inertial>\n  </link>\n\n  <!-- Left Arm (using Xacro macros for consistency) -->\n  <xacro:macro name=\"simple_arm\" params=\"side parent xyz rpy joint_limits_lower joint_limits_upper\">\n    <!-- Shoulder -->\n    <joint name=\"${side}_shoulder_pitch_joint\" type=\"revolute\">\n      <parent link=\"${parent}\"/>\n      <child link=\"${side}_shoulder_link\"/>\n      <origin xyz=\"${xyz}\" rpy=\"${rpy}\"/>\n      <axis xyz=\"0 1 0\"/>\n      <limit lower=\"${joint_limits_lower}\" upper=\"${joint_limits_upper}\" effort=\"15\" velocity=\"2\"/>\n      <dynamics damping=\"0.5\" friction=\"0.1\"/>\n    </joint>\n\n    <link name=\"${side}_shoulder_link\">\n      <visual>\n        <geometry>\n          <box size=\"0.08 0.08 0.1\"/>\n        </geometry>\n        <material name=\"dark_gray\">\n          <color rgba=\"0.3 0.3 0.3 1\"/>\n        </material>\n      </visual>\n      <collision>\n        <geometry>\n          <box size=\"0.08 0.08 0.1\"/>\n        </geometry>\n      </collision>\n      <inertial>\n        <mass value=\"1.0\"/>\n        <inertia ixx=\"0.001\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.001\" iyz=\"0.0\" izz=\"0.001\"/>\n      </inertial>\n    </link>\n\n    <joint name=\"${side}_shoulder_yaw_joint\" type=\"revolute\">\n      <parent link=\"${side}_shoulder_link\"/>\n      <child link=\"${side}_upper_arm_link\"/>\n      <origin xyz=\"0 0 -0.1\" rpy=\"0 0 0\"/>\n      <axis xyz=\"1 0 0\"/>\n      <limit lower=\"-1.57\" upper=\"1.57\" effort=\"15\" velocity=\"2\"/>\n      <dynamics damping=\"0.5\" friction=\"0.1\"/>\n    </joint>\n\n    <link name=\"${side}_upper_arm_link\">\n      <visual>\n        <geometry>\n          <cylinder length=\"0.3\" radius=\"0.05\"/>\n        </geometry>\n        <material name=\"dark_gray\"/>\n      </visual>\n      <collision>\n        <geometry>\n          <cylinder length=\"0.3\" radius=\"0.05\"/>\n        </geometry>\n      </collision>\n      <inertial>\n        <mass value=\"1.5\"/>\n        <inertia ixx=\"0.005\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.005\" iyz=\"0.0\" izz=\"0.0005\"/>\n      </inertial>\n    </link>\n\n    <joint name=\"${side}_elbow_joint\" type=\"revolute\">\n      <parent link=\"${side}_upper_arm_link\"/>\n      <child link=\"${side}_lower_arm_link\"/>\n      <origin xyz=\"0 0 -0.3\" rpy=\"0 0 0\"/>\n      <axis xyz=\"0 1 0\"/>\n      <limit lower=\"-0.1\" upper=\"2.0\" effort=\"10\" velocity=\"2\"/>\n      <dynamics damping=\"0.3\" friction=\"0.1\"/>\n    </joint>\n\n    <link name=\"${side}_lower_arm_link\">\n      <visual>\n        <geometry>\n          <cylinder length=\"0.25\" radius=\"0.04\"/>\n        </geometry>\n        <material name=\"dark_gray\"/>\n      </visual>\n      <collision>\n        <geometry>\n          <cylinder length=\"0.25\" radius=\"0.04\"/>\n        </geometry>\n      </collision>\n      <inertial>\n        <mass value=\"1.0\"/>\n        <inertia ixx=\"0.003\" ixy=\"0.0\" ixz=\"0.0\" iyy=\"0.003\" iyz=\"0.0\" izz=\"0.0003\"/>\n      </inertial>\n    </link>\n  </xacro:macro>\n\n  <!-- Use the macro for both arms -->\n  <xacro:simple_arm\n    side=\"left\"\n    parent=\"base_link\"\n    xyz=\"0.15 0 0.2\"\n    rpy=\"0 0 0\"\n    joint_limits_lower=\"-1.57\"\n    joint_limits_upper=\"1.57\"/>\n\n  <xacro:simple_arm\n    side=\"right\"\n    parent=\"base_link\"\n    xyz=\"-0.15 0 0.2\"\n    rpy=\"0 0 0\"\n    joint_limits_lower=\"-1.57\"\n    joint_limits_upper=\"1.57\"/>\n</robot>\n```\n\n### 2. Proper Inertial Properties\nAccurate inertial properties are crucial for realistic physics simulation:\n\n```xml\n<!-- For a solid box -->\n<inertial>\n  <mass value=\"5.0\"/>\n  <origin xyz=\"0 0 0\"/>\n  <inertia\n    ixx=\"0.0833333\"  <!-- m*(h + d)/12 -->\n    ixy=\"0.0\"\n    ixz=\"0.0\"\n    iyy=\"0.0833333\"  <!-- m*(w + d)/12 -->\n    iyz=\"0.0\"\n    izz=\"0.0833333\"/> <!-- m*(w + h)/12 -->\n</inertial>\n\n<!-- For a solid cylinder along Z-axis -->\n<inertial>\n  <mass value=\"1.0\"/>\n  <origin xyz=\"0 0 0\"/>\n  <inertia\n    ixx=\"0.0052083\"  <!-- m*(3*r + h)/12 -->\n    ixy=\"0.0\"\n    ixz=\"0.0\"\n    iyy=\"0.0052083\"\n    iyz=\"0.0\"\n    izz=\"0.0025\"/>    <!-- m*r/2 -->\n</inertial>\n\n<!-- For a solid sphere -->\n<inertial>\n  <mass value=\"2.0\"/>\n  <origin xyz=\"0 0 0\"/>\n  <inertia\n    ixx=\"0.02\"       <!-- 2*m*r/5 -->\n    ixy=\"0.0\"\n    ixz=\"0.0\"\n    iyy=\"0.02\"\n    iyz=\"0.0\"\n    izz=\"0.02\"/>\n</inertial>\n```\n\n## Validation Techniques\n\n### 1. URDF Validation Tools\n\n#### Check URDF\nUse the `check_urdf` command to validate your URDF:\n\n```bash\n# Install the tool if not already installed\nsudo apt install ros-humble-urdfdom-py\n\n# Check your URDF file\ncheck_urdf /path/to/your/robot.urdf\n```\n\n#### View URDF\nVisualize your robot structure:\n\n```bash\n# Install visualization tools\nsudo apt install ros-humble-joint-state-publisher-gui\n\n# View the robot\nurdf_to_graphiz /path/to/your/robot.urdf\n# This creates .gv files that can be viewed with Graphviz\n```\n\n### 2. Simulation Validation\n\n#### Gazebo Validation\nTest your URDF in Gazebo:\n\n```xml\n<!-- Add Gazebo-specific tags for simulation -->\n<gazebo reference=\"base_link\">\n  <material>Gazebo/Blue</material>\n  <mu1>0.2</mu1>\n  <mu2>0.2</mu2>\n  <kp>1000000.0</kp>  <!-- Contact stiffness -->\n  <kd>100.0</kd>      <!-- Contact damping -->\n</gazebo>\n\n<!-- Add transmission for ROS control -->\n<xacro:macro name=\"transmission_block\" params=\"joint_name\">\n  <transmission name=\"${joint_name}_trans\">\n    <type>transmission_interface/SimpleTransmission</type>\n    <joint name=\"${joint_name}\">\n      <hardwareInterface>hardware_interface/EffortJointInterface</hardwareInterface>\n    </joint>\n    <actuator name=\"${joint_name}_motor\">\n      <hardwareInterface>hardware_interface/EffortJointInterface</hardwareInterface>\n      <mechanicalReduction>1</mechanicalReduction>\n    </actuator>\n  </transmission>\n</xacro:macro>\n```\n\n### 3. Kinematic Validation\n\n#### Forward Kinematics\nValidate that your kinematic chain works correctly:\n\n```python\n#!/usr/bin/env python3\nimport rospy\nimport tf2_ros\nfrom urdf_parser_py.urdf import URDF\nfrom pykdl_utils.kdl_kinematics import KDLKinematics\n\ndef validate_kinematics():\n    # Load robot description\n    robot = URDF.from_xml_string(rospy.get_param('robot_description'))\n\n    # Create kinematic chain\n    kdl_kin = KDLKinematics(robot, 'base_link', 'left_hand_link')\n\n    # Test joint positions\n    q = [0.0, 0.0, 0.0, 0.0, 0.0]  # joint angles\n    pose = kdl_kin.forward(q)\n    print(f\"End effector pose: {pose}\")\n\n    # Test inverse kinematics\n    q_ik = kdl_kin.inverse(pose)\n    print(f\"IK solution: {q_ik}\")\n\nif __name__ == '__main__':\n    rospy.init_node('kinematic_validator')\n    validate_kinematics()\n```\n\n## Advanced Validation Examples\n\n### 1. Collision Detection Validation\nEnsure no self-collision in common poses:\n\n```xml\n<!-- Add self-collision checking parameters -->\n<robot name=\"collision_test_robot\">\n  <!-- Define collision pairs to ignore (if needed) -->\n  <gazebo>\n    <self_collide>false</self_collide>  <!-- Only for specific testing -->\n  </gazebo>\n\n  <!-- Or be more specific with collision filtering -->\n  <gazebo reference=\"link1\">\n    <collision>\n      <surface>\n        <contact>\n          <collide_without_contact>false</collide_without_contact>\n        </contact>\n      </surface>\n    </collision>\n  </gazebo>\n</robot>\n```\n\n### 2. Center of Mass Validation\nCalculate and verify center of mass:\n\n```python\n#!/usr/bin/env python3\nimport rospy\nfrom urdf_parser_py.urdf import URDF\nimport numpy as np\n\ndef calculate_total_com():\n    robot = URDF.from_xml_string(rospy.get_param('robot_description'))\n\n    total_mass = 0.0\n    weighted_com = np.array([0.0, 0.0, 0.0])\n\n    for link in robot.links:\n        if link.inertial and link.inertial.mass:\n            mass = link.inertial.mass\n            com = np.array([\n                link.inertial.origin.xyz[0],\n                link.inertial.origin.xyz[1],\n                link.inertial.origin.xyz[2]\n            ])\n\n            total_mass += mass\n            weighted_com += mass * com\n\n    overall_com = weighted_com / total_mass if total_mass > 0 else np.array([0, 0, 0])\n    print(f\"Overall center of mass: {overall_com}\")\n    print(f\"Total mass: {total_mass}\")\n\nif __name__ == '__main__':\n    rospy.init_node('com_calculator')\n    calculate_total_com()\n```\n\n### 3. Dynamic Validation\nTest dynamic properties in simulation:\n\n```yaml\n# controllers.yaml for testing\ncontroller_manager:\n  ros__parameters:\n    update_rate: 100\n\n    joint_state_broadcaster:\n      type: joint_state_broadcaster/JointStateBroadcaster\n\n    left_arm_controller:\n      type: position_controllers/JointGroupPositionController\n      joints:\n        - left_shoulder_pitch_joint\n        - left_shoulder_yaw_joint\n        - left_elbow_joint\n\n    right_arm_controller:\n      type: position_controllers/JointGroupPositionController\n      joints:\n        - right_shoulder_pitch_joint\n        - right_shoulder_yaw_joint\n        - right_elbow_joint\n```\n\n### 4. Range of Motion Validation\nTest joint limits and workspace:\n\n```python\n#!/usr/bin/env python3\nimport rospy\nfrom sensor_msgs.msg import JointState\nimport numpy as np\n\nclass JointRangeValidator:\n    def __init__(self):\n        self.joint_limits = {\n            'left_shoulder_pitch_joint': (-1.57, 1.57),\n            'left_shoulder_yaw_joint': (-1.57, 1.57),\n            'left_elbow_joint': (-0.1, 2.0)\n        }\n\n        rospy.Subscriber('/joint_states', JointState, self.joint_callback)\n\n    def joint_callback(self, msg):\n        for i, name in enumerate(msg.name):\n            if name in self.joint_limits:\n                pos = msg.position[i]\n                min_pos, max_pos = self.joint_limits[name]\n\n                if pos < min_pos or pos > max_pos:\n                    rospy.logwarn(f\"Joint {name} out of range: {pos} (limits: {min_pos}, {max_pos})\")\n                elif abs(pos - min_pos) < 0.1 or abs(pos - max_pos) < 0.1:\n                    rospy.loginfo(f\"Joint {name} approaching limit: {pos}\")\n\nif __name__ == '__main__':\n    rospy.init_node('joint_range_validator')\n    validator = JointRangeValidator()\n    rospy.spin()\n```\n\n## Common Validation Issues and Solutions\n\n### 1. Floating Point Errors\n- Use appropriate precision in URDF values\n- Validate with tolerance checks in code\n\n### 2. Mass Distribution Issues\n- Ensure center of mass is reasonable\n- Verify moments of inertia are positive and realistic\n\n### 3. Joint Limit Problems\n- Test extreme positions in simulation\n- Validate with physical constraints\n\n### 4. Collision Issues\n- Check for interpenetration at default positions\n- Test various poses for self-collision\n\n## Automated Validation Script\n\nCreate a comprehensive validation script:\n\n```bash\n#!/bin/bash\n# validate_robot.sh\n\nURDF_FILE=$1\nif [ -z \"$URDF_FILE\" ]; then\n    echo \"Usage: $0 <urdf_file>\"\n    exit 1\nfi\n\necho \"Validating $URDF_FILE...\"\n\n# Check if file exists\nif [ ! -f \"$URDF_FILE\" ]; then\n    echo \"Error: File $URDF_FILE does not exist\"\n    exit 1\nfi\n\n# Validate URDF structure\necho \"Checking URDF structure...\"\ncheck_urdf \"$URDF_FILE\" 2>&1 | tee urdf_check.log\nif [ $? -ne 0 ]; then\n    echo \"URDF validation failed!\"\n    exit 1\nfi\n\n# Check for common issues\necho \"Checking for common issues...\"\n\n# Check for duplicate names\nDUPLICATE_LINKS=$(grep -oP '(?<=<link name=\")[^\"]*' \"$URDF_FILE\" | sort | uniq -d)\nif [ ! -z \"$DUPLICATE_LINKS\" ]; then\n    echo \"Warning: Duplicate link names found: $DUPLICATE_LINKS\"\nfi\n\nDUPLICATE_JOINTS=$(grep -oP '(?<=<joint name=\")[^\"]*' \"$URDF_FILE\" | sort | uniq -d)\nif [ ! -z \"$DUPLICATE_JOINTS\" ]; then\n    echo \"Warning: Duplicate joint names found: $DUPLICATE_JOINTS\"\nfi\n\n# Check for missing inertial properties\nMISSING_INERTIAL=$(grep -B 10 'mass' \"$URDF_FILE\" | grep -E '^<link name=' | wc -l)\nTOTAL_LINKS=$(grep -c '<link name=' \"$URDF_FILE\")\nif [ $TOTAL_LINKS -ne $MISSING_INERTIAL ]; then\n    echo \"Warning: Some links may be missing inertial properties\"\nfi\n\necho \"Validation complete. Check urdf_check.log for details.\"\n```\n\n## Best Practices Summary\n\n1. **Always validate** URDF files before simulation\n2. **Use consistent units** (SI units: meters, kilograms, seconds)\n3. **Test kinematic chains** with forward and inverse kinematics\n4. **Verify inertial properties** with realistic values\n5. **Test collision detection** with various robot poses\n6. **Check joint limits** against physical constraints\n7. **Validate center of mass** for stability\n8. **Use Xacro macros** for complex, repetitive structures\n\n## Exercise\n\nCreate a complete humanoid robot URDF model with:\n1. Proper kinematic structure (tree topology)\n2. Accurate inertial properties for all links\n3. Appropriate joint limits based on human anatomy\n4. Validation tests to ensure the model is physically realistic\n5. Integration with both Gazebo and Unity simulation environments\n\nValidate your model using the tools and techniques described in this section, ensuring it behaves realistically in simulation.",
    "path": "module-2-digital-twin\\urdf-validation.md",
    "description": ""
  },
  "module-3-ai-perception\\index": {
    "title": "Module 3 AI Perception",
    "content": "\n# Module 3: The AI-Robot Brain (NVIDIA Isaac)\n\nWelcome to Module 3 of the Physical AI & Humanoid Robotics book! This module focuses on the AI components that power modern robotics: perception, navigation, and intelligent decision-making using NVIDIA Isaac technologies.\n\n## Overview\n\nIn this module, you'll learn to integrate AI systems with your humanoid robot, enabling it to perceive its environment, navigate autonomously, and make intelligent decisions. We'll leverage NVIDIA Isaac technologies to build an \"AI brain\" for your robot.\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n\n- Set up and configure NVIDIA Isaac Sim for robotics simulation\n- Implement Visual SLAM (Simultaneous Localization and Mapping) systems\n- Integrate Isaac ROS packages for perception and navigation\n- Perform VSLAM (Visual SLAM) for robot localization and mapping\n- Configure Nav2 for humanoid locomotion and navigation\n- Create autonomous navigation behaviors for humanoid robots\n- Generate synthetic data for AI training using Isaac Sim\n- Integrate multiple sensors for robust perception\n\n## Prerequisites\n\nBefore starting this module, ensure you have:\n\n- Completed Module 1 (ROS 2 fundamentals)\n- Completed Module 2 (Digital Twin simulation)\n- Basic understanding of computer vision concepts\n- Access to an NVIDIA GPU with CUDA support (recommended)\n- Isaac Sim installed (covered in this module)\n\n## Module Structure\n\nThis module is organized into the following sections:\n\n1. **Isaac Sim Fundamentals** - Core concepts and setup\n2. **Isaac Sim** - High-fidelity simulation environment\n3. **VSLAM and Navigation** - Visual SLAM and navigation concepts\n4. **VSLAM and Navigation** - Implementation with Isaac ROS\n5. **Isaac Sim Fundamentals** - Advanced Isaac Sim features\n6. **VSLAM and Navigation** - Navigation planning and obstacle avoidance\n7. **Practical Exercises** - Hands-on applications with Isaac AI\n8. **Perception and Navigation Pipeline Diagrams** - System architecture\n\n## NVIDIA Isaac Ecosystem\n\nThe NVIDIA Isaac ecosystem provides powerful tools for robotics AI:\n\n- **Isaac Sim**: High-fidelity simulation with synthetic data generation\n- **Isaac ROS**: ROS 2 packages for perception, navigation, and manipulation\n- **Isaac Apps**: Reference applications for common robotics tasks\n- **Deep Graph Library (DGL)**: For robot learning applications\n- **Omniverse**: For collaborative simulation and digital twin creation\n\n## Key Technologies Covered\n\n### Isaac ROS Packages\n- Isaac ROS Visual SLAM for localization and mapping\n- Isaac ROS DetectNet for object detection\n- Isaac ROS Bi3D for 3D segmentation\n- Isaac ROS Apriltag for fiducial detection\n- Isaac ROS Pose Estimation for 6DOF pose estimation\n\n### Navigation Technologies\n- Nav2 (Navigation 2) for ROS 2 navigation\n- VSLAM (Visual Simultaneous Localization and Mapping)\n- Path planning algorithms (A*, Dijkstra, etc.)\n- Obstacle avoidance and collision detection\n\n## Integration with Previous Modules\n\nThis module builds on the previous modules by:\n- Using ROS 2 communication patterns learned in Module 1\n- Incorporating digital twin concepts from Module 2\n- Adding AI perception and navigation capabilities\n- Preparing for the VLA (Vision-Language-Action) integration in Module 4\n\n## Next Steps\n\nBegin with the Isaac Sim fundamentals to establish your simulation environment, then proceed through the sections in order to build up your understanding of AI-powered robotics systems. Each section builds on the previous one, so follow the sequence for the best learning experience.",
    "path": "module-3-ai-perception\\index.md",
    "description": ""
  },
  "module-3-ai-perception\\installation-setup": {
    "title": "module-3-ai-perception\\installation-setup",
    "content": "# Isaac Sim Installation and Setup\n\nThis section provides comprehensive instructions for installing and setting up NVIDIA Isaac Sim for robotics perception and navigation applications.\n\n## System Requirements\n\n### Hardware Requirements\n- **GPU**: NVIDIA RTX 3080/4080/4090 or professional GPU (A40, A6000) with 10GB+ VRAM\n- **CPU**: 8+ core processor (Intel i7 / AMD Ryzen 7 or better recommended)\n- **RAM**: 32GB system memory (64GB+ recommended)\n- **Storage**: 50GB+ free space on SSD (100GB+ recommended)\n- **OS**: Ubuntu 20.04/22.04 LTS or Windows 10/11 (64-bit)\n\n### Software Requirements\n- **NVIDIA GPU Drivers**: Version 520+ (535+ recommended)\n- **CUDA Toolkit**: Version 11.8 or 12.x\n- **Docker**: Version 20.10+ (for containerized deployment)\n- **NVIDIA Container Toolkit**: For GPU-accelerated containers\n- **Python**: Version 3.8-3.10 for Isaac ROS integration\n\n## Pre-Installation Checks\n\n### Verify GPU and Drivers\n```bash\n# Check if NVIDIA GPU is detected\nnvidia-smi\n\n# Verify CUDA installation\nnvcc --version\n\n# Check for compatible GPU\nnvidia-ml-py3 # Python library for GPU management\n```\n\n### Install Required Dependencies\n```bash\n# Update system packages\nsudo apt update && sudo apt upgrade -y\n\n# Install basic dependencies\nsudo apt install -y build-essential cmake git python3-dev python3-pip\n\n# Install graphics drivers (Ubuntu)\nsudo apt install -y nvidia-driver-535 nvidia-utils-535\nsudo reboot\n```\n\n## Installation Methods\n\n### Method 1: Omniverse Launcher (Recommended for Beginners)\n\n#### 1. Download Omniverse Launcher\n1. Go to [NVIDIA Developer](https://developer.nvidia.com/omniverse) website\n2. Register or sign in to your NVIDIA Developer account\n3. Download Omniverse Launcher for your operating system\n4. Install the launcher following the on-screen instructions\n\n#### 2. Install Isaac Sim through Launcher\n1. Launch Omniverse Launcher\n2. Sign in with your NVIDIA Developer account\n3. Navigate to \"Isaac\" section\n4. Click \"Install\" next to Isaac Sim\n5. Choose installation location (default is recommended)\n6. Wait for download and installation to complete\n\n#### 3. Launch Isaac Sim\n1. From the launcher, click \"Launch\" next to Isaac Sim\n2. The application will start and load the initial scene\n3. Verify installation by checking Help  About Isaac Sim\n\n### Method 2: Containerized Installation (Recommended for Production)\n\n#### 1. Install Docker and NVIDIA Container Toolkit\n```bash\n# Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt update\nsudo apt install -y nvidia-container-toolkit\nsudo systemctl restart docker\n```\n\n#### 2. Pull and Run Isaac Sim Container\n```bash\n# Pull the Isaac Sim Docker image\ndocker pull nvcr.io/nvidia/isaac-sim:4.0.0\n\n# Create directories for persistent data\nmkdir -p ~/docker/isaac-sim/cache/kit\nmkdir -p ~/docker/isaac-sim/assets\nmkdir -p ~/docker/isaac-sim/home\n\n# Run Isaac Sim container with GUI support\nxhost +local:docker\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env \"ACCEPT_EULA=Y\" \\\n  --env \"NVIDIA_VISIBLE_DEVICES=all\" \\\n  --env \"NVIDIA_DRIVER_CAPABILITIES=all\" \\\n  --volume /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  --volume $HOME/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n  --volume $HOME/docker/isaac-sim/assets:/isaac-sim/assets:rw \\\n  --volume $HOME/docker/isaac-sim/home:/isaac-sim/home:rw \\\n  --volume $HOME/docker/isaac-sim/logs:/isaac-sim/logs:rw \\\n  --volume $HOME/docker/isaac-sim/config:/isaac-sim/config:rw \\\n  nvcr.io/nvidia/isaac-sim:4.0.0\n```\n\n### Method 3: Native Installation (Advanced Users)\n\n#### 1. Install Isaac Sim Prerequisites\n```bash\n# Install additional dependencies for native installation\nsudo apt install -y \\\n  libgl1-mesa-glx \\\n  libglib2.0-0 \\\n  libsm6 \\\n  libxext6 \\\n  libxrender-dev \\\n  libgomp1 \\\n  libssl1.1\n\n# Install Python dependencies\npip3 install --user \\\n  numpy \\\n  scipy \\\n  matplotlib \\\n  opencv-python \\\n  transforms3d\n```\n\n#### 2. Download and Install Isaac Sim\n1. Download Isaac Sim from NVIDIA Developer portal\n2. Extract the archive to your preferred location (e.g., `/opt/isaac-sim`)\n3. Set up environment variables:\n\n```bash\n# Add to ~/.bashrc\necho 'export ISAAC_SIM_PATH=/opt/isaac-sim' >> ~/.bashrc\necho 'export PYTHONPATH=$ISAAC_SIM_PATH/python:$PYTHONPATH' >> ~/.bashrc\necho 'export PATH=$ISAAC_SIM_PATH/python/bin:$PATH' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n## Isaac ROS Integration Setup\n\n### Install Isaac ROS Dependencies\n```bash\n# Install ROS 2 Humble (if not already installed)\nsudo apt update\nsudo apt install -y software-properties-common\nsudo add-apt-repository universe\nsudo apt update && sudo apt install curl -y\ncurl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\nsudo apt update\nsudo apt install -y ros-humble-desktop\nsudo apt install -y python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n```\n\n### Install Isaac ROS Packages\n```bash\n# Create workspace\nmkdir -p ~/isaac_ros_ws/src\ncd ~/isaac_ros_ws\n\n# Source ROS 2\nsource /opt/ros/humble/setup.bash\n\n# Install vcs tools\nsudo apt install python3-vcstool\n\n# Get Isaac ROS packages\nwget https://raw.githubusercontent.com/NVIDIA-ISAAC-ROS/.repos/main/isaac_ros.repos\nvcs import src < isaac_ros.repos\n\n# Install dependencies\nrosdep install --from-paths src --ignore-src -r -y\n\n# Build the workspace\ncolcon build --symlink-install --packages-select \\\n  isaac_ros_common \\\n  isaac_ros_image_pipeline \\\n  isaac_ros_visual_slam \\\n  isaac_ros_pose_estimation \\\n  isaac_ros_bi3d \\\n  isaac_ros_apriltag \\\n  isaac_ros_nitros_type_introspection \\\n  isaac_ros_managed_nh \\\n  isaac_ros_test \\\n  isaac_ros_test_utils\n```\n\n### Configure Isaac ROS Environment\n```bash\n# Add to ~/.bashrc\necho 'source ~/isaac_ros_ws/install/setup.bash' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n## Isaac Sim Configuration\n\n### Initial Configuration\nAfter first launch, configure Isaac Sim:\n\n1. **Extensions Setup**:\n   - Go to Window  Extensions\n   - Enable required extensions:\n     - Isaac  All Isaac extensions\n     - Robotics  All robotics extensions\n     - Perception  All perception extensions\n     - Navigation  All navigation extensions\n     - ROS/ROS2 Bridge  Enable bridge extensions\n\n2. **Preferences Configuration**:\n   - Edit  Preferences  Isaac Sim\n   - Set preferred units to meters\n   - Configure physics settings (see below)\n   - Set rendering quality based on your hardware\n\n### Physics Configuration\n```python\n# Example physics configuration script\nimport carb\n\nsettings = carb.settings.get_settings()\n\n# Physics settings for robotics simulation\nsettings.set(\"/physics/solverType\", 0)  # 0=PGS, 1=MLCP\nsettings.set(\"/physics/solverPositionIterationCount\", 8)\nsettings.set(\"/physics/solverVelocityIterationCount\", 4)\nsettings.set(\"/physics/defaultRestOffset\", 0.001)\nsettings.set(\"/physics/defaultContactOffset\", 0.002)\nsettings.set(\"/physics/bounceThreshold\", 2.0)\nsettings.set(\"/physics/sleepThreshold\", 0.005)\n```\n\n### Rendering Configuration\n```python\n# Rendering settings for optimal performance\nsettings.set(\"/app/performace/range\", 2)  # Performance level\nsettings.set(\"/rtx/indirectDiffuseLighting/enabled\", True)\nsettings.set(\"/rtx/reflections/enabled\", True)\nsettings.set(\"/rtx/dlss/enable\", True)  # If RTX GPU supports DLSS\nsettings.set(\"/rtx/dlss/mode\", 2)  # Quality mode\n```\n\n## Verification and Testing\n\n### Basic Functionality Test\n1. Launch Isaac Sim\n2. Create a new stage (File  New Stage)\n3. Add a simple primitive (Create  Cube)\n4. Verify physics by pressing Play and observing gravity effect\n5. Add a camera and verify rendering\n\n### ROS Bridge Test\n```bash\n# Terminal 1: Launch Isaac Sim with ROS bridge\n# In Isaac Sim, enable ROS Bridge extension\n# Add ROS Bridge node to stage\n\n# Terminal 2: Test ROS communication\nsource /opt/ros/humble/setup.bash\nsource ~/isaac_ros_ws/install/setup.bash\n\n# Check available topics\nros2 topic list\n\n# Test camera publishing\nros2 run image_view image_view image:=/rgb_camera\n\n# Test LIDAR publishing\nros2 topic echo /lidar_scan sensor_msgs/msg/LaserScan\n```\n\n### Isaac ROS Package Test\n```bash\n# Test Isaac ROS visual slam\nsource ~/isaac_ros_ws/install/setup.bash\nros2 launch isaac_ros_visual_slam visual_slam.launch.py input_is_rectified:=False\n\n# Test Isaac ROS apriltag\nros2 launch isaac_ros_apriltag apriltag.launch.py\n```\n\n## Troubleshooting Common Issues\n\n### GPU/CUDA Issues\n**Problem**: Isaac Sim fails to start or shows rendering errors\n**Solutions**:\n1. Verify NVIDIA drivers: `nvidia-smi`\n2. Check CUDA installation: `nvcc --version`\n3. Ensure correct GPU: `lspci | grep -i nvidia`\n4. Update drivers if necessary\n\n### ROS Communication Issues\n**Problem**: Isaac Sim cannot communicate with ROS\n**Solutions**:\n1. Check ROS network configuration\n2. Verify Isaac Sim ROS Bridge extension is enabled\n3. Check firewall settings\n4. Ensure correct IP addresses and ports\n\n### Performance Issues\n**Problem**: Isaac Sim runs slowly or has low frame rates\n**Solutions**:\n1. Reduce rendering quality in preferences\n2. Lower physics substeps\n3. Simplify scene geometry\n4. Check GPU memory usage\n\n### Container Issues\n**Problem**: Isaac Sim container fails to start\n**Solutions**:\n1. Verify NVIDIA Container Toolkit installation\n2. Check Docker permissions\n3. Ensure GPU is accessible in container: `docker run --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi`\n\n## Performance Optimization\n\n### For Development\n- Use lower rendering quality during development\n- Disable advanced rendering features (ray tracing, global illumination)\n- Use simpler collision geometries\n- Reduce physics update rate\n\n### For Production\n- Optimize asset complexity\n- Use level-of-detail (LOD) systems\n- Implement occlusion culling\n- Use appropriate texture resolutions\n\n## Environment Variables\n\nSet these environment variables for optimal Isaac Sim operation:\n\n```bash\n# Add to ~/.bashrc\nexport ISAAC_SIM_DISABLE_CUDA_DEVICE_GPU_INFO=1  # Disable CUDA device info logging\nexport ISAAC_SIM_DISABLE_OPEN_GL_GPU_INFO=1      # Disable OpenGL GPU info logging\nexport ISAAC_SIM_FORCE_GPU=1                     # Force GPU usage\nexport ISAAC_ROS_BRIDGE_DISABLE_TCP_NODE=1       # Disable TCP node if not needed\n```\n\n## Next Steps\n\nAfter successful installation:\n\n1. Complete the Isaac Sim tutorials to familiarize yourself with the interface\n2. Set up your first robot simulation\n3. Configure ROS bridges for your specific robot\n4. Begin developing perception and navigation pipelines\n\nYour Isaac Sim installation is now ready for developing AI-powered robotics applications!",
    "path": "module-3-ai-perception\\installation-setup.md",
    "description": ""
  },
  "module-3-ai-perception\\isaac-sim-fundamentals": {
    "title": "module-3-ai-perception\\isaac-sim-fundamentals",
    "content": "# Isaac Sim Fundamentals\n\nIsaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, designed specifically for developing, testing, and validating AI-based robotics applications. This section covers the core concepts and fundamentals of Isaac Sim.\n\n## Introduction to Isaac Sim\n\nIsaac Sim provides a comprehensive simulation environment that includes:\n- **Photorealistic rendering**: Physically accurate rendering with RTX real-time ray tracing\n- **High-fidelity physics**: Accurate rigid body dynamics, soft body simulation, and fluid dynamics\n- **Synthetic data generation**: High-quality training data for AI models\n- **Sensor simulation**: Realistic camera, LIDAR, RADAR, IMU, and other sensor models\n- **AI integration**: Built-in tools for training and testing AI models\n- **ROS/ROS2 bridge**: Seamless integration with ROS/ROS2 ecosystems\n\n## System Requirements and Setup\n\n### Hardware Requirements\n- **GPU**: NVIDIA RTX 3080/4080/4090 or professional GPU (A40, A6000) with 10GB+ VRAM\n- **CPU**: 8+ core processor (Intel i7 / AMD Ryzen 7 or better recommended)\n- **RAM**: 32GB system memory (64GB+ recommended)\n- **Storage**: 50GB+ free space on SSD (100GB+ recommended)\n\n### Installation Methods\n1. **Omniverse Launcher**: Recommended for beginners\n2. **Containerized Installation**: Recommended for production\n3. **Native Installation**: For advanced users\n\n### Core Concepts\n\n#### USD (Universal Scene Description)\nIsaac Sim uses NVIDIA's Universal Scene Description (USD) format for scene representation:\n- Hierarchical structure organized in a tree-like structure\n- Layered composition with multiple layers that can be composed together\n- Schema system for standardized object types and properties\n- Variant sets for different configurations of the same object\n\n#### Omniverse Nucleus\nThe collaborative platform that enables:\n- Multi-user editing of scenes\n- Asset management and sharing\n- Real-time synchronization across users\n\n#### Physics Simulation\nIsaac Sim uses PhysX for physics simulation with:\n- Rigid body dynamics with accurate collision detection and response\n- Soft body simulation for deformable objects and cloth\n- Fluid dynamics for liquid and gas simulation\n- Vehicle dynamics for realistic vehicle physics\n\n## Isaac Sim Architecture\n\n### Core Components\n```\nIsaac Sim\n Omniverse Kit (Foundation)\n    Physics Engine (PhysX)\n    Rendering Engine (RTX)\n    USD Scene Management\n    UI Framework\n Isaac Extensions\n    Robotics Extensions\n    Perception Extensions\n    Navigation Extensions\n    Manipulation Extensions\n ROS/ROS2 Bridge\n    Message Conversion\n    Service Integration\n    Action Support\n Synthetic Data Generation\n     Ground Truth\n     Sensor Simulation\n     Annotation Tools\n```\n\n### Extensions System\nIsaac Sim uses an extension-based architecture:\n- **Isaac Sim Robotics**: Core robotics functionality\n- **Isaac Sim Navigation**: Navigation and path planning\n- **Isaac Sim Perception**: Computer vision and sensor simulation\n- **Isaac Sim Manipulation**: Grasping and manipulation\n\n## Creating Your First Scene\n\n### Basic Robot Setup\nUsing Isaac Sim's Python API, you can create scenes programmatically:\n\n```python\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\n\n# Create world instance\nworld = World(stage_units_in_meters=1.0)\n\n# Add a robot to the stage\nassets_root_path = get_assets_root_path()\ncarter_asset_path = assets_root_path + \"/Isaac/Robots/Carter/carter.usd\"\nadd_reference_to_stage(usd_path=carter_asset_path, prim_path=\"/World/Carter\")\n\n# Initialize the world\nworld.reset()\n```\n\n### Scene Configuration\nConfigure your scene with proper physics and rendering settings:\n\n```python\n# Configure physics settings\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.physics import set_physics_dt\n\nworld = World(stage_units_in_meters=1.0)\n\n# Set physics time step\nset_physics_dt(physics_dt=1.0/60.0, physics_substeps=1)\n\n# Configure rendering settings\nimport carb\nsettings = carb.settings.get_settings()\nsettings.set(\"/app/window/spp\", 8)  # Samples per pixel\nsettings.set(\"/rtx/antiAliasing/accAntiAliasing\", True)\n```\n\n## Sensor Simulation\n\n### Camera Simulation\nIsaac Sim provides realistic camera simulation with various properties:\n\n```python\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\n# Create a camera sensor\ncamera = Camera(\n    prim_path=\"/World/Carter/chassis/camera\",\n    frequency=30,\n    resolution=(640, 480)\n)\n\n# Enable various camera outputs\ncamera.add_distortion_to_sensor(\n    distortion_model=\"fisheye\",\n    focal_length=12.0,\n    horizontal_aperture=20.955,\n    distortion_coefficient=0.8\n)\n\n# Get camera data\nrgb_data = camera.get_rgb()\ndepth_data = camera.get_depth()\nseg_data = camera.get_semantic_segmentation()\n```\n\n### LIDAR Simulation\n```python\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\n\n# Create a LIDAR sensor\nlidar = RotatingLidarPhysX(\n    prim_path=\"/World/Carter/chassis/lidar\",\n    translation=np.array([0, 0, 0.5]),\n    orientation=np.array([0, 0, 0, 1]),\n    config=\"Yosemite\",\n    rotation_frequency=10,\n    samples_per_scan=1000\n)\n\n# Get LIDAR data\nlidar_data = lidar.get_linear_depth_data()\n```\n\n## ROS/ROS2 Integration\n\n### Setting up ROS Bridge\nIsaac Sim provides seamless ROS/ROS2 integration through extensions:\n\n1. Enable the Isaac ROS Bridge extension in Isaac Sim\n2. Add ROS Bridge node to your scene\n3. Configure topic mappings and message types\n\n### Publishing Sensor Data\n```python\n# Example of publishing camera data to ROS2\nimport rclpy\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\n\nclass IsaacSimROSPublisher:\n    def __init__(self):\n        self.node = rclpy.create_node('isaac_sim_publisher')\n        self.image_pub = self.node.create_publisher(Image, '/camera/image_raw', 10)\n        self.camera_info_pub = self.node.create_publisher(CameraInfo, '/camera/camera_info', 10)\n        self.bridge = CvBridge()\n\n    def publish_camera_data(self, rgb_image):\n        ros_image = self.bridge.cv2_to_imgmsg(rgb_image, encoding=\"rgb8\")\n        ros_image.header.stamp = self.node.get_clock().now().to_msg()\n        ros_image.header.frame_id = \"camera_link\"\n        self.image_pub.publish(ros_image)\n```\n\n## Synthetic Data Generation\n\n### Ground Truth Annotation\nIsaac Sim can generate various types of ground truth data including:\n- Semantic segmentation masks\n- Instance segmentation masks\n- Depth maps\n- 3D bounding boxes\n- Pose annotations\n\n### Data Pipeline\nCreate a pipeline for synthetic data generation with realistic variations in lighting, textures, and object placements.\n\n## Performance Optimization\n\n### Level of Detail (LOD)\nUse LOD systems to maintain performance with complex scenes.\n\n### Rendering Optimization\nConfigure rendering settings appropriately for simulation performance versus visual quality needs.\n\n## Troubleshooting Common Issues\n\n### Performance Issues\n- Slow simulation: Reduce physics substeps or rendering quality\n- High GPU usage: Lower rendering resolution or disable advanced effects\n- Memory issues: Reduce scene complexity or use streaming assets\n\n### ROS Integration Issues\n- Connection failures: Check network settings and firewall\n- Message delays: Optimize publishing frequency\n- TF issues: Verify coordinate frame conventions\n\n### Physics Issues\n- Unstable simulation: Adjust solver parameters\n- Interpenetration: Improve collision geometry\n- Jittery movement: Increase physics frequency\n\n## Best Practices\n\n### Scene Organization\n- Use consistent naming conventions\n- Organize objects in logical hierarchies\n- Use tags and labels for easy selection\n\n### Asset Management\n- Use relative paths for portability\n- Organize assets in a clear directory structure\n- Use version control for scene files\n\n### Performance\n- Start simple and add complexity gradually\n- Profile regularly to identify bottlenecks\n- Use appropriate level of detail for your needs\n\n### Reproducibility\n- Document your scene configurations\n- Use version control for scenes and scripts\n- Create configuration files for different scenarios\n\nIsaac Sim provides a powerful platform for developing and testing robotics applications in a safe, controlled environment before deploying to real robots. Its integration with the Isaac ROS ecosystem makes it particularly valuable for AI-powered robotics development.",
    "path": "module-3-ai-perception\\isaac-sim-fundamentals.md",
    "description": ""
  },
  "module-3-ai-perception\\isaac-sim": {
    "title": "module-3-ai-perception\\isaac-sim",
    "content": "# Isaac Sim Fundamentals\n\nNVIDIA Isaac Sim is a high-fidelity simulation environment built on NVIDIA Omniverse, designed for developing, testing, and validating AI-based robotics applications. This section covers the fundamentals of Isaac Sim and its role in robotics development.\n\n## Introduction to Isaac Sim\n\nIsaac Sim provides:\n- **Photorealistic simulation**: Physically accurate rendering with RTX real-time ray tracing\n- **Synthetic data generation**: High-quality training data for AI models\n- **Physics simulation**: Accurate rigid body dynamics, soft body simulation, and fluid dynamics\n- **Sensor simulation**: Realistic camera, LIDAR, RADAR, IMU, and other sensor models\n- **AI integration**: Built-in tools for training and testing AI models\n- **ROS/ROS2 bridge**: Seamless integration with ROS/ROS2 ecosystems\n\n## System Requirements\n\nIsaac Sim has demanding system requirements due to its high-fidelity simulation:\n\n### Minimum Requirements\n- **GPU**: NVIDIA RTX 3080 or equivalent with 10GB+ VRAM\n- **CPU**: 8+ core processor (Intel i7 / AMD Ryzen 7 or better)\n- **RAM**: 32GB system memory\n- **Storage**: 50GB+ free space\n- **OS**: Ubuntu 20.04/22.04 or Windows 10/11\n\n### Recommended Requirements\n- **GPU**: NVIDIA RTX 4080/4090 or A40/A6000 with 24GB+ VRAM\n- **CPU**: 16+ core processor with high IPC\n- **RAM**: 64GB+ system memory\n- **Storage**: NVMe SSD with 100GB+ free space\n\n## Installation and Setup\n\n### Prerequisites\nBefore installing Isaac Sim, ensure you have:\n- NVIDIA GPU with CUDA support\n- NVIDIA drivers (520+ recommended)\n- CUDA toolkit (11.8+ recommended)\n- Docker (optional, for containerized deployment)\n\n### Installation Methods\n\n#### 1. Omniverse Launcher (Recommended for beginners)\n1. Download NVIDIA Omniverse Launcher from the NVIDIA Developer website\n2. Install Isaac Sim through the launcher\n3. Launch Isaac Sim directly from the launcher\n\n#### 2. Containerized Installation (Recommended for production)\n```bash\n# Pull the Isaac Sim Docker image\ndocker pull nvcr.io/nvidia/isaac-sim:4.0.0\n\n# Run Isaac Sim in a container\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env \"ACCEPT_EULA=Y\" \\\n  --env \"NVIDIA_VISIBLE_DEVICES=all\" \\\n  --env \"NVIDIA_DRIVER_CAPABILITIES=all\" \\\n  --volume /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  --volume $HOME/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n  --volume $HOME/docker/isaac-sim/assets:/isaac-sim/assets:rw \\\n  --volume $HOME/docker/isaac-sim/home:/isaac-sim/home:rw \\\n  nvcr.io/nvidia/isaac-sim:4.0.0\n```\n\n#### 3. Native Installation\nFor native installation, follow the Isaac Sim documentation for your specific platform.\n\n## Core Concepts\n\n### USD (Universal Scene Description)\nIsaac Sim uses NVIDIA's Universal Scene Description (USD) format for scene representation:\n- **Hierarchical structure**: Organized in a tree-like structure\n- **Layered composition**: Multiple layers can be composed together\n- **Schema system**: Standardized object types and properties\n- **Variant sets**: Different configurations of the same object\n\n### Omniverse Nucleus\nThe collaborative platform that enables:\n- Multi-user editing of scenes\n- Asset management and sharing\n- Real-time synchronization across users\n\n### Physics Simulation\nIsaac Sim uses PhysX for physics simulation:\n- **Rigid body dynamics**: Accurate collision detection and response\n- **Soft body simulation**: Deformable objects and cloth simulation\n- **Fluid dynamics**: Liquid and gas simulation\n- **Vehicle dynamics**: Realistic vehicle physics\n\n## Isaac Sim Architecture\n\n### Core Components\n```\nIsaac Sim\n Omniverse Kit (Foundation)\n    Physics Engine (PhysX)\n    Rendering Engine (RTX)\n    USD Scene Management\n    UI Framework\n Isaac Extensions\n    Robotics Extensions\n    Perception Extensions\n    Navigation Extensions\n    Manipulation Extensions\n ROS/ROS2 Bridge\n    Message Conversion\n    Service Integration\n    Action Support\n Synthetic Data Generation\n     Ground Truth\n     Sensor Simulation\n     Annotation Tools\n```\n\n### Extensions System\nIsaac Sim uses an extension-based architecture:\n- **Isaac Sim Robotics**: Core robotics functionality\n- **Isaac Sim Navigation**: Navigation and path planning\n- **Isaac Sim Perception**: Computer vision and sensor simulation\n- **Isaac Sim Manipulation**: Grasping and manipulation\n\n## Creating Your First Scene\n\n### Basic Robot Setup\n```python\n# Python API example for creating a simple scene\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\n\n# Create world instance\nworld = World(stage_units_in_meters=1.0)\n\n# Add a simple robot to the stage\n# This could be a URDF import or a native USD robot\nasset_root_path = get_assets_root_path()\ncarter_asset_path = asset_root_path + \"/Isaac/Robots/Carter/carter.usd\"\n\nadd_reference_to_stage(usd_path=carter_asset_path, prim_path=\"/World/Carter\")\n\n# Initialize the world\nworld.reset()\n```\n\n### Scene Configuration\nConfigure your scene with proper physics and rendering settings:\n\n```python\n# Configure physics settings\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.physics import set_physics_dt\n\nworld = World(stage_units_in_meters=1.0)\n\n# Set physics time step\nset_physics_dt(physics_dt=1.0/60.0, physics_substeps=1)\n\n# Configure rendering settings\nimport carb\nsettings = carb.settings.get_settings()\nsettings.set(\"/app/window/spp\", 8)  # Samples per pixel\nsettings.set(\"/rtx/antiAliasing/accAntiAliasing\", True)\n```\n\n## Sensor Simulation\n\n### Camera Simulation\nIsaac Sim provides realistic camera simulation:\n\n```python\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\n# Create a camera sensor\ncamera = Camera(\n    prim_path=\"/World/Carter/chassis/camera\",\n    frequency=30,\n    resolution=(640, 480)\n)\n\n# Enable various camera outputs\ncamera.add_distortion_to_sensor(\n    distortion_model=\"fisheye\",\n    focal_length=12.0,\n    horizontal_aperture=20.955,\n    distortion_coefficient=0.8\n)\n\n# Get camera data\nrgb_data = camera.get_rgb()\ndepth_data = camera.get_depth()\nseg_data = camera.get_semantic_segmentation()\n```\n\n### LIDAR Simulation\n```python\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\n\n# Create a LIDAR sensor\nlidar = RotatingLidarPhysX(\n    prim_path=\"/World/Carter/chassis/lidar\",\n    translation=np.array([0, 0, 0.5]),\n    orientation=np.array([0, 0, 0, 1]),\n    config=\"Yosemite\",\n    rotation_frequency=10,\n    samples_per_scan=1000\n)\n\n# Get LIDAR data\nlidar_data = lidar.get_linear_depth_data()\n```\n\n## ROS/ROS2 Integration\n\n### Setting up ROS Bridge\nIsaac Sim provides seamless ROS/ROS2 integration:\n\n```bash\n# Install ROS bridge extensions\n# In Isaac Sim, go to Window  Extensions  Isaac ROS Bridge\n# Enable the extensions you need:\n# - ROS2 Bridge\n# - ROS Bridge\n# - Various sensor bridges\n```\n\n### Publishing Sensor Data\n```python\n# Example of publishing camera data to ROS2\nimport rclpy\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\n\nclass IsaacSimROSPublisher:\n    def __init__(self):\n        self.node = rclpy.create_node('isaac_sim_publisher')\n        self.image_pub = self.node.create_publisher(Image, '/camera/image_raw', 10)\n        self.camera_info_pub = self.node.create_publisher(CameraInfo, '/camera/camera_info', 10)\n        self.bridge = CvBridge()\n\n    def publish_camera_data(self, rgb_image):\n        ros_image = self.bridge.cv2_to_imgmsg(rgb_image, encoding=\"rgb8\")\n        ros_image.header.stamp = self.node.get_clock().now().to_msg()\n        ros_image.header.frame_id = \"camera_link\"\n        self.image_pub.publish(ros_image)\n```\n\n## Synthetic Data Generation\n\n### Ground Truth Annotation\nIsaac Sim can generate various types of ground truth data:\n\n```python\n# Generate semantic segmentation\nfrom omni.isaac.core.utils.semantics import add_semantic_bboxes\nfrom pxr import UsdGeom, Usd, Sdf\n\n# Add semantic labels to objects\nstage = omni.usd.get_context().get_stage()\nprim = stage.GetPrimAtPath(\"/World/Box\")\nadd_semantic_bboxes([prim], \"box\", \"object\")\n\n# Generate instance segmentation\nfrom omni.isaac.synthetic_utils import visualize_segmentation_observations\n\n# This generates instance segmentation masks for training data\n```\n\n### Data Pipeline\nCreate a pipeline for synthetic data generation:\n\n```python\n# Example synthetic data generation script\nimport omni\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport cv2\n\ndef generate_training_data():\n    # Initialize synthetic data helper\n    sd_helper = SyntheticDataHelper()\n\n    # Configure output types\n    output_types = [\n        \"rgb\",\n        \"depth_linear\",\n        \"instance_segmentation\",\n        \"bounding_box_2d_tight\"\n    ]\n\n    # Generate multiple variations\n    for i in range(1000):  # Generate 1000 training samples\n        # Randomize scene\n        randomize_scene()\n\n        # Capture data\n        data = sd_helper.get_data(output_types)\n\n        # Save data with annotations\n        save_training_sample(data, f\"training_data_{i:04d}\")\n\n        # Reset for next sample\n        reset_scene()\n\ndef save_training_sample(data, filename):\n    # Save RGB image\n    cv2.imwrite(f\"{filename}_rgb.png\", cv2.cvtColor(data[\"rgb\"], cv2.COLOR_RGB2BGR))\n\n    # Save depth map\n    np.save(f\"{filename}_depth.npy\", data[\"depth_linear\"])\n\n    # Save segmentation mask\n    cv2.imwrite(f\"{filename}_seg.png\", data[\"instance_segmentation\"])\n\n    # Save bounding boxes\n    np.save(f\"{filename}_bbox.npy\", data[\"bounding_box_2d_tight\"])\n```\n\n## Performance Optimization\n\n### Level of Detail (LOD)\nUse LOD systems to maintain performance:\n\n```python\n# Configure LOD for complex models\nfrom pxr import UsdGeom, Usd\n\ndef setup_lod(prim_path, lod_paths, distances):\n    \"\"\"\n    Set up Level of Detail for a prim\n    lod_paths: List of USD paths for different LODs\n    distances: List of distances at which to switch LODs\n    \"\"\"\n    stage = omni.usd.get_context().get_stage()\n    prim = stage.GetPrimAtPath(prim_path)\n\n    # Add LOD properties\n    UsdGeom.LOD.AddLOD(prim, distances, lod_paths)\n```\n\n### Rendering Optimization\n```python\n# Optimize rendering settings for simulation\nimport carb\n\nsettings = carb.settings.get_settings()\n\n# Reduce rendering quality for better simulation performance\nsettings.set(\"/app/performace/range\", 2)  # Performance level\nsettings.set(\"/rtx/indirectDiffuseLighting/enabled\", False)\nsettings.set(\"/rtx/dlss/enable\", False)  # Disable DLSS if not needed\nsettings.set(\"/rtx/reflections/enabled\", False)  # Disable reflections for performance\n```\n\n## Troubleshooting Common Issues\n\n### Performance Issues\n- **Slow simulation**: Reduce physics substeps or rendering quality\n- **High GPU usage**: Lower rendering resolution or disable advanced effects\n- **Memory issues**: Reduce scene complexity or use streaming assets\n\n### ROS Integration Issues\n- **Connection failures**: Check network settings and firewall\n- **Message delays**: Optimize publishing frequency\n- **TF issues**: Verify coordinate frame conventions\n\n### Physics Issues\n- **Unstable simulation**: Adjust solver parameters\n- **Interpenetration**: Improve collision geometry\n- **Jittery movement**: Increase physics frequency\n\n## Best Practices\n\n### 1. Scene Organization\n- Use consistent naming conventions\n- Organize objects in logical hierarchies\n- Use tags and labels for easy selection\n\n### 2. Asset Management\n- Use relative paths for portability\n- Organize assets in a clear directory structure\n- Use version control for scene files\n\n### 3. Performance\n- Start simple and add complexity gradually\n- Profile regularly to identify bottlenecks\n- Use appropriate level of detail for your needs\n\n### 4. Reproducibility\n- Document your scene configurations\n- Use version control for scenes and scripts\n- Create configuration files for different scenarios\n\n## Exercise\n\nCreate a complete Isaac Sim scene that includes:\n1. A humanoid robot with proper physics properties\n2. Multiple sensor types (camera, LIDAR, IMU)\n3. A complex environment with various objects\n4. ROS2 bridge configuration for sensor data publishing\n5. Synthetic data generation pipeline for training datasets\n\nValidate your scene by running it and verifying that all sensors publish data correctly and the robot behaves realistically in the physics simulation.",
    "path": "module-3-ai-perception\\isaac-sim.md",
    "description": ""
  },
  "module-3-ai-perception\\nav2-locomotion": {
    "title": "module-3-ai-perception\\nav2-locomotion",
    "content": "# Nav2 for Humanoid Locomotion\n\nNavigation 2 (Nav2) is the ROS 2 navigation stack that enables autonomous navigation for robots. For humanoid robots, Nav2 requires special configuration to handle the unique challenges of bipedal locomotion. This section covers configuring and using Nav2 for humanoid robot navigation.\n\n## Introduction to Nav2\n\nNav2 is a complete navigation system that includes:\n- **Global Planner**: Plans path from start to goal\n- **Local Planner**: Executes path while avoiding obstacles\n- **Controller**: Sends commands to robot actuators\n- **Costmaps**: Represents obstacles and free space\n- **Behavior Trees**: Orchestrates navigation behaviors\n- **Recovery Behaviors**: Handles navigation failures\n\n## Nav2 Architecture\n\n### Core Components\n```\nNav2 System\n Navigation Server\n    Global Planner (NavFn, A*, etc.)\n    Local Planner (DWA, TEB, etc.)\n    Controller (PID, MPC, etc.)\n Costmap Server\n    Global Costmap\n    Local Costmap\n Lifecycle Manager\n Behavior Tree Navigator\n Recovery Server\n Transform Management (TF2)\n```\n\n### Humanoid-Specific Considerations\nHumanoid robots require special handling in Nav2:\n- **Bipedal dynamics**: Different from wheeled robots\n- **Balance constraints**: Must maintain center of mass\n- **Step planning**: Requires discrete foot placement\n- **Stair navigation**: Special locomotion patterns needed\n\n## Nav2 Configuration for Humanoid Robots\n\n### Basic Parameters Configuration\n\n```yaml\n# nav2_params_humanoid.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: \"base_link\"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: \"map\"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: \"likelihood_field\"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: \"odom\"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: \"nav2_amcl::DifferentialMotionModel\"  # This needs humanoid-specific model\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.2\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\namcl_map_client:\n  ros__parameters:\n    use_sim_time: True\n\namcl_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_through_poses_bt_xml: nav2_bt_navigator/navigate_through_poses_w_replanning_and_recovery.xml\n    default_nav_to_pose_bt_xml: nav2_bt_navigator/navigate_to_pose_w_replanning_and_recovery.xml\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_compute_path_through_poses_action_bt_node\n    - nav2_smooth_path_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_assisted_teleop_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_drive_on_heading_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_globally_consistent_condition_bt_node\n    - nav2_is_path_valid_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_truncate_path_local_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n    - nav2_controller_cancel_bt_node\n    - nav2_path_longer_on_approach_bt_node\n    - nav2_wait_cancel_bt_node\n    - nav2_spin_cancel_bt_node\n    - nav2_back_up_cancel_bt_node\n    - nav2_assisted_teleop_cancel_bt_node\n    - nav2_drive_on_heading_cancel_bt_node\n\nbt_navigator_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: \"progress_checker\"\n    goal_checker_plugin: \"goal_checker\"\n    controller_plugins: [\"FollowPath\"]\n\n    # Humanoid-specific controller\n    FollowPath:\n      plugin: \"nav2_mppi_controller::MppiController\"  # Or custom humanoid controller\n      time_steps: 20\n      control_frequency: 10.0\n      motion_model: \"DiffDrive\"  # Need humanoid-specific model\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.25\n      state_bounds_planner: 0.3\n      control_bounds_planner: 0.5\n      control_bounds: 1.0\n      state_bounds: 0.5\n      dt: 0.05\n      noise_coefficient: 0.0\n      convergence_integrator_gain: 0.1\n      oscillation_threshold: 0.01\n      oscillation_window: 10\n\ncontroller_server_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_link\n      use_sim_time: True\n      rolling_window: true\n      width: 6\n      height: 6\n      resolution: 0.05\n      robot_radius: 0.3  # Adjust for humanoid robot size\n      plugins: [\"voxel_layer\", \"inflation_layer\"]\n      inflation_layer:\n        plugin: \"nav2_costmap_2d::InflationLayer\"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      voxel_layer:\n        plugin: \"nav2_costmap_2d::VoxelLayer\"\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.2\n        z_voxels: 8\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: \"LaserScan\"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n  local_costmap_client:\n    ros__parameters:\n      use_sim_time: True\n  local_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: True\n\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 1.0\n      publish_frequency: 1.0\n      global_frame: map\n      robot_base_frame: base_link\n      use_sim_time: True\n      robot_radius: 0.3  # Adjust for humanoid robot\n      resolution: 0.05\n      track_unknown_space: true\n      plugins: [\"static_layer\", \"obstacle_layer\", \"inflation_layer\"]\n      obstacle_layer:\n        plugin: \"nav2_costmap_2d::ObstacleLayer\"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: \"LaserScan\"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      static_layer:\n        plugin: \"nav2_costmap_2d::StaticLayer\"\n        map_subscribe_transient_local: True\n      inflation_layer:\n        plugin: \"nav2_costmap_2d::InflationLayer\"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n  global_costmap_client:\n    ros__parameters:\n      use_sim_time: True\n  global_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: True\n\nmap_server:\n  ros__parameters:\n    use_sim_time: True\n    yaml_filename: \"turtlebot3_world.yaml\"\n\nmap_saver:\n  ros__parameters:\n    use_sim_time: True\n    save_map_timeout: 5.0\n    free_thresh_default: 0.25\n    occupied_thresh_default: 0.65\n\nplanner_server:\n  ros__parameters:\n    use_sim_time: True\n    planner_plugins: [\"GridBased\"]\n    GridBased:\n      plugin: \"nav2_navfn_planner::NavfnPlanner\"\n      tolerance: 0.5\n      use_astar: false\n      allow_unknown: true\n\nplanner_server_rclcpp_node:\n  ros__parameters:\n    use_sim_time: True\n\nrecoveries_server:\n  ros__parameters:\n    use_sim_time: True\n    recovery_plugins: [\"spin\", \"backup\", \"wait\"]\n    spin:\n      plugin: \"nav2_recoveries::Spin\"\n      ideal_linear_velocity: 0.0\n      ideal_angular_velocity: 1.0\n      tolerance: 1.57\n      sampling_frequency: 20.0\n      cmd_vel_timeout: 1.0\n    backup:\n      plugin: \"nav2_recoveries::BackUp\"\n      ideal_linear_velocity: -0.1\n      ideal_angular_velocity: 0.0\n      tolerance: 0.15\n      sampling_frequency: 10.0\n      cmd_vel_timeout: 1.0\n    wait:\n      plugin: \"nav2_recoveries::Wait\"\n      sleep_duration: 2.0\n      sampling_frequency: 10.0\n      cmd_vel_timeout: 1.0\n\nrobot_state_publisher:\n  ros__parameters:\n    use_sim_time: True\n```\n\n## Humanoid-Specific Navigation Challenges\n\n### 1. Bipedal Dynamics\nHumanoid robots have different locomotion characteristics:\n\n```cpp\n// Humanoid motion model for navigation\nclass HumanoidMotionModel\n{\npublic:\n    HumanoidMotionModel()\n    {\n        // Humanoid-specific parameters\n        step_length_ = 0.3;      // Average step length\n        step_duration_ = 0.8;    // Time for one step\n        max_step_width_ = 0.2;   // Maximum lateral step\n        balance_margin_ = 0.1;   // Safety margin for balance\n    }\n\n    geometry_msgs::msg::Pose predictPose(\n        const geometry_msgs::msg::Pose& current_pose,\n        const geometry_msgs::msg::Twist& cmd_vel,\n        double dt)\n    {\n        // Implement humanoid-specific motion prediction\n        // Consider step-by-step locomotion instead of continuous motion\n        geometry_msgs::msg::Pose predicted_pose = current_pose;\n\n        // Calculate step-based movement\n        double steps = dt / step_duration_;\n        double step_dx = cmd_vel.linear.x * step_duration_;\n        double step_dy = cmd_vel.linear.y * step_duration_;\n        double step_dtheta = cmd_vel.angular.z * step_duration_;\n\n        // Apply step constraints\n        step_dx = std::min(step_dx, step_length_);\n        step_dy = std::min(step_dy, max_step_width_);\n\n        // Update pose based on steps\n        predicted_pose.position.x += step_dx * steps;\n        predicted_pose.position.y += step_dy * steps;\n\n        // Update orientation\n        tf2::Quaternion q(\n            predicted_pose.orientation.x,\n            predicted_pose.orientation.y,\n            predicted_pose.orientation.z,\n            predicted_pose.orientation.w\n        );\n        tf2::Matrix3x3 m(q);\n        double roll, pitch, yaw;\n        m.getRPY(roll, pitch, yaw);\n        yaw += step_dtheta * steps;\n\n        // Convert back to quaternion\n        q.setRPY(roll, pitch, yaw);\n        predicted_pose.orientation.x = q.x();\n        predicted_pose.orientation.y = q.y();\n        predicted_pose.orientation.z = q.z();\n        predicted_pose.orientation.w = q.w();\n\n        return predicted_pose;\n    }\n\nprivate:\n    double step_length_;\n    double step_duration_;\n    double max_step_width_;\n    double balance_margin_;\n};\n```\n\n### 2. Step Planning\nHumanoid robots need discrete step planning:\n\n```cpp\n// Step planner for humanoid navigation\nclass StepPlanner\n{\npublic:\n    struct Step\n    {\n        geometry_msgs::msg::Point left_foot;\n        geometry_msgs::msg::Point right_foot;\n        double time;\n    };\n\n    std::vector<Step> planSteps(\n        const geometry_msgs::msg::Pose& start,\n        const geometry_msgs::msg::Pose& goal,\n        const nav_msgs::msg::Path& global_path)\n    {\n        std::vector<Step> steps;\n\n        // Plan discrete steps along the path\n        // Ensure each step maintains balance\n        for (size_t i = 0; i < global_path.poses.size(); i += step_spacing_)\n        {\n            Step step;\n            step.left_foot = calculateLeftFootPosition(global_path.poses[i].pose);\n            step.right_foot = calculateRightFootPosition(global_path.poses[i].pose);\n            step.time = i * step_duration_;\n\n            // Verify step is balanced\n            if (isStepBalanced(step))\n            {\n                steps.push_back(step);\n            }\n        }\n\n        return steps;\n    }\n\nprivate:\n    bool isStepBalanced(const Step& step)\n    {\n        // Check if the step maintains center of mass within support polygon\n        geometry_msgs::msg::Point com = calculateCOM();\n        return isInSupportPolygon(com, step);\n    }\n\n    geometry_msgs::msg::Point calculateLeftFootPosition(const geometry_msgs::msg::Pose& pose);\n    geometry_msgs::msg::Point calculateRightFootPosition(const geometry_msgs::msg::Pose& pose);\n    geometry_msgs::msg::Point calculateCOM();\n    bool isInSupportPolygon(const geometry_msgs::msg::Point& com, const Step& step);\n\n    size_t step_spacing_ = 2;  // Plan every 2nd point from path\n    double step_duration_ = 0.8;\n};\n```\n\n### 3. Balance Controller\nMaintain balance during navigation:\n\n```cpp\n// Balance controller for humanoid navigation\nclass BalanceController\n{\npublic:\n    BalanceController()\n    {\n        // Initialize balance control parameters\n        com_height_ = 0.8;  // Height of center of mass\n        control_frequency_ = 100.0;  // Balance control frequency\n    }\n\n    geometry_msgs::msg::Twist computeBalanceControl(\n        const geometry_msgs::msg::Pose& current_pose,\n        const geometry_msgs::msg::Twist& desired_twist)\n    {\n        geometry_msgs::msg::Twist balance_twist = desired_twist;\n\n        // Calculate center of mass position and velocity\n        geometry_msgs::msg::Point com = calculateCOM();\n        geometry_msgs::msg::Vector3 com_vel = calculateCOMVelocity();\n\n        // Calculate Zero Moment Point (ZMP)\n        geometry_msgs::msg::Point zmp = calculateZMP(com, com_vel);\n\n        // Calculate balance error\n        double balance_error_x = zmp.x - desired_zmp_.x;\n        double balance_error_y = zmp.y - desired_zmp_.y;\n\n        // Apply balance correction using PID control\n        balance_twist.linear.x += balance_pid_x_.compute(balance_error_x);\n        balance_twist.linear.y += balance_pid_y_.compute(balance_error_y);\n\n        // Limit corrections to maintain stability\n        balance_twist.linear.x = std::clamp(balance_twist.linear.x, -0.1, 0.1);\n        balance_twist.linear.y = std::clamp(balance_twist.linear.y, -0.05, 0.05);\n\n        return balance_twist;\n    }\n\nprivate:\n    geometry_msgs::msg::Point calculateCOM();\n    geometry_msgs::msg::Vector3 calculateCOMVelocity();\n    geometry_msgs::msg::Point calculateZMP(\n        const geometry_msgs::msg::Point& com,\n        const geometry_msgs::msg::Vector3& com_vel);\n\n    geometry_msgs::msg::Point desired_zmp_;\n    double com_height_;\n    double control_frequency_;\n\n    // PID controllers for balance\n    PIDController balance_pid_x_;\n    PIDController balance_pid_y_;\n};\n```\n\n## Nav2 Launch Configuration\n\n### Complete Launch File\n\n```python\n# launch/humanoid_nav2.launch.py\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, SetEnvironmentVariable\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nfrom nav2_common.launch import RewrittenYaml\n\n\ndef generate_launch_description():\n    # Get the launch directory\n    package_dir = get_package_share_directory('humanoid_navigation')\n\n    # Create the launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    autostart = LaunchConfiguration('autostart')\n    params_file = LaunchConfiguration('params_file')\n    bt_xml_filename = LaunchConfiguration('bt_xml_filename')\n    map_subscribe_transient_local = LaunchConfiguration('map_subscribe_transient_local')\n\n    # Declare the launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation (Gazebo) clock if true')\n\n    declare_autostart = DeclareLaunchArgument(\n        'autostart',\n        default_value='true',\n        description='Automatically startup the nav2 stack')\n\n    declare_params_file = DeclareLaunchArgument(\n        'params_file',\n        default_value=os.path.join(package_dir, 'config', 'nav2_params_humanoid.yaml'),\n        description='Full path to the ROS2 parameters file to use for all launched nodes')\n\n    declare_bt_xml = DeclareLaunchArgument(\n        'bt_xml_filename',\n        default_value=os.path.join(\n            get_package_share_directory('nav2_bt_navigator'),\n            'behavior_trees', 'navigate_w_replanning_and_recovery.xml'),\n        description='Full path to the behavior tree xml file to use')\n\n    declare_map_subscribe_transient_local = DeclareLaunchArgument(\n        'map_subscribe_transient_local',\n        default_value='false',\n        description='Whether to set the map subscriber QoS to transient local')\n\n    # Make sure we have the right parameters file\n    param_substitutions = {\n        'use_sim_time': use_sim_time,\n        'autostart': autostart,\n        'bt_xml_filename': bt_xml_filename,\n        'map_subscribe_transient_local': map_subscribe_transient_local}\n\n    configured_params = RewrittenYaml(\n        source_file=params_file,\n        root_key='',\n        param_rewrites=param_substitutions,\n        convert_types=True)\n\n    # Specify the actions\n    lifecycle_nodes = ['controller_server',\n                       'planner_server',\n                       'recoveries_server',\n                       'bt_navigator',\n                       'waypoint_follower']\n\n    # Create the node\n    navigation_server = Node(\n        package='nav2_lifecycle_manager',\n        executable='lifecycle_manager',\n        name='lifecycle_manager_navigation',\n        output='screen',\n        parameters=[{'use_sim_time': use_sim_time},\n                    {'autostart': autostart},\n                    {'node_names': lifecycle_nodes}])\n\n    # Localization node (AMCL)\n    localization = Node(\n        package='nav2_amcl',\n        executable='amcl',\n        name='amcl',\n        output='screen',\n        parameters=[configured_params])\n\n    # Controller server\n    controller_server = Node(\n        package='nav2_controller',\n        executable='controller_server',\n        output='screen',\n        parameters=[configured_params])\n\n    # Planner server\n    planner_server = Node(\n        package='nav2_planner',\n        executable='planner_server',\n        name='planner_server',\n        output='screen',\n        parameters=[configured_params])\n\n    # BT navigator\n    bt_navigator = Node(\n        package='nav2_bt_navigator',\n        executable='bt_navigator',\n        name='bt_navigator',\n        output='screen',\n        parameters=[configured_params])\n\n    # Recovery server\n    recovery_server = Node(\n        package='nav2_recoveries',\n        executable='recoveries_server',\n        name='recoveries_server',\n        output='screen',\n        parameters=[configured_params])\n\n    # Waypoint follower\n    waypoint_follower = Node(\n        package='nav2_waypoint_follower',\n        executable='waypoint_follower',\n        name='waypoint_follower',\n        output='screen',\n        parameters=[configured_params])\n\n    # Add the actions to the launch description\n    ld = LaunchDescription()\n\n    # Declare the launch options\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_autostart)\n    ld.add_action(declare_params_file)\n    ld.add_action(declare_bt_xml)\n    ld.add_action(declare_map_subscribe_transient_local)\n\n    # Add the nodes to the launch description\n    ld.add_action(navigation_server)\n    ld.add_action(localization)\n    ld.add_action(controller_server)\n    ld.add_action(planner_server)\n    ld.add_action(bt_navigator)\n    ld.add_action(recovery_server)\n    ld.add_action(waypoint_follower)\n\n    return ld\n```\n\n## Custom Controllers for Humanoid Robots\n\n### Humanoid Path Following Controller\n\n```cpp\n#include <nav2_core/controller.hpp>\n#include <nav2_util/lifecycle_node.hpp>\n#include <nav2_costmap_2d/costmap_2d_ros.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n\nclass HumanoidController : public nav2_core::Controller\n{\npublic:\n    HumanoidController() = default;\n    ~HumanoidController() = default;\n\n    void configure(\n        const rclcpp_lifecycle::LifecycleNode::WeakPtr & parent,\n        std::string name,\n        const std::shared_ptr<tf2_ros::Buffer> & tf,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & costmap_ros) override\n    {\n        node_ = parent.lock();\n        name_ = name;\n        tf_ = tf;\n        costmap_ros_ = costmap_ros;\n        costmap_ = costmap_ros_->getCostmap();\n\n        // Initialize humanoid-specific parameters\n        step_frequency_ = node_->declare_parameter(name_ + \".step_frequency\", 1.25);\n        max_step_length_ = node_->declare_parameter(name_ + \".max_step_length\", 0.3);\n        balance_margin_ = node_->declare_parameter(name_ + \".balance_margin\", 0.1);\n\n        RCLCPP_INFO(node_->get_logger(), \"HumanoidController configured\");\n    }\n\n    void cleanup() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"HumanoidController cleaned up\");\n    }\n\n    void activate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"HumanoidController activated\");\n    }\n\n    void deactivate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"HumanoidController deactivated\");\n    }\n\n    geometry_msgs::msg::Twist computeVelocityCommands(\n        const geometry_msgs::msg::PoseStamped & pose,\n        const geometry_msgs::msg::Twist & velocity,\n        nav2_core::GoalChecker * goal_checker) override\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        if (current_path_.poses.empty()) {\n            RCLCPP_WARN(node_->get_logger(), \"No path received, stopping robot\");\n            return cmd_vel;  // Return zero velocity\n        }\n\n        // Find closest point on path\n        size_t closest_idx = findClosestPoseIndex(pose, current_path_);\n\n        // Calculate desired velocity based on path\n        cmd_vel = calculatePathFollowingVelocity(pose, current_path_, closest_idx);\n\n        // Apply humanoid-specific constraints\n        cmd_vel = applyHumanoidConstraints(cmd_vel, pose);\n\n        // Check for obstacles in local costmap\n        if (isPathObstructed(pose, cmd_vel)) {\n            cmd_vel = handleObstacles(cmd_vel, pose);\n        }\n\n        return cmd_vel;\n    }\n\n    void setPlan(const nav_msgs::msg::Path & path) override\n    {\n        current_path_ = path;\n        RCLCPP_INFO(node_->get_logger(), \"New plan set with %zu waypoints\", path.poses.size());\n    }\n\nprivate:\n    size_t findClosestPoseIndex(\n        const geometry_msgs::msg::PoseStamped & pose,\n        const nav_msgs::msg::Path & path)\n    {\n        double min_dist = std::numeric_limits<double>::max();\n        size_t closest_idx = 0;\n\n        for (size_t i = 0; i < path.poses.size(); ++i) {\n            double dist = euclideanDistance(pose.pose.position, path.poses[i].pose.position);\n            if (dist < min_dist) {\n                min_dist = dist;\n                closest_idx = i;\n            }\n        }\n\n        return closest_idx;\n    }\n\n    geometry_msgs::msg::Twist calculatePathFollowingVelocity(\n        const geometry_msgs::msg::PoseStamped & pose,\n        const nav_msgs::msg::Path & path,\n        size_t closest_idx)\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        if (closest_idx >= path.poses.size() - 1) {\n            // At end of path, slow down\n            cmd_vel.linear.x = 0.0;\n            cmd_vel.angular.z = 0.0;\n            return cmd_vel;\n        }\n\n        // Calculate direction to follow path\n        auto target_pose = path.poses[std::min(closest_idx + 1, path.poses.size() - 1)];\n\n        // Calculate error to path\n        double dx = target_pose.pose.position.x - pose.pose.position.x;\n        double dy = target_pose.pose.position.y - pose.pose.position.y;\n\n        // Convert to robot frame\n        double yaw = tf2::getYaw(pose.pose.orientation);\n        double local_dx = dx * cos(yaw) + dy * sin(yaw);\n        double local_dy = -dx * sin(yaw) + dy * cos(yaw);\n\n        // PID-like control for path following\n        cmd_vel.linear.x = std::min(max_linear_speed_,\n                                   std::max(-max_linear_speed_, local_dx * linear_gain_));\n        cmd_vel.linear.y = std::min(max_linear_speed_,\n                                   std::max(-max_linear_speed_, local_dy * linear_gain_));\n        cmd_vel.angular.z = std::min(max_angular_speed_,\n                                    std::max(-max_angular_speed_, -local_dy * angular_gain_));\n\n        return cmd_vel;\n    }\n\n    geometry_msgs::msg::Twist applyHumanoidConstraints(\n        const geometry_msgs::msg::Twist & raw_cmd,\n        const geometry_msgs::msg::PoseStamped & pose)\n    {\n        geometry_msgs::msg::Twist constrained_cmd = raw_cmd;\n\n        // Apply humanoid-specific velocity limits\n        constrained_cmd.linear.x = std::clamp(constrained_cmd.linear.x,\n                                            -max_step_length_ * step_frequency_,\n                                            max_step_length_ * step_frequency_);\n        constrained_cmd.linear.y = std::clamp(constrained_cmd.linear.y,\n                                            -max_step_width_ * step_frequency_,\n                                            max_step_width_ * step_frequency_);\n        constrained_cmd.angular.z = std::clamp(constrained_cmd.angular.z,\n                                             -max_angular_velocity_,\n                                             max_angular_velocity_);\n\n        return constrained_cmd;\n    }\n\n    bool isPathObstructed(\n        const geometry_msgs::msg::PoseStamped & pose,\n        const geometry_msgs::msg::Twist & cmd_vel)\n    {\n        // Check costmap for obstacles in the robot's path\n        unsigned int mx, my;\n        if (!costmap_->worldToMap(pose.pose.position.x, pose.pose.position.y, mx, my)) {\n            return true;  // Robot position not in costmap\n        }\n\n        // Check ahead in the direction of movement\n        double ahead_x = pose.pose.position.x + cmd_vel.linear.x * 0.5;  // 0.5 seconds ahead\n        double ahead_y = pose.pose.position.y + cmd_vel.linear.y * 0.5;\n\n        unsigned int ahead_mx, ahead_my;\n        if (costmap_->worldToMap(ahead_x, ahead_y, ahead_mx, ahead_my)) {\n            unsigned char cost = costmap_->getCost(ahead_mx, ahead_my);\n            return cost > nav2_costmap_2d::INSCRIBED_INFLATED_OBSTACLE;\n        }\n\n        return false;\n    }\n\n    geometry_msgs::msg::Twist handleObstacles(\n        const geometry_msgs::msg::Twist & cmd_vel,\n        const geometry_msgs::msg::PoseStamped & pose)\n    {\n        // Implement humanoid-specific obstacle avoidance\n        geometry_msgs::msg::Twist avoidance_cmd = cmd_vel;\n\n        // For humanoid robots, we might need to step around obstacles\n        // rather than just turn\n        avoidance_cmd.linear.x = 0.0;  // Stop forward motion\n        avoidance_cmd.angular.z = 0.0;  // Stop turning\n\n        // Implement step-based obstacle avoidance\n        // This could involve planning discrete steps around obstacles\n        // which is more complex than simple velocity modification\n\n        return avoidance_cmd;\n    }\n\n    double euclideanDistance(const geometry_msgs::msg::Point & p1,\n                           const geometry_msgs::msg::Point & p2)\n    {\n        return sqrt(pow(p1.x - p2.x, 2) + pow(p1.y - p2.y, 2));\n    }\n\n    // Member variables\n    rclcpp_lifecycle::LifecycleNode::SharedPtr node_;\n    std::string name_;\n    std::shared_ptr<tf2_ros::Buffer> tf_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> costmap_ros_;\n    nav2_costmap_2d::Costmap2D * costmap_;\n    nav_msgs::msg::Path current_path_;\n\n    // Humanoid-specific parameters\n    double step_frequency_;\n    double max_step_length_;\n    double max_step_width_;\n    double balance_margin_;\n    double max_linear_speed_ = 0.5;\n    double max_angular_speed_ = 0.5;\n    double max_angular_velocity_ = 0.5;\n    double linear_gain_ = 1.0;\n    double angular_gain_ = 2.0;\n};\n```\n\n## Integration with Isaac Sim\n\n### Isaac Sim Navigation Setup\n\n```python\n# Isaac Sim navigation integration\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nimport carb\n\nclass IsaacSimNavigation:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_scene()\n\n    def setup_scene(self):\n        # Add humanoid robot\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Add a humanoid robot to the scene\n        humanoid_asset_path = assets_root_path + \"/Isaac/Robots/Humanoid/humanoid.usd\"\n        add_reference_to_stage(usd_path=humanoid_asset_path, prim_path=\"/World/Humanoid\")\n\n        # Add a simple environment\n        room_asset_path = assets_root_path + \"/Isaac/Environments/Simple_Room/simple_room.usd\"\n        add_reference_to_stage(usd_path=room_asset_path, prim_path=\"/World/Room\")\n\n        # Initialize the world\n        self.world.reset()\n\n    def setup_navigation(self):\n        # Configure navigation-specific components\n        # This would include setting up sensors, costmaps, etc.\n        pass\n\n    def run_navigation(self, goal_position):\n        # Main navigation loop\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get robot position\n            robot_position = self.get_robot_position()\n\n            # Check if reached goal\n            if self.is_at_goal(robot_position, goal_position):\n                print(\"Reached goal!\")\n                break\n\n            # Continue navigation\n            self.navigate_to_goal(goal_position)\n\n    def get_robot_position(self):\n        # Get current robot position from Isaac Sim\n        pass\n\n    def navigate_to_goal(self, goal_position):\n        # Send navigation commands to robot\n        pass\n\n    def is_at_goal(self, current_pos, goal_pos):\n        # Check if robot is at goal position\n        distance = ((current_pos[0] - goal_pos[0])**2 +\n                   (current_pos[1] - goal_pos[1])**2)**0.5\n        return distance < 0.2  # 20cm tolerance\n```\n\n## Performance Evaluation\n\n### Navigation Metrics for Humanoid Robots\n\n```cpp\n// Navigation performance evaluation\nclass NavigationEvaluator\n{\npublic:\n    struct NavigationMetrics\n    {\n        double success_rate;\n        double average_time_to_goal;\n        double path_efficiency;  // actual_path_length / optimal_path_length\n        double collision_count;\n        double balance_loss_count;\n        double step_success_rate;\n    };\n\n    NavigationMetrics evaluateNavigation(\n        const std::vector<geometry_msgs::msg::Pose>& trajectory,\n        const geometry_msgs::msg::Pose& goal,\n        bool navigation_successful)\n    {\n        NavigationMetrics metrics;\n\n        // Calculate success rate\n        metrics.success_rate = navigation_successful ? 1.0 : 0.0;\n\n        // Calculate path efficiency\n        double actual_path_length = calculatePathLength(trajectory);\n        double optimal_path_length = calculateEuclideanDistance(trajectory.front().position, goal.position);\n        metrics.path_efficiency = optimal_path_length > 0 ? actual_path_length / optimal_path_length : 1.0;\n\n        // Count collisions (high cost areas in trajectory)\n        metrics.collision_count = countCollisions(trajectory);\n\n        // Count balance losses (if available)\n        metrics.balance_loss_count = countBalanceLosses(trajectory);\n\n        return metrics;\n    }\n\nprivate:\n    double calculatePathLength(const std::vector<geometry_msgs::msg::Pose>& trajectory)\n    {\n        double length = 0.0;\n        for (size_t i = 1; i < trajectory.size(); ++i) {\n            length += calculateEuclideanDistance(\n                trajectory[i-1].position,\n                trajectory[i].position\n            );\n        }\n        return length;\n    }\n\n    double calculateEuclideanDistance(\n        const geometry_msgs::msg::Point& p1,\n        const geometry_msgs::msg::Point& p2)\n    {\n        return sqrt(pow(p1.x - p2.x, 2) + pow(p1.y - p2.y, 2));\n    }\n\n    int countCollisions(const std::vector<geometry_msgs::msg::Pose>& trajectory)\n    {\n        // Count poses in high-cost areas of costmap\n        int collision_count = 0;\n        // Implementation would check each pose against costmap\n        return collision_count;\n    }\n\n    int countBalanceLosses(const std::vector<geometry_msgs::msg::Pose>& trajectory)\n    {\n        // Count instances where robot lost balance\n        // This would require access to balance metrics\n        return 0;\n    }\n};\n```\n\n## Troubleshooting Navigation Issues\n\n### Common Problems and Solutions\n\n#### 1. Robot Gets Stuck\n**Problem**: Robot stops moving or oscillates in place\n**Solutions**:\n- Check costmap inflation settings\n- Verify sensor data is being received\n- Adjust local planner parameters\n- Implement proper recovery behaviors\n\n#### 2. Poor Path Quality\n**Problem**: Robot takes inefficient or unsafe paths\n**Solutions**:\n- Tune global planner parameters\n- Adjust costmap resolution\n- Verify map quality and accuracy\n- Check for proper obstacle detection\n\n#### 3. Balance Issues During Navigation\n**Problem**: Humanoid robot loses balance while following path\n**Solutions**:\n- Implement step-by-step planning\n- Add balance controller\n- Reduce navigation speed\n- Improve path smoothing\n\n## Best Practices\n\n### 1. Parameter Tuning\n- Start with conservative parameters\n- Test in simulation before real robot\n- Use systematic parameter tuning methods\n- Document parameter sets for different environments\n\n### 2. Safety Considerations\n- Implement emergency stops\n- Monitor robot state continuously\n- Set appropriate velocity limits\n- Use proper collision checking\n\n### 3. Performance Optimization\n- Use appropriate costmap resolution\n- Optimize sensor update rates\n- Implement efficient path planning\n- Monitor computational resources\n\n## Exercise\n\nCreate a complete navigation system for a humanoid robot that includes:\n\n1. Custom Nav2 configuration optimized for humanoid locomotion\n2. A step planner that generates discrete foot placements\n3. A balance controller to maintain stability during navigation\n4. Integration with Isaac Sim for testing\n5. Performance evaluation tools to measure navigation success\n\nTest your system in various scenarios including:\n- Indoor navigation with furniture\n- Narrow passages\n- Obstacle avoidance\n- Stair navigation (if applicable)\n- Dynamic obstacle avoidance\n\nEvaluate the system's performance using the metrics discussed in this section.",
    "path": "module-3-ai-perception\\nav2-locomotion.md",
    "description": ""
  },
  "module-3-ai-perception\\navigation-planning-obstacle-avoidance": {
    "title": "module-3-ai-perception\\navigation-planning-obstacle-avoidance",
    "content": "# Navigation Planning and Obstacle Avoidance Examples\n\nThis section covers navigation planning and obstacle avoidance techniques for humanoid robots using AI-powered perception and navigation systems. We'll explore how to integrate perception data with navigation planning for safe and efficient robot movement.\n\n## Introduction to Navigation Planning\n\nNavigation planning involves determining a safe and efficient path for a robot to reach its goal while avoiding obstacles. For humanoid robots, this includes additional complexities like balance maintenance, step planning, and dynamic stability.\n\n### Key Components of Navigation Planning\n- **Global Path Planning**: Long-term path from start to goal\n- **Local Path Planning**: Short-term path adjustment based on obstacles\n- **Trajectory Generation**: Smooth motion trajectories\n- **Obstacle Avoidance**: Real-time obstacle detection and avoidance\n- **Recovery Behaviors**: Handling navigation failures\n\n## Global Path Planning\n\n### A* Algorithm Implementation\n\n```cpp\n#include <vector>\n#include <queue>\n#include <cmath>\n#include <algorithm>\n\nstruct Point {\n    int x, y;\n    double g_cost = 0;  // Cost from start\n    double h_cost = 0;  // Heuristic cost to goal\n    double f_cost = 0;  // g + h\n    Point* parent = nullptr;\n\n    bool operator>(const Point& other) const {\n        return f_cost > other.f_cost;\n    }\n};\n\nclass GlobalPlanner {\npublic:\n    GlobalPlanner(const std::vector<std::vector<int>>& grid) : grid_(grid) {}\n\n    std::vector<Point> planPath(const Point& start, const Point& goal) {\n        // Initialize open and closed sets\n        std::priority_queue<Point, std::vector<Point>, std::greater<Point>> open_set;\n        std::vector<std::vector<bool>> closed_set(grid_.size(),\n                                                 std::vector<bool>(grid_[0].size(), false));\n\n        // Add start point to open set\n        Point start_copy = start;\n        start_copy.h_cost = heuristic(start_copy, goal);\n        start_copy.f_cost = start_copy.h_cost;\n        open_set.push(start_copy);\n\n        while (!open_set.empty()) {\n            Point current = open_set.top();\n            open_set.pop();\n\n            // Check if we reached the goal\n            if (current.x == goal.x && current.y == goal.y) {\n                return reconstructPath(current);\n            }\n\n            // Mark as visited\n            closed_set[current.x][current.y] = true;\n\n            // Check neighbors\n            std::vector<Point> neighbors = getNeighbors(current);\n            for (auto& neighbor : neighbors) {\n                if (neighbor.x < 0 || neighbor.x >= grid_.size() ||\n                    neighbor.y < 0 || neighbor.y >= grid_[0].size() ||\n                    grid_[neighbor.x][neighbor.y] == 1 ||  // Obstacle\n                    closed_set[neighbor.x][neighbor.y]) {\n                    continue;\n                }\n\n                double tentative_g = current.g_cost + distance(current, neighbor);\n\n                if (tentative_g < neighbor.g_cost || neighbor.parent == nullptr) {\n                    neighbor.parent = new Point(current);\n                    neighbor.g_cost = tentative_g;\n                    neighbor.h_cost = heuristic(neighbor, goal);\n                    neighbor.f_cost = neighbor.g_cost + neighbor.h_cost;\n\n                    open_set.push(neighbor);\n                }\n            }\n        }\n\n        // No path found\n        return {};\n    }\n\nprivate:\n    std::vector<std::vector<int>> grid_;\n\n    double heuristic(const Point& a, const Point& b) {\n        // Manhattan distance heuristic\n        return std::abs(a.x - b.x) + std::abs(a.y - b.y);\n    }\n\n    double distance(const Point& a, const Point& b) {\n        // Euclidean distance\n        return std::sqrt(std::pow(a.x - b.x, 2) + std::pow(a.y - b.y, 2));\n    }\n\n    std::vector<Point> getNeighbors(const Point& point) {\n        std::vector<Point> neighbors;\n        // 8-directional movement\n        int dx[] = {-1, -1, -1, 0, 0, 1, 1, 1};\n        int dy[] = {-1, 0, 1, -1, 1, -1, 0, 1};\n\n        for (int i = 0; i < 8; i++) {\n            Point neighbor;\n            neighbor.x = point.x + dx[i];\n            neighbor.y = point.y + dy[i];\n            neighbors.push_back(neighbor);\n        }\n\n        return neighbors;\n    }\n\n    std::vector<Point> reconstructPath(Point current) {\n        std::vector<Point> path;\n        while (current.parent != nullptr) {\n            path.push_back(current);\n            current = *(current.parent);\n        }\n        path.push_back(current);  // Add start point\n        std::reverse(path.begin(), path.end());\n        return path;\n    }\n};\n```\n\n### Nav2 Global Planner Integration\n\n```cpp\n#include <nav2_core/global_planner.hpp>\n#include <nav2_costmap_2d/costmap_2d_ros.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <pluginlib/class_list_macros.hpp>\n\nclass HumanoidGlobalPlanner : public nav2_core::GlobalPlanner\n{\npublic:\n    HumanoidGlobalPlanner() = default;\n    ~HumanoidGlobalPlanner() override = default;\n\n    void configure(\n        const rclcpp_lifecycle::LifecycleNode::WeakPtr & parent,\n        std::string name,\n        const std::shared_ptr<tf2_ros::Buffer> & tf,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & costmap_ros) override\n    {\n        node_ = parent.lock();\n        name_ = name;\n        tf_ = tf;\n        costmap_ros_ = costmap_ros;\n        costmap_ = costmap_ros_->getCostmap();\n\n        RCLCPP_INFO(node_->get_logger(), \"Configured HumanoidGlobalPlanner\");\n\n        // Initialize humanoid-specific parameters\n        step_height_threshold_ = node_->declare_parameter(name_ + \".step_height_threshold\", 0.2);\n        max_slope_angle_ = node_->declare_parameter(name_ + \".max_slope_angle\", 15.0);\n    }\n\n    void cleanup() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"Cleaning up HumanoidGlobalPlanner\");\n    }\n\n    void activate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"Activating HumanoidGlobalPlanner\");\n    }\n\n    void deactivate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"Deactivating HumanoidGlobalPlanner\");\n    }\n\n    nav_msgs::msg::Path createPlan(\n        const geometry_msgs::msg::PoseStamped & start,\n        const geometry_msgs::msg::PoseStamped & goal) override\n    {\n        nav_msgs::msg::Path path;\n\n        // Check if start and goal are valid\n        if (!isStartValid(start) || !isGoalValid(goal)) {\n            RCLCPP_WARN(node_->get_logger(), \"Invalid start or goal position\");\n            return path;\n        }\n\n        // Convert poses to grid coordinates\n        unsigned int start_x, start_y, goal_x, goal_y;\n        if (!costmap_->worldToMap(start.pose.position.x, start.pose.position.y, start_x, start_y) ||\n            !costmap_->worldToMap(goal.pose.position.x, goal.pose.position.y, goal_x, goal_y)) {\n            RCLCPP_WARN(node_->get_logger(), \"Start or goal position not in costmap\");\n            return path;\n        }\n\n        // Plan path using A* with humanoid constraints\n        auto path_points = planHumanoidPath(start_x, start_y, goal_x, goal_y);\n\n        // Convert to ROS message\n        path = convertToPathMsg(path_points, start.header);\n\n        // Apply path smoothing for humanoid locomotion\n        smoothPath(path);\n\n        return path;\n    }\n\nprivate:\n    bool isStartValid(const geometry_msgs::msg::PoseStamped & start)\n    {\n        unsigned int mx, my;\n        if (!costmap_->worldToMap(start.pose.position.x, start.pose.position.y, mx, my)) {\n            return false;\n        }\n\n        // Check if start is in free space\n        return costmap_->getCost(mx, my) < nav2_costmap_2d::FREE_SPACE;\n    }\n\n    bool isGoalValid(const geometry_msgs::msg::PoseStamped & goal)\n    {\n        unsigned int mx, my;\n        if (!costmap_->worldToMap(goal.pose.position.x, goal.pose.position.y, mx, my)) {\n            return false;\n        }\n\n        // Check if goal is in free space and not too close to obstacles\n        unsigned char cost = costmap_->getCost(mx, my);\n        return cost < nav2_costmap_2d::INSCRIBED_INFLATED_OBSTACLE;\n    }\n\n    std::vector<Point> planHumanoidPath(unsigned int start_x, unsigned int start_y,\n                                       unsigned int goal_x, unsigned int goal_y)\n    {\n        // Create grid representation of costmap\n        std::vector<std::vector<int>> grid(costmap_->getSizeInCellsY(),\n                                          std::vector<int>(costmap_->getSizeInCellsX()));\n\n        // Populate grid with costmap data\n        for (unsigned int y = 0; y < costmap_->getSizeInCellsY(); ++y) {\n            for (unsigned int x = 0; x < costmap_->getSizeInCellsX(); ++x) {\n                unsigned char cost = costmap_->getCost(x, y);\n                grid[y][x] = (cost >= nav2_costmap_2d::LETHAL_OBSTACLE) ? 1 : 0;\n            }\n        }\n\n        // Create start and goal points\n        Point start_point, goal_point;\n        start_point.x = start_x;\n        start_point.y = start_y;\n        goal_point.x = goal_x;\n        goal_point.y = goal_y;\n\n        // Run A* planning\n        GlobalPlanner planner(grid);\n        return planner.planPath(start_point, goal_point);\n    }\n\n    nav_msgs::msg::Path convertToPathMsg(const std::vector<Point>& points,\n                                        const std_msgs::msg::Header& header)\n    {\n        nav_msgs::msg::Path path;\n        path.header = header;\n\n        for (const auto& point : points) {\n            geometry_msgs::msg::PoseStamped pose;\n            pose.header = header;\n\n            // Convert grid coordinates to world coordinates\n            double x, y;\n            costmap_->mapToWorld(point.x, point.y, x, y);\n            pose.pose.position.x = x;\n            pose.pose.position.y = y;\n            pose.pose.position.z = 0.0;\n\n            // Set orientation to face next point\n            if (&point != &points.back()) {  // Not the last point\n                auto next_it = std::next(&point);\n                if (next_it != &points.back() + 1) {\n                    double dx = points[&point - &points[0] + 1].x - point.x;\n                    double dy = points[&point - &points[0] + 1].y - point.y;\n\n                    double yaw = atan2(dy, dx);\n                    tf2::Quaternion q;\n                    q.setRPY(0, 0, yaw);\n                    pose.pose.orientation.x = q.x();\n                    pose.pose.orientation.y = q.y();\n                    pose.pose.orientation.z = q.z();\n                    pose.pose.orientation.w = q.w();\n                }\n            }\n\n            path.poses.push_back(pose);\n        }\n\n        return path;\n    }\n\n    void smoothPath(nav_msgs::msg::Path& path)\n    {\n        // Apply path smoothing for smoother humanoid locomotion\n        // This could implement techniques like:\n        // - Dubins curves for curvature-constrained paths\n        // - B-spline smoothing\n        // - Gradient descent-based smoothing\n\n        // Simple smoothing by averaging adjacent points\n        if (path.poses.size() < 3) return;\n\n        for (size_t i = 1; i < path.poses.size() - 1; ++i) {\n            auto& curr = path.poses[i].pose.position;\n            auto prev = path.poses[i-1].pose.position;\n            auto next = path.poses[i+1].pose.position;\n\n            // Weighted average: 25% prev, 50% current, 25% next\n            curr.x = 0.25 * prev.x + 0.5 * curr.x + 0.25 * next.x;\n            curr.y = 0.25 * prev.y + 0.5 * curr.y + 0.25 * next.y;\n        }\n    }\n\n    rclcpp_lifecycle::LifecycleNode::SharedPtr node_;\n    std::string name_;\n    std::shared_ptr<tf2_ros::Buffer> tf_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> costmap_ros_;\n    nav2_costmap_2d::Costmap2D* costmap_;\n\n    // Humanoid-specific parameters\n    double step_height_threshold_;\n    double max_slope_angle_;\n};\n\nPLUGINLIB_EXPORT_CLASS(HumanoidGlobalPlanner, nav2_core::GlobalPlanner)\n```\n\n## Local Path Planning and Obstacle Avoidance\n\n### Dynamic Window Approach (DWA) for Humanoid Robots\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass HumanoidLocalPlanner {\npublic:\n    HumanoidLocalPlanner() {\n        // Initialize humanoid-specific parameters\n        max_vel_x_ = 0.5;      // Max forward velocity (m/s)\n        min_vel_x_ = 0.05;     // Min forward velocity (m/s)\n        max_vel_th_ = 0.5;     // Max angular velocity (rad/s)\n        min_vel_th_ = -0.5;    // Min angular velocity (rad/s)\n\n        max_acc_x_ = 0.5;      // Max acceleration (m/s)\n        max_acc_th_ = 1.0;     // Max angular acceleration (rad/s)\n\n        vtheta_samp_ = 20;     // Number of angular velocity samples\n        vx_samp_ = 10;         // Number of forward velocity samples\n\n        sim_time_ = 2.0;       // Simulation time horizon (seconds)\n        sim_granularity_ = 0.05; // Simulation granularity (meters)\n\n        // Humanoid-specific parameters\n        step_frequency_ = 1.25;  // Steps per second\n        balance_margin_ = 0.1;   // Balance safety margin\n    }\n\n    geometry_msgs::msg::Twist calculateVelocityCommands(\n        const geometry_msgs::msg::PoseStamped& robot_pose,\n        const geometry_msgs::msg::PoseStamped& goal_pose,\n        const geometry_msgs::msg::Twist& current_vel,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        // Get possible velocities\n        auto vel_samples = getVelocitySamples(current_vel);\n\n        double best_score = -std::numeric_limits<double>::infinity();\n        geometry_msgs::msg::Twist best_vel;\n\n        for (const auto& vel : vel_samples) {\n            // Simulate trajectory for this velocity\n            auto trajectory = simulateTrajectory(robot_pose.pose, vel, current_vel);\n\n            // Evaluate trajectory\n            double heading_score = calculateHeadingScore(trajectory, goal_pose);\n            double dist_score = calculateDistScore(trajectory);\n            double obs_score = calculateObstacleScore(trajectory, scan_data);\n\n            // Weighted combination of scores\n            double score = 0.3 * heading_score + 0.2 * dist_score + 0.5 * obs_score;\n\n            if (score > best_score) {\n                best_score = score;\n                best_vel = vel;\n            }\n        }\n\n        return best_vel;\n    }\n\nprivate:\n    struct Trajectory {\n        std::vector<geometry_msgs::msg::Pose> poses;\n        geometry_msgs::msg::Twist final_vel;\n    };\n\n    std::vector<geometry_msgs::msg::Twist> getVelocitySamples(\n        const geometry_msgs::msg::Twist& current_vel)\n    {\n        std::vector<geometry_msgs::msg::Twist> samples;\n\n        // Calculate velocity windows based on current velocity and accelerations\n        double dt = 0.1;  // Time step for sampling\n        double max_delta_vx = max_acc_x_ * dt;\n        double max_delta_vth = max_acc_th_ * dt;\n\n        double min_vx = std::max(min_vel_x_, current_vel.linear.x - max_delta_vx);\n        double max_vx = std::min(max_vel_x_, current_vel.linear.x + max_delta_vx);\n        double min_vth = std::max(min_vel_th_, current_vel.angular.z - max_delta_vth);\n        double max_vth = std::min(max_vel_th_, current_vel.angular.z + max_delta_vth);\n\n        // Sample velocities\n        double dvx = (max_vx - min_vx) / vx_samp_;\n        double dvth = (max_vth - min_vth) / vtheta_samp_;\n\n        for (int i = 0; i <= vx_samp_; ++i) {\n            for (int j = 0; j <= vtheta_samp_; ++j) {\n                geometry_msgs::msg::Twist vel;\n                vel.linear.x = min_vx + i * dvx;\n                vel.angular.z = min_vth + j * dvth;\n\n                // Humanoid-specific constraints\n                if (isValidHumanoidVelocity(vel)) {\n                    samples.push_back(vel);\n                }\n            }\n        }\n\n        return samples;\n    }\n\n    bool isValidHumanoidVelocity(const geometry_msgs::msg::Twist& vel)\n    {\n        // Check humanoid-specific constraints\n        // For example, ensure velocity is within safe walking parameters\n        double speed = sqrt(vel.linear.x * vel.linear.x + vel.linear.y * vel.linear.y);\n\n        // Simple check: speed should be within walking range\n        if (speed > max_vel_x_ * 1.5) return false;  // Too fast for stable walking\n\n        return true;\n    }\n\n    Trajectory simulateTrajectory(\n        const geometry_msgs::msg::Pose& start_pose,\n        const geometry_msgs::msg::Twist& target_vel,\n        const geometry_msgs::msg::Twist& current_vel)\n    {\n        Trajectory traj;\n        geometry_msgs::msg::Pose current_pose = start_pose;\n        geometry_msgs::msg::Twist current_vel_local = current_vel;\n\n        double dt = sim_granularity_ / std::max(std::abs(target_vel.linear.x), 0.1);\n        int steps = static_cast<int>(sim_time_ / dt);\n\n        for (int i = 0; i < steps; ++i) {\n            // Update pose based on current velocity\n            double yaw = tf2::getYaw(current_pose.orientation);\n\n            // Update position\n            current_pose.position.x += current_vel_local.linear.x * cos(yaw) * dt;\n            current_pose.position.y += current_vel_local.linear.x * sin(yaw) * dt;\n            current_pose.position.z += current_vel_local.linear.z * dt;  // For 3D movement\n\n            // Update orientation\n            yaw += current_vel_local.angular.z * dt;\n            tf2::Quaternion quat;\n            quat.setRPY(0, 0, yaw);\n            current_pose.orientation = tf2::toMsg(quat);\n\n            // Update velocity towards target (with acceleration constraints)\n            double ax = std::min(max_acc_x_,\n                                std::abs(target_vel.linear.x - current_vel_local.linear.x) / dt);\n            double ath = std::min(max_acc_th_,\n                                 std::abs(target_vel.angular.z - current_vel_local.angular.z) / dt);\n\n            if (target_vel.linear.x > current_vel_local.linear.x) {\n                current_vel_local.linear.x = std::min(target_vel.linear.x,\n                                                    current_vel_local.linear.x + ax * dt);\n            } else {\n                current_vel_local.linear.x = std::max(target_vel.linear.x,\n                                                    current_vel_local.linear.x - ax * dt);\n            }\n\n            if (target_vel.angular.z > current_vel_local.angular.z) {\n                current_vel_local.angular.z = std::min(target_vel.angular.z,\n                                                     current_vel_local.angular.z + ath * dt);\n            } else {\n                current_vel_local.angular.z = std::max(target_vel.angular.z,\n                                                     current_vel_local.angular.z - ath * dt);\n            }\n\n            traj.poses.push_back(current_pose);\n        }\n\n        traj.final_vel = current_vel_local;\n        return traj;\n    }\n\n    double calculateHeadingScore(\n        const Trajectory& traj,\n        const geometry_msgs::msg::PoseStamped& goal_pose)\n    {\n        if (traj.poses.empty()) return 0.0;\n\n        const auto& final_pose = traj.poses.back();\n\n        // Calculate angle to goal\n        double goal_x = goal_pose.pose.position.x;\n        double goal_y = goal_pose.pose.position.y;\n        double robot_x = final_pose.position.x;\n        double robot_y = final_pose.position.y;\n\n        double angle_to_goal = atan2(goal_y - robot_y, goal_x - robot_x);\n        double robot_yaw = tf2::getYaw(final_pose.orientation);\n\n        // Normalize angles\n        double angle_diff = angle_to_goal - robot_yaw;\n        angle_diff = std::atan2(std::sin(angle_diff), std::cos(angle_diff));\n\n        // Score based on how well the robot is oriented toward the goal\n        return 1.0 - std::abs(angle_diff) / M_PI;  // Higher score for smaller angle difference\n    }\n\n    double calculateDistScore(const Trajectory& traj)\n    {\n        // Score based on how far the trajectory takes the robot\n        if (traj.poses.size() < 2) return 0.0;\n\n        const auto& start = traj.poses.front();\n        const auto& end = traj.poses.back();\n\n        double dist = sqrt(pow(end.position.x - start.position.x, 2) +\n                          pow(end.position.y - start.position.y, 2));\n\n        // Normalize by simulation time\n        return dist / sim_time_;\n    }\n\n    double calculateObstacleScore(\n        const Trajectory& traj,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        double score = 0.0;\n        double min_dist_to_obstacle = std::numeric_limits<double>::infinity();\n\n        for (const auto& pose : traj.poses) {\n            // Check distance to nearest obstacle at this pose\n            double dist = getMinDistanceToObstacle(pose, scan_data);\n            min_dist_to_obstacle = std::min(min_dist_to_obstacle, dist);\n\n            if (dist < 0.2) {  // Very close to obstacle\n                return -std::numeric_limits<double>::infinity();  // Invalid trajectory\n            }\n        }\n\n        // Score based on minimum distance to obstacles\n        // Prefer trajectories that stay farther from obstacles\n        return min_dist_to_obstacle;\n    }\n\n    double getMinDistanceToObstacle(\n        const geometry_msgs::msg::Pose& pose,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        // Convert laser scan points to robot coordinates and check for obstacles\n        double min_dist = std::numeric_limits<double>::infinity();\n        double robot_yaw = tf2::getYaw(pose.orientation);\n\n        for (size_t i = 0; i < scan_data.ranges.size(); ++i) {\n            if (scan_data.ranges[i] < scan_data.range_min ||\n                scan_data.ranges[i] > scan_data.range_max) {\n                continue;  // Invalid range\n            }\n\n            // Convert polar coordinates to Cartesian in laser frame\n            double angle = scan_data.angle_min + i * scan_data.angle_increment;\n            double x_laser = scan_data.ranges[i] * cos(angle);\n            double y_laser = scan_data.ranges[i] * sin(angle);\n\n            // Transform to robot base frame\n            double cos_yaw = cos(robot_yaw);\n            double sin_yaw = sin(robot_yaw);\n\n            double x_robot = x_laser * cos_yaw - y_laser * sin_yaw + pose.position.x;\n            double y_robot = x_laser * sin_yaw + y_laser * cos_yaw + pose.position.y;\n\n            // Calculate distance from robot center to this point\n            double dist = sqrt(pow(x_robot - pose.position.x, 2) +\n                              pow(y_robot - pose.position.y, 2));\n\n            min_dist = std::min(min_dist, dist);\n        }\n\n        return min_dist;\n    }\n\n    // Parameters\n    double max_vel_x_, min_vel_x_, max_vel_th_, min_vel_th_;\n    double max_acc_x_, max_acc_th_;\n    int vtheta_samp_, vx_samp_;\n    double sim_time_, sim_granularity_;\n    double step_frequency_, balance_margin_;\n};\n```\n\n## Obstacle Detection and Avoidance Integration\n\n### Perception-Based Obstacle Detection\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <pcl/point_cloud.h>\n#include <pcl/point_types.h>\n#include <pcl_conversions/pcl_conversions.h>\n\nclass PerceptionObstacleDetector : public rclcpp::Node\n{\npublic:\n    PerceptionObstacleDetector() : Node(\"perception_obstacle_detector\")\n    {\n        // Subscribe to various sensor inputs\n        laser_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\n            \"scan\", 10,\n            std::bind(&PerceptionObstacleDetector::laserCallback, this, std::placeholders::_1)\n        );\n\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            \"detections\", 10,\n            std::bind(&PerceptionObstacleDetector::detectionCallback, this, std::placeholders::_1)\n        );\n\n        pointcloud_sub_ = this->create_subscription<sensor_msgs::msg::PointCloud2>(\n            \"points\", 10,\n            std::bind(&PerceptionObstacleDetector::pointcloudCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for combined obstacle information\n        obstacle_pub_ = this->create_publisher<geometry_msgs::msg::PolygonStamped>(\n            \"obstacle_polygon\", 10\n        );\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::msg::LaserScan::SharedPtr scan_msg)\n    {\n        // Process LIDAR data to detect obstacles\n        std::vector<geometry_msgs::msg::Point32> laser_obstacles = processLaserScan(*scan_msg);\n\n        // Update obstacle map\n        updateObstacleMap(laser_obstacles, \"laser\");\n    }\n\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr detection_msg)\n    {\n        // Process 2D detections and convert to 3D obstacle positions\n        std::vector<geometry_msgs::msg::Point32> vision_obstacles = processDetections(*detection_msg);\n\n        // Update obstacle map\n        updateObstacleMap(vision_obstacles, \"vision\");\n    }\n\n    void pointcloudCallback(const sensor_msgs::msg::PointCloud2::SharedPtr cloud_msg)\n    {\n        // Process point cloud to detect obstacles\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>);\n        pcl::fromROSMsg(*cloud_msg, *cloud);\n\n        std::vector<geometry_msgs::msg::Point32> pointcloud_obstacles = processPointCloud(cloud);\n\n        // Update obstacle map\n        updateObstacleMap(pointcloud_obstacles, \"pointcloud\");\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processLaserScan(const sensor_msgs::msg::LaserScan& scan)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        for (size_t i = 0; i < scan.ranges.size(); ++i) {\n            if (scan.ranges[i] < scan.range_min || scan.ranges[i] > scan.range_max) {\n                continue;  // Invalid range\n            }\n\n            if (scan.ranges[i] < obstacle_distance_threshold_) {\n                // Convert polar to Cartesian coordinates\n                double angle = scan.angle_min + i * scan.angle_increment;\n                geometry_msgs::msg::Point32 point;\n                point.x = scan.ranges[i] * cos(angle);\n                point.y = scan.ranges[i] * sin(angle);\n                point.z = 0.0;  // Assume ground level\n\n                obstacles.push_back(point);\n            }\n        }\n\n        return obstacles;\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processDetections(\n        const isaac_ros_detectnet_interfaces::msg::DetectionArray& detections)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        for (const auto& detection : detections.detections) {\n            if (detection.confidence > detection_confidence_threshold_) {\n                // Convert 2D bounding box to 3D position estimate\n                // This requires depth information from stereo or depth sensor\n                geometry_msgs::msg::Point32 obstacle_pos;\n\n                // For now, assume a fixed depth based on object type and size\n                double estimated_depth = estimateDepthFromSize(detection.bbox, detection.label);\n\n                // Convert image coordinates to robot coordinates\n                // This requires camera calibration parameters\n                obstacle_pos.x = (detection.bbox.center.x - camera_cx_) * estimated_depth / camera_fx_;\n                obstacle_pos.y = (detection.bbox.center.y - camera_cy_) * estimated_depth / camera_fy_;\n                obstacle_pos.z = estimated_depth;\n\n                obstacles.push_back(obstacle_pos);\n            }\n        }\n\n        return obstacles;\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processPointCloud(const pcl::PointCloud<pcl::PointXYZ>::Ptr cloud)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        // Use PCL to segment obstacles from ground plane\n        pcl::ModelCoefficients::Ptr coefficients(new pcl::ModelCoefficients);\n        pcl::PointIndices::Ptr inliers(new pcl::PointIndices);\n\n        // Create the segmentation object\n        pcl::SACSegmentation<pcl::PointXYZ> seg;\n        seg.setOptimizeCoefficients(true);\n        seg.setModelType(pcl::SACMODEL_PLANE);\n        seg.setMethodType(pcl::SAC_RANSAC);\n        seg.setMaxIterations(100);\n        seg.setDistanceThreshold(0.05);  // 5cm tolerance for ground plane\n\n        seg.setInputCloud(cloud);\n        seg.segment(*inliers, *coefficients);\n\n        // Extract obstacles (points not belonging to ground plane)\n        pcl::ExtractIndices<pcl::PointXYZ> extract;\n        extract.setInputCloud(cloud);\n        extract.setIndices(inliers);\n        extract.setNegative(true);  // Extract points NOT on the plane\n        extract.filter(obstacles);\n\n        // Convert PCL points to ROS points\n        std::vector<geometry_msgs::msg::Point32> obstacle_points;\n        for (const auto& point : obstacles.points) {\n            geometry_msgs::msg::Point32 ros_point;\n            ros_point.x = point.x;\n            ros_point.y = point.y;\n            ros_point.z = point.z;\n            obstacle_points.push_back(ros_point);\n        }\n\n        return obstacle_points;\n    }\n\n    void updateObstacleMap(\n        const std::vector<geometry_msgs::msg::Point32>& new_obstacles,\n        const std::string& sensor_type)\n    {\n        // Fuse obstacles from different sensors\n        for (const auto& obstacle : new_obstacles) {\n            // Add to combined obstacle map with sensor type information\n            auto it = std::find_if(combined_obstacles_.begin(), combined_obstacles_.end(),\n                [&obstacle](const FusedObstacle& existing) {\n                    double dist = sqrt(pow(existing.point.x - obstacle.x, 2) +\n                                      pow(existing.point.y - obstacle.y, 2));\n                    return dist < fusion_distance_threshold_;\n                });\n\n            if (it != combined_obstacles_.end()) {\n                // Update existing obstacle with new information\n                it->update(obstacle, sensor_type);\n            } else {\n                // Add new obstacle\n                FusedObstacle new_fused_obstacle(obstacle, sensor_type);\n                combined_obstacles_.push_back(new_fused_obstacle);\n            }\n        }\n\n        // Publish combined obstacle information\n        publishCombinedObstacles();\n    }\n\n    void publishCombinedObstacles()\n    {\n        geometry_msgs::msg::PolygonStamped obstacle_polygon;\n        obstacle_polygon.header.frame_id = \"map\";\n        obstacle_polygon.header.stamp = this->now();\n\n        for (const auto& obstacle : combined_obstacles_) {\n            geometry_msgs::msg::Point32 point;\n            point.x = obstacle.point.x;\n            point.y = obstacle.point.y;\n            point.z = obstacle.point.z;\n            obstacle_polygon.polygon.points.push_back(point);\n        }\n\n        obstacle_pub_->publish(obstacle_polygon);\n    }\n\n    double estimateDepthFromSize(\n        const isaac_ros_detectnet_interfaces::msg::BoundingBox& bbox,\n        const std::string& label)\n    {\n        // Estimate depth based on expected object size\n        // This is a simplified approach - in practice, you'd use stereo or depth sensor\n        double expected_width = getExpectedWidth(label);\n        double pixel_width = bbox.size_x;\n\n        // Using thin lens equation: depth = (focal_length * real_width) / pixel_width\n        return (camera_fx_ * expected_width) / pixel_width;\n    }\n\n    double getExpectedWidth(const std::string& label)\n    {\n        // Return expected width for common object types (in meters)\n        if (label == \"person\") return 0.5;      // Average person width\n        if (label == \"car\") return 1.8;         // Average car width\n        if (label == \"chair\") return 0.6;       // Average chair width\n        if (label == \"table\") return 1.0;       // Average table width\n        return 0.5;  // Default assumption\n    }\n\n    struct FusedObstacle {\n        geometry_msgs::msg::Point32 point;\n        std::map<std::string, int> sensor_votes;  // Count of detections from each sensor\n        double confidence;                        // Overall confidence\n\n        FusedObstacle(const geometry_msgs::msg::Point32& p, const std::string& sensor_type) :\n            point(p), confidence(0.5) {\n            sensor_votes[sensor_type] = 1;\n        }\n\n        void update(const geometry_msgs::msg::Point32& new_point, const std::string& sensor_type) {\n            // Update position with weighted average\n            point.x = (point.x + new_point.x) / 2.0;\n            point.y = (point.y + new_point.y) / 2.0;\n            point.z = (point.z + new_point.z) / 2.0;\n\n            // Update sensor votes\n            sensor_votes[sensor_type]++;\n\n            // Update confidence based on number of confirming sensors\n            confidence = std::min(1.0, static_cast<double>(sensor_votes.size()) / 3.0);\n        }\n    };\n\n    // Subscriptions\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_sub_;\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::PointCloud2>::SharedPtr pointcloud_sub_;\n\n    // Publisher\n    rclcpp::Publisher<geometry_msgs::msg::PolygonStamped>::SharedPtr obstacle_pub_;\n\n    // Obstacle data\n    std::vector<FusedObstacle> combined_obstacles_;\n\n    // Parameters\n    const double obstacle_distance_threshold_ = 2.0;  // Max distance to consider obstacle\n    const double detection_confidence_threshold_ = 0.7;  // Min confidence for detections\n    const double fusion_distance_threshold_ = 0.3;    // Distance to fuse detections\n    const double camera_fx_ = 616.363;  // Camera focal length x\n    const double camera_fy_ = 616.363;  // Camera focal length y\n    const double camera_cx_ = 313.071;  // Camera principal point x\n    const double camera_cy_ = 245.091;  // Camera principal point y\n};\n```\n\n## Humanoid-Specific Navigation Behaviors\n\n### Step Planning for Bipedal Locomotion\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/path.h>\n#include <visualization_msgs/msg/marker_array.h>\n\nclass HumanoidStepPlanner {\npublic:\n    HumanoidStepPlanner() {\n        // Initialize humanoid-specific parameters\n        step_length_ = 0.3;      // Average step length in meters\n        step_width_ = 0.2;       // Side-to-side step distance\n        step_height_ = 0.05;     // Step clearance height\n        max_step_rotation_ = 0.2; // Max rotation per step (rad)\n        step_duration_ = 0.8;    // Time for one step (sec)\n    }\n\n    struct Step {\n        geometry_msgs::msg::Point left_foot;\n        geometry_msgs::msg::Point right_foot;\n        double time;\n        bool is_support_step;  // Which foot is supporting weight\n    };\n\n    std::vector<Step> planSteps(const nav_msgs::msg::Path& path,\n                               const geometry_msgs::msg::Pose& start_pose)\n    {\n        std::vector<Step> steps;\n\n        if (path.poses.empty()) return steps;\n\n        // Start with current stance\n        Step initial_step;\n        initial_step.left_foot = calculateInitialFootPosition(start_pose, true);\n        initial_step.right_foot = calculateInitialFootPosition(start_pose, false);\n        initial_step.time = 0.0;\n        initial_step.is_support_step = true;  // Right foot starts as support\n        steps.push_back(initial_step);\n\n        // Plan steps along the path\n        size_t current_path_idx = 0;\n        geometry_msgs::msg::Pose current_pose = start_pose;\n        double current_time = 0.0;\n\n        while (current_path_idx < path.poses.size()) {\n            // Calculate next step based on path direction\n            auto next_waypoint = getNextWaypoint(path, current_path_idx, current_pose);\n            auto next_step = calculateNextStep(current_pose, next_waypoint, steps.back());\n\n            if (isValidStep(next_step, path)) {\n                next_step.time = current_time + step_duration_;\n                steps.push_back(next_step);\n\n                // Update current pose based on step\n                updatePoseFromStep(current_pose, next_step);\n                current_time += step_duration_;\n\n                // Move to next significant waypoint\n                current_path_idx = findNextSignificantWaypoint(path, current_path_idx);\n            } else {\n                // Handle invalid step (obstacle, unstable, etc.)\n                auto recovery_step = planRecoveryStep(steps.back());\n                if (isValidStep(recovery_step, path)) {\n                    recovery_step.time = current_time + step_duration_;\n                    steps.push_back(recovery_step);\n                    updatePoseFromStep(current_pose, recovery_step);\n                    current_time += step_duration_;\n                } else {\n                    // Cannot proceed, return current plan\n                    break;\n                }\n            }\n        }\n\n        return steps;\n    }\n\nprivate:\n    geometry_msgs::msg::Point calculateInitialFootPosition(\n        const geometry_msgs::msg::Pose& robot_pose, bool is_left_foot)\n    {\n        geometry_msgs::msg::Point foot_pos;\n        double yaw = tf2::getYaw(robot_pose.orientation);\n\n        // Place feet shoulder-width apart initially\n        double offset_x = 0.0;\n        double offset_y = is_left_foot ? step_width_/2.0 : -step_width_/2.0;\n\n        // Transform offset to robot frame\n        foot_pos.x = robot_pose.position.x + offset_x * cos(yaw) - offset_y * sin(yaw);\n        foot_pos.y = robot_pose.position.y + offset_x * sin(yaw) + offset_y * cos(yaw);\n        foot_pos.z = robot_pose.position.z;  // Ground level\n\n        return foot_pos;\n    }\n\n    geometry_msgs::msg::PoseStamped getNextWaypoint(\n        const nav_msgs::msg::Path& path, size_t current_idx,\n        const geometry_msgs::msg::Pose& current_pose)\n    {\n        // Find the next waypoint that's ahead of the robot\n        for (size_t i = current_idx; i < path.poses.size(); ++i) {\n            double dist_sq = pow(path.poses[i].pose.position.x - current_pose.position.x, 2) +\n                            pow(path.poses[i].pose.position.y - current_pose.position.y, 2);\n\n            if (dist_sq > pow(step_length_ * 0.8, 2)) {  // Look ahead 80% of step length\n                return path.poses[i];\n            }\n        }\n\n        // If no significant waypoint found, return the last one\n        if (!path.poses.empty()) {\n            return path.poses.back();\n        }\n\n        // Return current pose if no path\n        geometry_msgs::msg::PoseStamped dummy;\n        dummy.pose = current_pose;\n        return dummy;\n    }\n\n    Step calculateNextStep(const geometry_msgs::msg::Pose& current_pose,\n                          const geometry_msgs::msg::PoseStamped& target_waypoint,\n                          const Step& previous_step)\n    {\n        Step next_step;\n\n        // Calculate direction to target\n        double dx = target_waypoint.pose.position.x - current_pose.position.x;\n        double dy = target_waypoint.pose.position.y - current_pose.position.y;\n        double target_yaw = atan2(dy, dx);\n        double current_yaw = tf2::getYaw(current_pose.orientation);\n\n        // Determine which foot to move (opposite of support foot)\n        bool move_left_foot = previous_step.is_support_step;  // If right was support, move left\n\n        // Calculate new foot position\n        double step_yaw = current_yaw + (move_left_foot ? max_step_rotation_ : -max_step_rotation_);\n\n        geometry_msgs::msg::Point new_foot_pos;\n        if (move_left_foot) {\n            // Move left foot toward target\n            new_foot_pos.x = current_pose.position.x + step_length_ * cos(step_yaw);\n            new_foot_pos.y = current_pose.position.y + step_length_ * sin(step_yaw);\n            new_foot_pos.z = current_pose.position.z;\n\n            // Keep right foot in place\n            next_step.right_foot = previous_step.right_foot;\n            next_step.left_foot = new_foot_pos;\n        } else {\n            // Move right foot toward target\n            new_foot_pos.x = current_pose.position.x + step_length_ * cos(step_yaw);\n            new_foot_pos.y = current_pose.position.y + step_length_ * sin(step_yaw);\n            new_foot_pos.z = current_pose.position.z;\n\n            // Keep left foot in place\n            next_step.left_foot = previous_step.left_foot;\n            next_step.right_foot = new_foot_pos;\n        }\n\n        // Update support foot (alternates with each step)\n        next_step.is_support_step = !previous_step.is_support_step;\n\n        return next_step;\n    }\n\n    bool isValidStep(const Step& step, const nav_msgs::msg::Path& path)\n    {\n        // Check if step is stable (center of mass within support polygon)\n        geometry_msgs::msg::Point com = calculateCOMPosition(step);\n\n        if (!isWithinSupportPolygon(com, step)) {\n            return false;\n        }\n\n        // Check for obstacles at step location\n        if (isStepLocationBlocked(step)) {\n            return false;\n        }\n\n        // Check if step deviates too much from planned path\n        if (isStepOffPath(step, path)) {\n            return false;\n        }\n\n        return true;\n    }\n\n    bool isWithinSupportPolygon(const geometry_msgs::msg::Point& com, const Step& step)\n    {\n        // For bipedal locomotion, support polygon is the convex hull of both feet\n        // This is a simplified check - in practice, you'd calculate the actual convex hull\n\n        // Check if COM is roughly between the feet\n        double min_x = std::min(step.left_foot.x, step.right_foot.x);\n        double max_x = std::max(step.left_foot.x, step.right_foot.x);\n        double min_y = std::min(step.left_foot.y, step.right_foot.y);\n        double max_y = std::max(step.left_foot.y, step.right_foot.y);\n\n        // Add a safety margin\n        double margin = balance_margin_;\n\n        return (com.x >= min_x - margin && com.x <= max_x + margin &&\n                com.y >= min_y - margin && com.y <= max_y + margin);\n    }\n\n    bool isStepLocationBlocked(const Step& step)\n    {\n        // Check if the step location has obstacles\n        // This would interface with the costmap or obstacle detection system\n        // For now, return false as a placeholder\n        return false;\n    }\n\n    bool isStepOffPath(const Step& step, const nav_msgs::msg::Path& path)\n    {\n        // Check if the step deviates too much from the global path\n        // This would require path tracking algorithms\n        // For now, return false as a placeholder\n        return false;\n    }\n\n    geometry_msgs::msg::Point calculateCOMPosition(const Step& step)\n    {\n        // Simplified COM calculation - in reality, this would consider\n        // the full robot kinematics and mass distribution\n        geometry_msgs::msg::Point com;\n        com.x = (step.left_foot.x + step.right_foot.x) / 2.0;\n        com.y = (step.left_foot.y + step.right_foot.y) / 2.0;\n        com.z = com_height_;  // Approximate COM height\n\n        return com;\n    }\n\n    Step planRecoveryStep(const Step& previous_step)\n    {\n        // Plan a recovery step when normal stepping is not possible\n        // This might involve: stepping in place, taking a smaller step, etc.\n\n        Step recovery_step = previous_step;\n\n        // For now, just return the previous step as a placeholder\n        // In practice, this would implement various recovery behaviors\n        return recovery_step;\n    }\n\n    size_t findNextSignificantWaypoint(const nav_msgs::msg::Path& path, size_t current_idx)\n    {\n        // Find the next waypoint that represents a significant change in direction\n        // This prevents excessive step planning for dense paths\n        size_t next_idx = current_idx + 1;\n\n        // Simple approach: skip waypoints that are very close together\n        while (next_idx < path.poses.size()) {\n            double dist_sq = pow(path.poses[next_idx].pose.position.x -\n                                path.poses[current_idx].pose.position.x, 2) +\n                            pow(path.poses[next_idx].pose.position.y -\n                               path.poses[current_idx].pose.position.y, 2);\n\n            if (dist_sq > pow(step_length_ * 0.5, 2)) {  // Minimum distance between processed waypoints\n                return next_idx;\n            }\n            next_idx++;\n        }\n\n        return path.poses.size();  // Return end if no significant waypoint found\n    }\n\n    void updatePoseFromStep(geometry_msgs::msg::Pose& pose, const Step& step)\n    {\n        // Update robot pose based on completed step\n        // This would consider the kinematics of the step\n        pose.position.x = (step.left_foot.x + step.right_foot.x) / 2.0;\n        pose.position.y = (step.left_foot.y + step.right_foot.y) / 2.0;\n\n        // Update orientation based on foot positions\n        double dx = step.right_foot.x - step.left_foot.x;\n        double dy = step.right_foot.y - step.left_foot.y;\n        double yaw = atan2(dy, dx) + M_PI/2;  // Rotate 90 degrees for forward direction\n\n        tf2::Quaternion q;\n        q.setRPY(0, 0, yaw);\n        pose.orientation = tf2::toMsg(q);\n    }\n\n    // Humanoid-specific parameters\n    double step_length_;\n    double step_width_;\n    double step_height_;\n    double max_step_rotation_;\n    double step_duration_;\n    double com_height_ = 0.8;  // Approximate height of center of mass\n    double balance_margin_ = 0.1;  // Safety margin for balance\n};\n```\n\n## Isaac Sim Navigation Integration\n\n### Isaac Sim Navigation Controller\n\n```python\n# Isaac Sim navigation integration\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport carb\n\nclass IsaacSimNavigationController:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_navigation_environment()\n\n    def setup_navigation_environment(self):\n        # Add robot to the scene\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Add a humanoid robot (example with Carter robot)\n        robot_asset_path = assets_root_path + \"/Isaac/Robots/Carter/carter_navigate.usd\"\n        add_reference_to_stage(usd_path=robot_asset_path, prim_path=\"/World/Carter\")\n\n        # Add a LIDAR sensor to the robot\n        self.lidar = LidarRtx(\n            prim_path=\"/World/Carter/chassis/lidar\",\n            translation=np.array([0.0, 0.0, 0.3]),\n            orientation=np.array([0.0, 0.0, 0.0, 1.0]),\n            config=\"Carter\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add a simple environment\n        room_asset_path = assets_root_path + \"/Isaac/Environments/Simple_Room/simple_room.usd\"\n        add_reference_to_stage(usd_path=room_asset_path, prim_path=\"/World/Room\")\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_navigation_simulation(self, goal_position):\n        \"\"\"Run navigation simulation with obstacle avoidance\"\"\"\n        self.world.reset()\n\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get robot state\n            robot_position = self.get_robot_position()\n            robot_orientation = self.get_robot_orientation()\n            lidar_data = self.lidar.get_linear_depth_data()\n\n            # Check if reached goal\n            if self.is_at_goal(robot_position, goal_position):\n                print(f\"Reached goal at {goal_position}!\")\n                break\n\n            # Plan and execute navigation\n            cmd_vel = self.plan_navigation_command(\n                robot_position, robot_orientation,\n                goal_position, lidar_data\n            )\n\n            # Apply command to robot\n            self.execute_command(cmd_vel)\n\n    def get_robot_position(self):\n        \"\"\"Get current robot position from Isaac Sim\"\"\"\n        # In a real implementation, this would get the robot's position\n        # from the simulation\n        pass\n\n    def get_robot_orientation(self):\n        \"\"\"Get current robot orientation from Isaac Sim\"\"\"\n        pass\n\n    def is_at_goal(self, current_pos, goal_pos, tolerance=0.2):\n        \"\"\"Check if robot is at goal position\"\"\"\n        distance = np.linalg.norm(np.array(current_pos[:2]) - np.array(goal_pos[:2]))\n        return distance < tolerance\n\n    def plan_navigation_command(self, current_pos, current_orient, goal_pos, lidar_data):\n        \"\"\"Plan navigation command based on goal and sensor data\"\"\"\n        # Calculate direction to goal\n        dx = goal_pos[0] - current_pos[0]\n        dy = goal_pos[1] - current_pos[1]\n        goal_distance = np.sqrt(dx*dx + dy*dy)\n\n        # Calculate goal angle\n        goal_angle = np.arctan2(dy, dx)\n\n        # Get robot's current angle\n        robot_yaw = self.orientation_to_yaw(current_orient)\n\n        # Calculate angle difference\n        angle_diff = self.normalize_angle(goal_angle - robot_yaw)\n\n        # Simple proportional controller\n        linear_vel = min(0.5, goal_distance * 0.5)  # Scale with distance\n        angular_vel = angle_diff * 1.0  # Proportional control\n\n        # Obstacle avoidance\n        min_distance = np.min(lidar_data) if len(lidar_data) > 0 else float('inf')\n\n        if min_distance < 0.5:  # Obstacle detected\n            # Slow down and turn away from obstacle\n            linear_vel *= 0.3\n            angular_vel += self.avoid_obstacle(lidar_data)\n\n        # Ensure velocities are within limits\n        linear_vel = np.clip(linear_vel, 0.0, 0.5)\n        angular_vel = np.clip(angular_vel, -0.5, 0.5)\n\n        return [linear_vel, 0.0, 0.0], [0.0, 0.0, angular_vel]  # linear, angular velocities\n\n    def orientation_to_yaw(self, orientation):\n        \"\"\"Convert quaternion orientation to yaw angle\"\"\"\n        # Simplified conversion - in practice, use proper quaternion to euler conversion\n        return np.arctan2(2*(orientation[3]*orientation[2] + orientation[0]*orientation[1]),\n                         1 - 2*(orientation[1]**2 + orientation[2]**2))\n\n    def normalize_angle(self, angle):\n        \"\"\"Normalize angle to [-pi, pi] range\"\"\"\n        while angle > np.pi:\n            angle -= 2*np.pi\n        while angle < -np.pi:\n            angle += 2*np.pi\n        return angle\n\n    def avoid_obstacle(self, lidar_data):\n        \"\"\"Calculate avoidance angular velocity based on LIDAR data\"\"\"\n        if len(lidar_data) == 0:\n            return 0.0\n\n        # Find the direction of the closest obstacle\n        min_idx = np.argmin(lidar_data)\n        angle_resolution = 2 * np.pi / len(lidar_data)\n        obstacle_angle = min_idx * angle_resolution - np.pi  # Convert to [-pi, pi]\n\n        # Turn away from the obstacle\n        # If obstacle is on the right, turn left (negative angular velocity)\n        # If obstacle is on the left, turn right (positive angular velocity)\n        if abs(obstacle_angle) < np.pi/2:  # Obstacle is in front\n            return -np.sign(obstacle_angle) * 0.3  # Turn away from obstacle\n        else:\n            return 0.0  # Obstacle is behind, no need to turn\n\n    def execute_command(self, cmd_vel):\n        \"\"\"Execute the navigation command in Isaac Sim\"\"\"\n        # In a real implementation, this would send the command to the robot\n        # controller in Isaac Sim\n        linear_vel, angular_vel = cmd_vel\n        # Apply these velocities to the robot's differential drive controller\n        pass\n```\n\n## Recovery Behaviors\n\n### Navigation Recovery Behaviors\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav2_core/recovery.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <pluginlib/class_list_macros.hpp>\n\nclass HumanoidSpinRecovery : public nav2_core::Recovery\n{\npublic:\n    HumanoidSpinRecovery() = default;\n    ~HumanoidSpinRecovery() override = default;\n\n    void configure(\n        const rclcpp_lifecycle::LifecycleNode::WeakPtr & parent,\n        const std::string & name,\n        const std::shared_ptr<tf2_ros::Buffer> & tf,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & global_costmap,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & local_costmap) override\n    {\n        node_ = parent.lock();\n        name_ = name;\n        tf_ = tf;\n        global_costmap_ = global_costmap;\n        local_costmap_ = local_costmap;\n\n        // Declare parameters specific to humanoid recovery\n        spin_angular_vel_ = node_->declare_parameter(name_ + \".spin_angular_vel\", 0.5);\n        min_spin_duration_ = node_->declare_parameter(name_ + \".min_spin_duration\", 1.0);\n        max_spin_duration_ = node_->declare_parameter(name_ + \".max_spin_duration\", 10.0);\n\n        vel_pub_ = node_->create_publisher<geometry_msgs::msg::Twist>(\"cmd_vel\", 1);\n    }\n\n    void cleanup() override\n    {\n        vel_pub_->on_deactivate();\n    }\n\n    void activate() override\n    {\n        vel_pub_->on_activate();\n    }\n\n    void deactivate() override\n    {\n        vel_pub_->on_deactivate();\n    }\n\n    nav2_core::RecoveryResult run(\n        const std::shared_ptr<const nav2_msgs::action::Recovery::Goal> command) override\n    {\n        RCLCPP_INFO(node_->get_logger(), \"Starting humanoid spin recovery behavior\");\n\n        // For humanoid robots, spinning in place may not be feasible\n        // Instead, implement a gentle turning motion with steps\n        return executeHumanoidSpin();\n    }\n\nprivate:\n    nav2_core::RecoveryResult executeHumanoidSpin()\n    {\n        nav2_core::RecoveryResult result;\n        result.outcome = nav2_core::RecoveryResult::SUCCESS;\n\n        auto start_time = node_->now();\n        auto current_time = start_time;\n\n        while (rclcpp::ok()) {\n            current_time = node_->now();\n\n            // Check if we've spun enough\n            if ((current_time - start_time).seconds() > min_spin_duration_) {\n                // Check if we've cleared the obstacle\n                if (isObstacleClear()) {\n                    RCLCPP_INFO(node_->get_logger(), \"Obstacle cleared, stopping spin recovery\");\n                    break;\n                }\n\n                // Check if we've spun too long\n                if ((current_time - start_time).seconds() > max_spin_duration_) {\n                    RCLCPP_WARN(node_->get_logger(), \"Spin recovery timed out\");\n                    result.outcome = nav2_core::RecoveryResult::FAILURE;\n                    break;\n                }\n            }\n\n            // Generate spin command for humanoid\n            auto spin_cmd = generateHumanoidSpinCommand();\n            vel_pub_->publish(spin_cmd);\n\n            // Sleep briefly to allow other processes\n            std::this_thread::sleep_for(std::chrono::milliseconds(100));\n        }\n\n        // Stop the robot\n        geometry_msgs::msg::Twist stop_cmd;\n        vel_pub_->publish(stop_cmd);\n\n        return result;\n    }\n\n    geometry_msgs::msg::Twist generateHumanoidSpinCommand()\n    {\n        geometry_msgs::msg::Twist cmd;\n\n        // For humanoid, instead of pure rotation, we might want to step-turn\n        // This is a simplified approach - real implementation would plan actual steps\n        cmd.angular.z = spin_angular_vel_;\n\n        // Small forward motion to maintain momentum\n        cmd.linear.x = 0.05;\n\n        return cmd;\n    }\n\n    bool isObstacleClear()\n    {\n        // Check if obstacles are clear in the local costmap\n        auto costmap = local_costmap_->getCostmap();\n        unsigned int mx, my;\n\n        // Check multiple directions around the robot\n        double robot_x = costmap->getOriginX() + costmap->getSizeInMetersX() / 2.0;\n        double robot_y = costmap->getOriginY() + costmap->getSizeInMetersY() / 2.0;\n\n        for (double angle = 0; angle < 2*M_PI; angle += M_PI/4) {\n            double check_x = robot_x + 0.5 * cos(angle);  // Check 0.5m out\n            double check_y = robot_y + 0.5 * sin(angle);\n\n            if (costmap->worldToMap(check_x, check_y, mx, my)) {\n                unsigned char cost = costmap->getCost(mx, my);\n                if (cost >= nav2_costmap_2d::INSCRIBED_INFLATED_OBSTACLE) {\n                    return false;  // Found an obstacle\n                }\n            }\n        }\n\n        return true;  // No obstacles detected\n    }\n\n    rclcpp_lifecycle::LifecycleNode::SharedPtr node_;\n    std::string name_;\n    std::shared_ptr<tf2_ros::Buffer> tf_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> global_costmap_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> local_costmap_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr vel_pub_;\n\n    // Parameters\n    double spin_angular_vel_;\n    double min_spin_duration_;\n    double max_spin_duration_;\n};\n\nPLUGINLIB_EXPORT_CLASS(HumanoidSpinRecovery, nav2_core::Recovery)\n```\n\n## Performance Evaluation\n\n### Navigation Performance Metrics\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <geometry_msgs/msg/twist.h>\n\nclass NavigationPerformanceEvaluator\n{\npublic:\n    struct NavigationMetrics\n    {\n        double success_rate = 0.0;\n        double average_time_to_goal = 0.0;\n        double path_efficiency = 0.0;  // actual_path_length / optimal_path_length\n        double average_velocity = 0.0;\n        int collision_count = 0;\n        int oscillation_count = 0;\n        int recovery_count = 0;\n        double energy_efficiency = 0.0;\n        double obstacle_avoidance_quality = 0.0;\n    };\n\n    NavigationPerformanceEvaluator() {}\n\n    void startTrial(const geometry_msgs::msg::Pose& start, const geometry_msgs::msg::Pose& goal)\n    {\n        trial_start_time_ = std::chrono::high_resolution_clock::now();\n        start_pose_ = start;\n        goal_pose_ = goal;\n        path_length_ = 0.0;\n        collision_count_ = 0;\n        oscillation_count_ = 0;\n        recovery_count_ = 0;\n        previous_pose_ = start;\n    }\n\n    void update(const geometry_msgs::msg::Pose& current_pose,\n               const geometry_msgs::msg::Twist& cmd_vel,\n               bool in_collision = false,\n               bool in_recovery = false)\n    {\n        // Update path length\n        double delta = std::sqrt(std::pow(current_pose.position.x - previous_pose_.position.x, 2) +\n                                std::pow(current_pose.position.y - previous_pose_.position.y, 2));\n        path_length_ += delta;\n        previous_pose_ = current_pose;\n\n        // Count collisions\n        if (in_collision) {\n            collision_count_++;\n        }\n\n        // Count oscillations (rapid direction changes)\n        if (std::abs(cmd_vel.angular.z) > oscillation_threshold_) {\n            oscillation_count_++;\n        }\n\n        // Count recovery behaviors\n        if (in_recovery) {\n            recovery_count_++;\n        }\n    }\n\n    NavigationMetrics completeTrial(bool success)\n    {\n        auto end_time = std::chrono::high_resolution_clock::now();\n        double trial_time = std::chrono::duration<double>(end_time - trial_start_time_).count();\n\n        NavigationMetrics metrics;\n        metrics.success_rate = success ? 1.0 : 0.0;\n        metrics.average_time_to_goal = success ? trial_time : 0.0;\n\n        // Calculate optimal path length (straight line)\n        double optimal_length = std::sqrt(\n            std::pow(goal_pose_.position.x - start_pose_.position.x, 2) +\n            std::pow(goal_pose_.position.y - start_pose_.position.y, 2)\n        );\n\n        metrics.path_efficiency = (optimal_length > 0) ? path_length_ / optimal_length : 1.0;\n        metrics.average_velocity = (trial_time > 0) ? path_length_ / trial_time : 0.0;\n        metrics.collision_count = collision_count_;\n        metrics.oscillation_count = oscillation_count_;\n        metrics.recovery_count = recovery_count_;\n\n        // Energy efficiency could be calculated based on actuator commands\n        metrics.energy_efficiency = calculateEnergyEfficiency();\n\n        // Obstacle avoidance quality based on minimum distances to obstacles\n        metrics.obstacle_avoidance_quality = calculateObstacleAvoidanceQuality();\n\n        return metrics;\n    }\n\n    void printMetrics(const NavigationMetrics& metrics)\n    {\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"Navigation Performance Metrics:\");\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Success Rate: %.2f\", metrics.success_rate);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Avg Time to Goal: %.2f s\", metrics.average_time_to_goal);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Path Efficiency: %.2f\", metrics.path_efficiency);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Avg Velocity: %.2f m/s\", metrics.average_velocity);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Collisions: %d\", metrics.collision_count);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Oscillations: %d\", metrics.oscillation_count);\n        RCLCPP_INFO(rclcpp::get_logger(\"navigation_eval\"),\n            \"  Recoveries: %d\", metrics.recovery_count);\n    }\n\nprivate:\n    double calculateEnergyEfficiency()\n    {\n        // Placeholder for energy efficiency calculation\n        // This would consider motor commands, robot dynamics, etc.\n        return 1.0;  // Perfect efficiency for now\n    }\n\n    double calculateObstacleAvoidanceQuality()\n    {\n        // Placeholder for obstacle avoidance quality\n        // This would consider minimum distances to obstacles during navigation\n        return 1.0;  // Perfect avoidance for now\n    }\n\n    std::chrono::high_resolution_clock::time_point trial_start_time_;\n    geometry_msgs::msg::Pose start_pose_;\n    geometry_msgs::msg::Pose goal_pose_;\n    geometry_msgs::msg::Pose previous_pose_;\n    double path_length_;\n    int collision_count_;\n    int oscillation_count_;\n    int recovery_count_;\n\n    const double oscillation_threshold_ = 0.5;  // rad/s\n};\n```\n\n## Best Practices\n\n### 1. Multi-Layered Safety System\nImplement multiple layers of safety:\n- Perception-based obstacle detection\n- Costmap-based obstacle representation\n- Collision avoidance algorithms\n- Emergency stop mechanisms\n\n### 2. Parameter Tuning\n- Use systematic parameter tuning methods\n- Test in simulation before real robot deployment\n- Monitor performance metrics continuously\n- Adapt parameters based on environment conditions\n\n### 3. Humanoid-Specific Considerations\n- Balance maintenance during navigation\n- Step planning for bipedal locomotion\n- Fall prevention mechanisms\n- Dynamic stability during turning\n\n## Exercise\n\nCreate a complete navigation system that includes:\n\n1. Global path planning with A* algorithm adapted for humanoid robots\n2. Local path planning with obstacle avoidance using DWA\n3. Integration with perception data from Isaac ROS\n4. Step planning for bipedal locomotion\n5. Recovery behaviors for humanoid robots\n6. Performance evaluation metrics\n7. Isaac Sim integration for testing\n\nTest your system in various scenarios including:\n- Navigation around static obstacles\n- Dynamic obstacle avoidance\n- Stair climbing (if applicable)\n- Tight spaces navigation\n- Multi-goal navigation tasks\n\nEvaluate the system's performance using the metrics discussed in this section.",
    "path": "module-3-ai-perception\\navigation-planning-obstacle-avoidance.md",
    "description": ""
  },
  "module-3-ai-perception\\object-detection-localization": {
    "title": "module-3-ai-perception\\object-detection-localization",
    "content": "# Object Detection and Localization Examples\n\nThis section provides practical examples of object detection and localization using NVIDIA Isaac technologies, demonstrating how AI-powered perception systems work in robotics applications.\n\n## Introduction to Object Detection in Robotics\n\nObject detection in robotics involves identifying and localizing objects in the robot's environment. This capability is crucial for:\n- Navigation and path planning\n- Manipulation and grasping\n- Scene understanding\n- Human-robot interaction\n- Autonomous decision making\n\n## Isaac ROS Perception Pipeline\n\n### Overview of Isaac ROS Perception Stack\n\n````\nIsaac ROS Perception\n Isaac ROS Image Pipeline\n    Image Proc\n    Rectification\n    Format Conversion\n Isaac ROS Visual SLAM\n    Feature Detection\n    Pose Estimation\n    Map Building\n Isaac ROS Object Detection\n    Deep Learning Models\n    TensorRT Optimization\n    Post-processing\n Isaac ROS Pose Estimation\n    2D-3D Correspondence\n    PnP Solvers\n    Refinement\n Isaac ROS Bi3D\n     3D Segmentation\n     Depth Estimation\n     Instance Segmentation\n```\n\n## Isaac ROS Object Detection Examples\n\n### 1. Isaac ROS DetectNet\n\nDetectNet is NVIDIA's specialized network for object detection optimized for robotics applications.\n\n#### Basic DetectNet Node Implementation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass IsaacDetectNetNode : public rclcpp::Node\n{\npublic:\n    IsaacDetectNetNode() : Node(\"isaac_detectnet_node\")\n    {\n        // Create subscribers\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"image_input\", 10,\n            std::bind(&IsaacDetectNetNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            \"camera_info\", 10,\n            std::bind(&IsaacDetectNetNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        // Create publisher for detections\n        detection_pub_ = this->create_publisher<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            \"detections\", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        // Process image through DetectNet model\n        auto detections = runDetectNetInference(cv_ptr->image);\n\n        // Create detection message\n        auto detection_msg = createDetectionMessage(detections, image_msg->header);\n\n        // Publish detections\n        detection_pub_->publish(detection_msg);\n    }\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr info_msg)\n    {\n        camera_info_ = *info_msg;\n    }\n\n    std::vector<Detection> runDetectNetInference(const cv::Mat& image)\n    {\n        // This would interface with the actual DetectNet model\n        // In practice, this uses TensorRT for optimized inference\n        std::vector<Detection> detections;\n\n        // Placeholder for actual inference\n        // In real implementation, this would:\n        // 1. Preprocess image for the model\n        // 2. Run inference using TensorRT\n        // 3. Post-process results\n        // 4. Apply non-maximum suppression\n        // 5. Filter by confidence threshold\n\n        return detections;\n    }\n\n    isaac_ros_detectnet_interfaces::msg::DetectionArray createDetectionMessage(\n        const std::vector<Detection>& detections,\n        const std_msgs::msg::Header& header)\n    {\n        isaac_ros_detectnet_interfaces::msg::DetectionArray detection_array;\n        detection_array.header = header;\n\n        for (const auto& detection : detections) {\n            isaac_ros_detectnet_interfaces::msg::Detection det_msg;\n            det_msg.label = detection.label;\n            det_msg.confidence = detection.confidence;\n\n            // Bounding box coordinates\n            det_msg.bbox.center.x = detection.center_x;\n            det_msg.bbox.center.y = detection.center_y;\n            det_msg.bbox.size_x = detection.width;\n            det_msg.bbox.size_y = detection.height;\n\n            detection_array.detections.push_back(det_msg);\n        }\n\n        return detection_array;\n    }\n\n    struct Detection {\n        std::string label;\n        float confidence;\n        float center_x, center_y;\n        float width, height;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_pub_;\n    sensor_msgs::msg::CameraInfo camera_info_;\n};\n```\n\n#### DetectNet Launch Configuration\n\n```xml\n<!-- detectnet.launch.xml -->\n<launch>\n  <!-- Image rectification -->\n  <node pkg=\"isaac_ros_image_proc\" exec=\"isaac_ros_image_proc\" name=\"image_proc\">\n    <param name=\"input_encoding\" value=\"bgr8\"/>\n    <param name=\"output_encoding\" value=\"bgr8\"/>\n  </node>\n\n  <!-- DetectNet node -->\n  <node pkg=\"isaac_ros_detectnet\" exec=\"isaac_ros_detectnet\" name=\"detectnet\">\n    <param name=\"model_name\" value=\"ssd_mobilenet_v2_coco\"/>\n    <param name=\"input_topic\" value=\"/image_rect_color\"/>\n    <param name=\"output_topic\" value=\"/detections\"/>\n    <param name=\"confidence_threshold\" value=\"0.5\"/>\n    <param name=\"max_objects\" value=\"10\"/>\n  </node>\n\n  <!-- Visualization node -->\n  <node pkg=\"isaac_ros_visualization\" exec=\"detection_visualizer\" name=\"detection_visualizer\">\n    <param name=\"image_topic\" value=\"/image_rect_color\"/>\n    <param name=\"detection_topic\" value=\"/detections\"/>\n    <param name=\"output_topic\" value=\"/detection_image\"/>\n  </node>\n</launch>\n```\n\n### 2. Isaac ROS Bi3D (3D Object Detection)\n\nBi3D provides 3D object detection and segmentation capabilities.\n\n#### Bi3D Node Implementation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <stereo_msgs/msg/disparity_image.hpp>\n#include <isaac_ros_bi3d_interfaces/msg/bi3_d_inference_array.hpp>\n\nclass IsaacBi3DNode : public rclcpp::Node\n{\npublic:\n    IsaacBi3DNode() : Node(\"isaac_bi3d_node\")\n    {\n        // Subscribe to stereo image pair\n        left_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"left/image_rect_color\", 10,\n            std::bind(&IsaacBi3DNode::leftImageCallback, this, std::placeholders::_1)\n        );\n\n        right_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"right/image_rect_color\", 10,\n            std::bind(&IsaacBi3DNode::rightImageCallback, this, std::placeholders::_1)\n        );\n\n        // Subscribe to disparity for depth\n        disparity_sub_ = this->create_subscription<stereo_msgs::msg::DisparityImage>(\n            \"disparity\", 10,\n            std::bind(&IsaacBi3DNode::disparityCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for 3D detections\n        bi3d_pub_ = this->create_publisher<isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray>(\n            \"bi3d_detections\", 10\n        );\n    }\n\nprivate:\n    void leftImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        if (has_right_image_ && has_disparity_) {\n            processStereoPair(msg, right_image_, disparity_);\n        } else {\n            left_image_ = msg;\n        }\n    }\n\n    void rightImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        has_right_image_ = true;\n        right_image_ = msg;\n\n        if (has_left_image_ && has_disparity_) {\n            processStereoPair(left_image_, msg, disparity_);\n        }\n    }\n\n    void disparityCallback(const stereo_msgs::msg::DisparityImage::SharedPtr msg)\n    {\n        has_disparity_ = true;\n        disparity_ = msg;\n\n        if (has_left_image_ && has_right_image_) {\n            processStereoPair(left_image_, right_image_, msg);\n        }\n    }\n\n    void processStereoPair(\n        const sensor_msgs::msg::Image::SharedPtr left,\n        const sensor_msgs::msg::Image::SharedPtr right,\n        const stereo_msgs::msg::DisparityImage::SharedPtr disparity)\n    {\n        // Run Bi3D inference\n        auto bi3d_results = runBi3DInference(left, right);\n\n        // Create 3D detection message\n        auto bi3d_msg = createBi3DMessage(bi3d_results, left->header);\n\n        // Publish results\n        bi3d_pub_->publish(bi3d_msg);\n\n        // Reset flags\n        has_left_image_ = false;\n        has_right_image_ = false;\n        has_disparity_ = false;\n    }\n\n    std::vector<Bi3DResult> runBi3DInference(\n        const sensor_msgs::msg::Image::SharedPtr left,\n        const sensor_msgs::msg::Image::SharedPtr right)\n    {\n        // Placeholder for actual Bi3D inference\n        // This would:\n        // 1. Process stereo images through Bi3D network\n        // 2. Generate 3D segmentation masks\n        // 3. Extract 3D bounding boxes\n        // 4. Estimate 3D poses\n\n        std::vector<Bi3DResult> results;\n        // Implementation would go here\n        return results;\n    }\n\n    isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray createBi3DMessage(\n        const std::vector<Bi3DResult>& results,\n        const std_msgs::msg::Header& header)\n    {\n        isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray bi3d_array;\n        bi3d_array.header = header;\n\n        for (const auto& result : results) {\n            isaac_ros_bi3d_interfaces::msg::Bi3DInference bi3d_msg;\n            bi3d_msg.class_id = result.class_id;\n            bi3d_msg.confidence = result.confidence;\n\n            // 3D bounding box\n            bi3d_msg.bounding_box_3d.center.position.x = result.center_x;\n            bi3d_msg.bounding_box_3d.center.position.y = result.center_y;\n            bi3d_msg.bounding_box_3d.center.position.z = result.center_z;\n\n            // Convert Euler angles to quaternion\n            tf2::Quaternion q;\n            q.setRPY(result.roll, result.pitch, result.yaw);\n            bi3d_msg.bounding_box_3d.center.orientation.x = q.x();\n            bi3d_msg.bounding_box_3d.center.orientation.y = q.y();\n            bi3d_msg.bounding_box_3d.center.orientation.z = q.z();\n            bi3d_msg.bounding_box_3d.center.orientation.w = q.w();\n\n            bi3d_msg.bounding_box_3d.size.x = result.size_x;\n            bi3d_msg.bounding_box_3d.size.y = result.size_y;\n            bi3d_msg.bounding_box_3d.size.z = result.size_z;\n\n            bi3d_array.inferences.push_back(bi3d_msg);\n        }\n\n        return bi3d_array;\n    }\n\n    struct Bi3DResult {\n        int class_id;\n        float confidence;\n        float center_x, center_y, center_z;\n        float roll, pitch, yaw;\n        float size_x, size_y, size_z;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr left_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr right_sub_;\n    rclcpp::Subscription<stereo_msgs::msg::DisparityImage>::SharedPtr disparity_sub_;\n    rclcpp::Publisher<isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray>::SharedPtr bi3d_pub_;\n\n    sensor_msgs::msg::Image::SharedPtr left_image_;\n    sensor_msgs::msg::Image::SharedPtr right_image_;\n    stereo_msgs::msg::DisparityImage::SharedPtr disparity_;\n\n    bool has_left_image_ = false;\n    bool has_right_image_ = false;\n    bool has_disparity_ = false;\n};\n```\n\n## Object Localization Examples\n\n### 1. Camera-Object 3D Localization\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectLocalizationNode : public rclcpp::Node\n{\npublic:\n    ObjectLocalizationNode() : Node(\"object_localization_node\"), tf_buffer_(this->get_clock())\n    {\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            \"detections\", 10,\n            std::bind(&ObjectLocalizationNode::detectionCallback, this, std::placeholders::_1)\n        );\n\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            \"camera_info\", 10,\n            std::bind(&ObjectLocalizationNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        object_pose_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            \"object_3d_position\", 10\n        );\n\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(tf_buffer_);\n    }\n\nprivate:\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr detections)\n    {\n        if (!camera_info_received_ || !has_camera_to_robot_tf_) {\n            RCLCPP_WARN(this->get_logger(), \"Camera info or TF not available yet\");\n            return;\n        }\n\n        for (const auto& detection : detections->detections) {\n            if (detection.confidence < confidence_threshold_) {\n                continue;  // Skip low-confidence detections\n            }\n\n            // Convert 2D bounding box center to 3D point\n            geometry_msgs::msg::PointStamped pixel_point;\n            pixel_point.header = detections->header;\n            pixel_point.point.x = detection.bbox.center.x;\n            pixel_point.point.y = detection.bbox.center.y;\n            pixel_point.point.z = 1.0;  // Placeholder depth\n\n            // Convert pixel coordinates to 3D camera frame\n            geometry_msgs::msg::PointStamped camera_point;\n            camera_point = pixelToCameraFrame(pixel_point);\n\n            // Transform to robot base frame\n            geometry_msgs::msg::PointStamped robot_point;\n            robot_point = transformToRobotFrame(camera_point);\n\n            // Create and publish object position\n            geometry_msgs::msg::PointStamped object_position;\n            object_position.header = robot_point.header;\n            object_position.point = robot_point.point;\n\n            // Add object label as metadata (in a real system, you might publish this separately)\n            RCLCPP_INFO(this->get_logger(),\n                \"Detected %s at position: (%.2f, %.2f, %.2f) with confidence %.2f\",\n                detection.label.c_str(),\n                object_position.point.x,\n                object_position.point.y,\n                object_position.point.z,\n                detection.confidence\n            );\n\n            object_pose_pub_->publish(object_position);\n        }\n    }\n\n    geometry_msgs::msg::PointStamped pixelToCameraFrame(const geometry_msgs::msg::PointStamped& pixel_point)\n    {\n        geometry_msgs::msg::PointStamped camera_point;\n        camera_point.header = pixel_point.header;  // Keep same timestamp/frame initially\n\n        // Convert pixel coordinates to normalized coordinates\n        double x_norm = (pixel_point.point.x - camera_info_.k[2]) / camera_info_.k[0];  // cx, fx\n        double y_norm = (pixel_point.point.y - camera_info_.k[5]) / camera_info_.k[4];  // cy, fy\n\n        // For this example, assume depth is known from other sources\n        // In practice, you'd get depth from stereo, LIDAR, or depth sensor\n        double depth = estimateDepth(pixel_point.point.x, pixel_point.point.y);\n\n        camera_point.point.x = x_norm * depth;\n        camera_point.point.y = y_norm * depth;\n        camera_point.point.z = depth;\n\n        return camera_point;\n    }\n\n    geometry_msgs::msg::PointStamped transformToRobotFrame(const geometry_msgs::msg::PointStamped& camera_point)\n    {\n        geometry_msgs::msg::PointStamped robot_point;\n\n        try {\n            // Transform from camera frame to robot base frame\n            tf_buffer_.transform(camera_point, robot_point, \"base_link\");\n        } catch (tf2::TransformException& ex) {\n            RCLCPP_ERROR(this->get_logger(), \"Transform failed: %s\", ex.what());\n            return camera_point;  // Return original if transform fails\n        }\n\n        return robot_point;\n    }\n\n    double estimateDepth(double u, double v)\n    {\n        // Placeholder depth estimation\n        // In a real system, this would come from:\n        // 1. Stereo vision\n        // 2. Depth sensor (RGB-D camera, LIDAR)\n        // 3. Monocular depth estimation\n        // 4. Object size-based estimation (if object size is known)\n\n        // For this example, return a fixed depth\n        // A more realistic approach would use stereo disparity or other depth sources\n        return 1.0;  // 1 meter depth as placeholder\n    }\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        camera_info_ = *msg;\n        camera_info_received_ = true;\n    }\n\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pose_pub_;\n\n    tf2_ros::Buffer tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n\n    sensor_msgs::msg::CameraInfo camera_info_;\n    bool camera_info_received_ = false;\n    bool has_camera_to_robot_tf_ = false;\n\n    const double confidence_threshold_ = 0.7;\n};\n```\n\n### 2. Semantic Segmentation for Object Localization\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass SemanticSegmentationNode : public rclcpp::Node\n{\npublic:\n    SemanticSegmentationNode() : Node(\"semantic_segmentation_node\")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"image_input\", 10,\n            std::bind(&SemanticSegmentationNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        segmentation_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            \"segmentation_output\", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        // Run semantic segmentation\n        cv::Mat segmentation_mask = runSegmentationInference(cv_ptr->image);\n\n        // Create result image with segmentation overlay\n        cv::Mat result_image = createSegmentationOverlay(cv_ptr->image, segmentation_mask);\n\n        // Publish segmentation result\n        publishSegmentationResult(result_image, image_msg->header);\n    }\n\n    cv::Mat runSegmentationInference(const cv::Mat& image)\n    {\n        // Placeholder for actual segmentation inference\n        // This would typically use a model like DeepLab, SegNet, or similar\n        // For Isaac ROS, this might use Isaac ROS Segmentation packages\n\n        cv::Mat segmentation_mask;\n\n        // In a real implementation, this would:\n        // 1. Preprocess image for the segmentation model\n        // 2. Run inference using TensorRT\n        // 3. Post-process to get class labels for each pixel\n        // 4. Return a mask where each pixel value represents the class ID\n\n        // For this example, return a dummy mask\n        segmentation_mask = cv::Mat::zeros(image.size(), CV_8UC1);\n\n        // Simulate detection of a few classes in specific regions\n        cv::rectangle(segmentation_mask, cv::Rect(100, 100, 200, 150), cv::Scalar(1), -1); // Class 1\n        cv::rectangle(segmentation_mask, cv::Rect(300, 200, 150, 100), cv::Scalar(2), -1); // Class 2\n\n        return segmentation_mask;\n    }\n\n    cv::Mat createSegmentationOverlay(const cv::Mat& original_image, const cv::Mat& segmentation_mask)\n    {\n        cv::Mat overlay = original_image.clone();\n\n        // Define colors for different classes\n        std::vector<cv::Vec3b> class_colors = {\n            cv::Vec3b(0, 0, 0),      // Class 0: background (black)\n            cv::Vec3b(255, 0, 0),    // Class 1: red\n            cv::Vec3b(0, 255, 0),    // Class 2: green\n            cv::Vec3b(0, 0, 255),    // Class 3: blue\n            cv::Vec3b(255, 255, 0),  // Class 4: cyan\n            cv::Vec3b(255, 0, 255),  // Class 5: magenta\n        };\n\n        // Create overlay with transparency\n        for (int y = 0; y < segmentation_mask.rows; y++) {\n            for (int x = 0; x < segmentation_mask.cols; x++) {\n                int class_id = segmentation_mask.at<uchar>(y, x);\n                if (class_id > 0 && class_id < class_colors.size()) {\n                    // Blend original color with class color\n                    cv::Vec3b& pixel = overlay.at<cv::Vec3b>(y, x);\n                    cv::Vec3b class_color = class_colors[class_id];\n\n                    // Simple blending (50% original, 50% class color)\n                    pixel = 0.5 * pixel + 0.5 * class_color;\n                }\n            }\n        }\n\n        return overlay;\n    }\n\n    void publishSegmentationResult(const cv::Mat& result_image, const std_msgs::msg::Header& header)\n    {\n        cv_bridge::CvImage cv_image;\n        cv_image.header = header;\n        cv_image.encoding = sensor_msgs::image_encodings::BGR8;\n        cv_image.image = result_image;\n\n        segmentation_pub_->publish(*cv_image.toImageMsg());\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr segmentation_pub_;\n};\n```\n\n## Isaac Sim Perception Integration\n\n### Isaac Sim Perception Configuration\n\n```python\n# Isaac Sim perception setup\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nimport numpy as np\n\nclass IsaacSimPerception:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_perception_sensors()\n\n    def setup_perception_sensors(self):\n        # Add a robot to the scene\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets\")\n            return\n\n        # Add a simple robot with sensors\n        robot_path = assets_root_path + \"/Isaac/Robots/Carter/carter_navigate.usd\"\n        add_reference_to_stage(usd_path=robot_path, prim_path=\"/World/Carter\")\n\n        # Add a camera sensor\n        self.camera = Camera(\n            prim_path=\"/World/Carter/chassis/camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add a LIDAR sensor\n        self.lidar = RotatingLidarPhysX(\n            prim_path=\"/World/Carter/chassis/lidar\",\n            translation=np.array([0.0, 0.0, 0.25]),\n            config=\"Carter\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def get_sensor_data(self):\n        # Get camera data\n        rgb_data = self.camera.get_rgb()\n        depth_data = self.camera.get_depth()\n        seg_data = self.camera.get_semantic_segmentation()\n\n        # Get LIDAR data\n        lidar_data = self.lidar.get_linear_depth_data()\n\n        return {\n            'rgb': rgb_data,\n            'depth': depth_data,\n            'segmentation': seg_data,\n            'lidar': lidar_data\n        }\n\n    def run_perception_pipeline(self):\n        # Main perception loop\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get sensor data\n            sensor_data = self.get_sensor_data()\n\n            # Process perception data\n            objects = self.detect_objects(sensor_data)\n\n            # Localize objects in world coordinates\n            object_poses = self.localize_objects(objects, sensor_data)\n\n            # Print results\n            self.print_perception_results(object_poses)\n\n    def detect_objects(self, sensor_data):\n        # Placeholder for object detection\n        # In Isaac Sim, this would interface with Isaac ROS perception packages\n        # or use built-in synthetic data generation\n\n        # For this example, return simulated detections\n        objects = [\n            {'class': 'box', 'confidence': 0.95, 'bbox': [100, 100, 200, 150]},\n            {'class': 'cylinder', 'confidence': 0.89, 'bbox': [300, 200, 150, 100]}\n        ]\n\n        return objects\n\n    def localize_objects(self, objects, sensor_data):\n        # Convert 2D detections to 3D world coordinates\n        # This would use depth information and camera parameters\n\n        object_poses = []\n\n        for obj in objects:\n            # Convert 2D bbox center to 3D using depth\n            center_x = (obj['bbox'][0] + obj['bbox'][2]) // 2\n            center_y = (obj['bbox'][1] + obj['bbox'][3]) // 2\n\n            # Get depth at center point\n            depth = sensor_data['depth'][center_y, center_x]\n\n            if depth < 10.0:  # Valid depth check\n                # Convert pixel coordinates to world coordinates\n                # This requires camera intrinsic parameters\n                world_pos = self.pixel_to_world(\n                    center_x, center_y, depth,\n                    self.camera.prim.GetAttribute(\"xformOp:transform\").Get()\n                )\n\n                object_poses.append({\n                    'class': obj['class'],\n                    'position': world_pos,\n                    'confidence': obj['confidence']\n                })\n\n        return object_poses\n\n    def pixel_to_world(self, u, v, depth, camera_transform):\n        # Convert pixel coordinates to world coordinates\n        # This is a simplified version - in practice, you'd use camera intrinsics\n\n        # Camera intrinsic parameters (these would come from camera config)\n        fx = 616.363  # Focal length x\n        fy = 616.363  # Focal length y\n        cx = 313.071  # Principal point x\n        cy = 245.091  # Principal point y\n\n        # Convert to camera coordinates\n        x_cam = (u - cx) * depth / fx\n        y_cam = (v - cy) * depth / fy\n        z_cam = depth\n\n        # Transform to world coordinates using camera pose\n        # (simplified - would need proper transformation matrix)\n        x_world = x_cam  # Simplified\n        y_world = y_cam\n        z_world = z_cam\n\n        return [x_world, y_world, z_world]\n\n    def print_perception_results(self, object_poses):\n        print(\"Perception Results:\")\n        for obj in object_poses:\n            print(f\"  {obj['class']}: ({obj['position'][0]:.2f}, {obj['position'][1]:.2f}, {obj['position'][2]:.2f}), conf: {obj['confidence']:.2f}\")\n```\n\n## Practical Examples\n\n### Example 1: Person Detection and Localization\n\n```cpp\n// Complete example for detecting and localizing people\nclass PersonDetectionNode : public rclcpp::Node\n{\npublic:\n    PersonDetectionNode() : Node(\"person_detection_node\")\n    {\n        // Subscribe to camera image\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/image_raw\", 10,\n            std::bind(&PersonDetectionNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        // Subscribe to camera info\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            \"camera/camera_info\", 10,\n            std::bind(&PersonDetectionNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for person positions\n        person_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            \"person_position\", 10\n        );\n\n        // Publisher for visualization\n        viz_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            \"person_detection_viz\", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        if (!camera_info_received_) return;\n\n        // Convert to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        // Run person detection\n        std::vector<PersonDetection> persons = detectPersons(cv_ptr->image);\n\n        // Process each detected person\n        for (const auto& person : persons) {\n            if (person.confidence > 0.8) {  // Confidence threshold\n                // Localize person in 3D space\n                geometry_msgs::msg::PointStamped person_3d = localizePerson(\n                    person, image_msg->header\n                );\n\n                // Publish person position\n                person_pub_->publish(person_3d);\n\n                // Add to visualization\n                cv::rectangle(cv_ptr->image, person.bbox, cv::Scalar(0, 255, 0), 2);\n                std::string label = \"Person: \" + std::to_string(person.confidence);\n                cv::putText(cv_ptr->image, label,\n                           cv::Point(person.bbox.x, person.bbox.y - 10),\n                           cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 1);\n            }\n        }\n\n        // Publish visualization image\n        viz_pub_->publish(*cv_ptr->toImageMsg());\n    }\n\n    std::vector<PersonDetection> detectPersons(const cv::Mat& image)\n    {\n        std::vector<PersonDetection> detections;\n\n        // In a real implementation, this would run a DNN model\n        // such as YOLO, SSD MobileNet, or Isaac ROS DetectNet\n        // For this example, we'll use OpenCV's HOG descriptor\n\n        cv::HOGDescriptor hog;\n        hog.setSVMDetector(cv::HOGDescriptor::getDefaultPeopleDetector());\n\n        std::vector<cv::Rect> found_locations;\n        std::vector<double> found_weights;\n\n        hog.detectMultiScale(image, found_locations, found_weights, 0, cv::Size(8,8), cv::Size(32,32), 1.05, 2, false);\n\n        for (size_t i = 0; i < found_locations.size(); ++i) {\n            PersonDetection detection;\n            detection.bbox = found_locations[i];\n            detection.confidence = found_weights[i];\n            detection.center_x = detection.bbox.x + detection.bbox.width / 2.0;\n            detection.center_y = detection.bbox.y + detection.bbox.height / 2.0;\n            detections.push_back(detection);\n        }\n\n        return detections;\n    }\n\n    geometry_msgs::msg::PointStamped localizePerson(\n        const PersonDetection& person,\n        const std_msgs::msg::Header& header)\n    {\n        geometry_msgs::msg::PointStamped person_3d;\n        person_3d.header = header;\n\n        // Estimate depth using simple heuristics\n        // In practice, you'd use stereo vision or depth sensor\n        double depth = estimatePersonDepth(person.bbox.height);\n\n        // Convert pixel to 3D coordinates\n        double x_norm = (person.center_x - camera_info_.k[2]) / camera_info_.k[0];\n        double y_norm = (person.center_y - camera_info_.k[5]) / camera_info_.k[4];\n\n        person_3d.point.x = x_norm * depth;\n        person_3d.point.y = y_norm * depth;\n        person_3d.point.z = depth;\n\n        return person_3d;\n    }\n\n    double estimatePersonDepth(int bbox_height)\n    {\n        // Simple depth estimation based on bounding box height\n        // Assumes average person height is ~1.7m\n        // height_in_pixels = (focal_length * real_height) / depth\n        // So depth = (focal_length * real_height) / height_in_pixels\n\n        double focal_length = camera_info_.k[0];  // fx\n        double real_person_height = 1.7;  // meters\n        double pixel_height = static_cast<double>(bbox_height);\n\n        return (focal_length * real_person_height) / pixel_height;\n    }\n\n    struct PersonDetection {\n        cv::Rect bbox;\n        double confidence;\n        double center_x, center_y;\n    };\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        camera_info_ = *msg;\n        camera_info_received_ = true;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr person_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr viz_pub_;\n\n    sensor_msgs::msg::CameraInfo camera_info_;\n    bool camera_info_received_ = false;\n};\n```\n\n### Example 2: Object Detection with Isaac ROS and Isaac Sim\n\n```python\n# Isaac Sim + Isaac ROS integration example\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom isaac_ros_detectnet_interfaces.msg import DetectionArray\nfrom geometry_msgs.msg import PointStamped\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # ROS 2 interface\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            DetectionArray, '/detectnet/detections', self.detection_callback, 10\n        )\n        self.object_pub = self.create_publisher(\n            PointStamped, '/detected_object_position', 10\n        )\n        self.viz_pub = self.create_publisher(\n            Image, '/perception_visualization', 10\n        )\n\n        # Storage\n        self.camera_info = None\n        self.latest_image = None\n\n    def image_callback(self, msg):\n        self.latest_image = msg\n\n    def camera_info_callback(self, msg):\n        self.camera_info = msg\n\n    def detection_callback(self, msg):\n        if self.latest_image is None or self.camera_info is None:\n            return\n\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(self.latest_image, \"bgr8\")\n\n        # Process detections\n        for detection in msg.detections:\n            if detection.confidence > 0.7:  # Confidence threshold\n                # Localize object in 3D\n                object_3d = self.localize_object_3d(\n                    detection.bbox.center.x,\n                    detection.bbox.center.y,\n                    detection\n                )\n\n                # Publish 3D position\n                self.object_pub.publish(object_3d)\n\n                # Draw bounding box on image\n                pt1 = (int(detection.bbox.center.x - detection.bbox.size_x/2),\n                       int(detection.bbox.center.y - detection.bbox.size_y/2))\n                pt2 = (int(detection.bbox.center.x + detection.bbox.size_x/2),\n                       int(detection.bbox.center.y + detection.bbox.size_y/2))\n                cv2.rectangle(cv_image, pt1, pt2, (0, 255, 0), 2)\n                cv2.putText(cv_image, f\"{detection.label}: {detection.confidence:.2f}\",\n                           (pt1[0], pt1[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n        # Publish visualization\n        viz_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding=\"bgr8\")\n        viz_msg.header = self.latest_image.header\n        self.viz_pub.publish(viz_msg)\n\n    def localize_object_3d(self, x_2d, y_2d, detection):\n        # This is a simplified example\n        # In practice, you'd use depth information from stereo or depth sensor\n        point_3d = PointStamped()\n        point_3d.header = detection.header\n\n        # Estimate depth based on object size or use depth map\n        # For this example, assume a fixed depth of 2 meters\n        estimated_depth = 2.0  # meters\n\n        # Convert 2D pixel coordinates to 3D using camera intrinsics\n        if self.camera_info:\n            fx = self.camera_info.k[0]  # Focal length x\n            fy = self.camera_info.k[4]  # Focal length y\n            cx = self.camera_info.k[2]  # Principal point x\n            cy = self.camera_info.k[5]  # Principal point y\n\n            point_3d.point.x = (x_2d - cx) * estimated_depth / fx\n            point_3d.point.y = (y_2d - cy) * estimated_depth / fy\n            point_3d.point.z = estimated_depth\n\n        return point_3d\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Performance Optimization\n\n### 1. TensorRT Optimization for Deep Learning Models\n\n```cpp\n// Example of TensorRT optimization for perception\n#include <NvInfer.h>\n#include <cuda_runtime_api.h>\n\nclass OptimizedPerceptionNode : public rclcpp::Node\n{\npublic:\n    OptimizedPerceptionNode() : Node(\"optimized_perception_node\")\n    {\n        // Initialize TensorRT engine\n        initializeTensorRTEngine();\n    }\n\nprivate:\n    void initializeTensorRTEngine()\n    {\n        // This would load a pre-built TensorRT engine\n        // for optimized inference of perception models\n        // The engine would be built offline from ONNX models\n    }\n\n    std::vector<Detection> runOptimizedInference(const cv::Mat& image)\n    {\n        // Run inference using TensorRT for maximum performance\n        // This would include:\n        // 1. Memory management for GPU\n        // 2. Batch processing\n        // 3. Asynchronous execution\n        // 4. Proper input/output binding\n\n        std::vector<Detection> detections;\n        // Implementation would go here\n        return detections;\n    }\n\n    nvinfer1::ICudaEngine* engine_;\n    nvinfer1::IExecutionContext* context_;\n    cudaStream_t stream_;\n    void* buffers_[2];  // Input and output buffers\n};\n```\n\n### 2. Multi-Threaded Perception Pipeline\n\n```cpp\n#include <thread>\n#include <queue>\n#include <mutex>\n#include <condition_variable>\n\nclass MultiThreadedPerceptionNode : public rclcpp::Node\n{\npublic:\n    MultiThreadedPerceptionNode() : Node(\"multithreaded_perception_node\")\n    {\n        // Create threads for different perception tasks\n        detection_thread_ = std::thread(&MultiThreadedPerceptionNode::detectionLoop, this);\n        localization_thread_ = std::thread(&MultiThreadedPerceptionNode::localizationLoop, this);\n\n        // Subscribe to image\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"image_input\", 10,\n            std::bind(&MultiThreadedPerceptionNode::imageCallback, this, std::placeholders::_1)\n        );\n    }\n\n    ~MultiThreadedPerceptionNode()\n    {\n        running_ = false;\n        if (detection_thread_.joinable()) detection_thread_.join();\n        if (localization_thread_.joinable()) localization_thread_.join();\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        std::lock_guard<std::mutex> lock(image_queue_mutex_);\n        image_queue_.push(image_msg);\n\n        if (image_queue_.size() > max_queue_size_) {\n            image_queue_.pop();  // Drop oldest if queue is full\n        }\n\n        image_queue_cond_.notify_one();\n    }\n\n    void detectionLoop()\n    {\n        while (running_) {\n            sensor_msgs::msg::Image::SharedPtr image_msg;\n\n            {\n                std::unique_lock<std::mutex> lock(image_queue_mutex_);\n                image_queue_cond_.wait(lock, [this] { return !image_queue_.empty() || !running_; });\n\n                if (!running_) break;\n\n                image_msg = image_queue_.front();\n                image_queue_.pop();\n            }\n\n            // Run object detection\n            auto detections = runDetection(image_msg);\n\n            // Add to detection queue\n            {\n                std::lock_guard<std::mutex> lock(detection_queue_mutex_);\n                detection_queue_.push(std::make_pair(image_msg->header, detections));\n            }\n\n            detection_queue_cond_.notify_one();\n        }\n    }\n\n    void localizationLoop()\n    {\n        while (running_) {\n            std_msgs::msg::Header header;\n            std::vector<Detection> detections;\n\n            {\n                std::unique_lock<std::mutex> lock(detection_queue_mutex_);\n                detection_queue_cond_.wait(lock, [this] { return !detection_queue_.empty() || !running_; });\n\n                if (!running_) break;\n\n                auto detection_pair = detection_queue_.front();\n                header = detection_pair.first;\n                detections = detection_pair.second;\n                detection_queue_.pop();\n            }\n\n            // Run localization\n            auto object_positions = runLocalization(detections, header);\n\n            // Publish results\n            publishResults(object_positions);\n        }\n    }\n\n    std::vector<Detection> runDetection(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Run object detection on the image\n        // Implementation would go here\n        return std::vector<Detection>();\n    }\n\n    std::vector<ObjectPosition> runLocalization(\n        const std::vector<Detection>& detections,\n        const std_msgs::msg::Header& header)\n    {\n        // Localize objects in 3D space\n        // Implementation would go here\n        return std::vector<ObjectPosition>();\n    }\n\n    void publishResults(const std::vector<ObjectPosition>& positions)\n    {\n        // Publish localization results\n        // Implementation would go here\n    }\n\n    struct Detection {\n        std::string label;\n        float confidence;\n        cv::Rect bbox;\n    };\n\n    struct ObjectPosition {\n        std::string label;\n        geometry_msgs::msg::Point position;\n        float confidence;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n\n    // Image processing queue\n    std::queue<sensor_msgs::msg::Image::SharedPtr> image_queue_;\n    std::mutex image_queue_mutex_;\n    std::condition_variable image_queue_cond_;\n\n    // Detection queue\n    std::queue<std::pair<std_msgs::msg::Header, std::vector<Detection>>> detection_queue_;\n    std::mutex detection_queue_mutex_;\n    std::condition_variable detection_queue_cond_;\n\n    std::thread detection_thread_;\n    std::thread localization_thread_;\n    std::atomic<bool> running_{true};\n    const size_t max_queue_size_ = 5;\n};\n```\n\n## Best Practices\n\n### 1. Confidence Thresholding\nAlways use confidence thresholds to filter out low-quality detections:\n- Set appropriate thresholds based on your application requirements\n- Consider using adaptive thresholds based on scene complexity\n- Validate detections with geometric consistency checks\n\n### 2. Multi-Sensor Fusion\nCombine data from multiple sensors for robust perception:\n- Fuse camera, LIDAR, and radar data\n- Use Kalman filters or particle filters for tracking\n- Implement sensor validation and fault detection\n\n### 3. Performance Monitoring\nMonitor perception performance in real-time:\n- Track inference time and frame rates\n- Monitor memory and GPU usage\n- Log detection accuracy and false positive rates\n\n## Exercise\n\nCreate a complete perception pipeline that includes:\n\n1. Object detection using Isaac ROS DetectNet\n2. 3D localization using stereo vision or depth information\n3. Multi-threaded processing for real-time performance\n4. Integration with Isaac Sim for testing\n5. Visualization of detection results\n6. Performance evaluation metrics\n\nTest your pipeline with various objects in different lighting conditions and evaluate its accuracy and performance.",
    "path": "module-3-ai-perception\\object-detection-localization.md",
    "description": ""
  },
  "module-3-ai-perception\\perception-navigation-pipeline-diagrams": {
    "title": "module-3-ai-perception\\perception-navigation-pipeline-diagrams",
    "content": "# Perception and Navigation Pipeline Diagrams\n\nThis section describes the diagrams that illustrate the perception and navigation pipeline for humanoid robots using NVIDIA Isaac technologies. These diagrams help visualize the data flow and system architecture.\n\n## 1. Overall Perception-Navigation System Architecture\n\n### Diagram: isaac-perception-navigation-overview.svg\n\nThis diagram shows the complete system architecture from sensors to navigation commands:\n\n```\nSensors\n Cameras (RGB, Stereo, RGB-D)\n LIDAR/RADAR\n IMU\n Encoders\n GPS (if available)\n\n (Raw Data)\n\nIsaac Sim Environment\n Scene Rendering\n Physics Simulation\n Sensor Simulation\n\n (Simulated Data)\n\nROS 2 Middleware\n Message Passing\n Service Calls\n Action Servers\n\n (Processed Data)\n\nIsaac ROS Components\n Isaac ROS Image Pipeline\n    Image Rectification\n    Format Conversion\n    Rectification\n Isaac ROS Visual SLAM\n    Feature Detection\n    Pose Estimation\n    Map Building\n Isaac ROS Object Detection\n    Deep Learning Models\n    TensorRT Optimization\n    Post-processing\n Isaac ROS Bi3D\n    3D Segmentation\n    Depth Estimation\n    Instance Segmentation\n Isaac ROS Navigation\n     Path Planning\n     Trajectory Generation\n     Control\n\n (Processed Information)\n\nNavigation Stack (Nav2)\n Global Planner (A*)\n Local Planner (DWA/TEB)\n Controller (PID/MPC)\n Costmaps (Global/Local)\n Behavior Trees\n Recovery Behaviors\n\n (Navigation Commands)\n\nRobot Actuators\n Joint Controllers\n Balance Controllers\n Locomotion Controllers\n```\n\n## 2. Isaac ROS Perception Pipeline\n\n### Diagram: isaac-ros-perception-pipeline.svg\n\nThis diagram illustrates the flow of perception processing:\n\n```\n        \n   Raw Images      Isaac ROS Image   Isaac ROS DetectNet \n (Camera/LIDAR)         Processing             Object Detection  \n                                                                 \n  RGB Images          Rectification         TensorRT          \n  Depth Maps          Calibration           Bounding Boxes    \n  Point Clouds        Format Conv.          Class Labels      \n        \n                                                                 \n                                               \n   Depth Data                                                   \n                         \n  Stereo Depth   Isaac ROS Visual   Isaac ROS Pose      \n  RGB-D Depth           SLAM                  Estimation        \n  LIDAR Points                                                  \n      Feature Detect        2D-3D Corres.     \n                         Pose Estimation       PnP Solvers       \n                         Map Building          Refinement        \n                           \n                                                         \n                                                         \n                           \n                        Isaac ROS Bi3D       Isaac ROS Bi3D      \n                          3D Segmentation          Inference Array   \n                                                                     \n                         3D Segmentation         3D Bounding Boxes \n                         Depth Estimation        Instance Masks    \n                         Instance Seg.           3D Poses          \n                           \n```\n\n## 3. Navigation Planning Pipeline\n\n### Diagram: navigation-planning-pipeline.svg\n\nThis diagram shows the navigation planning process:\n\n```\n        \n   Global Map      Global Planner        Path Smoothing    \n                        (A*/Dijkstra)                              \n  Static Map                                   Path Optimization \n  Occupancy           Path to Goal            Curvature Limits  \n  Semantics           Waypoints               Smooth Trajectory \n        \n                                                          \n                                                          \n        \n Robot Position   Local Planner         Trajectory Ctrl.   \n (AMCL/SLAM)          (DWA/TEB/MBF)                                \n                                                Velocity Commands \n  Current Pose        Local Path Adj.         Twist Messages    \n  Uncertainty         Obstacle Avoid.         Dynamic Control   \n        \n                                                          \n         \n                                 \n                       \n                         Robot Controller   \n                                            \n                         Joint Commands    \n                         Balance Control   \n                         Step Planning     \n                       \n```\n\n## 4. Humanoid-Specific Navigation Pipeline\n\n### Diagram: humanoid-nav-pipeline.svg\n\nThis diagram highlights humanoid-specific aspects of navigation:\n\n```\n        \n Perception Data  Humanoid Path        Step Planner        \n                        Planning                                   \n  Objects                                      Foot Placement    \n  Obstacles           Global Path             Balance Maint.    \n  Free Space          Local Adjustments       Gait Generation   \n        \n                                                          \n                                                          \n        \n State Estimator  Balance Controller   Gait Controller     \n (Extended Kalman                                                   \n  Filter)              COM Position            Joint Trajectories\n                       Stability Metrics       Walking Patterns  \n  Robot State         Fall Prevention         Step Timing     \n  Uncertainty         Recovery Actions        Foot Trajectory   \n        \n                                                          \n         \n                                 \n                       \n                         Actuator Commands  \n                                            \n                         Hip/Knee/Ankle   \n                         Balance Adjust.   \n                         Fall Recovery     \n                       \n```\n\n## 5. Isaac Sim Integration Pipeline\n\n### Diagram: isaac-sim-integration-pipeline.svg\n\nThis diagram shows how Isaac Sim integrates with the perception and navigation system:\n\n```\n        \n Isaac Sim        Isaac ROS Bridge     ROS 2 Ecosystem     \n Environment                                                       \n                       Message Bridge          Perception Nodes  \n  Scene               TF Transforms           Navigation Stack  \n  Physics             Service Bridge          Control Nodes     \n  Sensors             Action Bridge           Visualization     \n        \n                                                          \n                                                          \n        \n Isaac Sim        Isaac ROS Perception Isaac ROS Navigation\n Sensors              Components               Components          \n                                                                   \n  Cameras             Isaac ROS DetectNet     Isaac ROS VSLAM   \n  LIDAR               Isaac ROS Visual        Isaac ROS Bi3D    \n  IMU                  SLAM                    Isaac ROS Pose    \n  Joint States        Isaac ROS Bi3D           Estimation        \n        \n                                                          \n         \n                                 \n                       \n                         Real Robot         \n                         (when deployed)    \n                                            \n                         Hardware Drivers  \n                         Real Sensors      \n                         Physical Robot    \n                       \n```\n\n## 6. Data Flow in Perception System\n\n### Diagram: perception-data-flow.svg\n\nThis diagram shows the detailed data flow in the perception system:\n\n```\n\n   Raw Sensors   \n                 \n  Camera Images \n  LIDAR Scans   \n  IMU Data      \n  Joint States  \n\n         \n         \n    \n Preprocessing    Feature Extraction  \n                                          \n  Calibration         Keypoint Detect.  \n  Rectification       Descriptor Comp.  \n  Denoising           Feature Matching  \n  Normalization       Optical Flow      \n    \n                                \n                                \n    \n Deep Learning    Post-Processing     \n Inference                                \n                       NMS (Non-Max Sup.)\n  TensorRT            Bounding Box Adj. \n  CNN Models          Confidence Thresh.\n  GPU Acceler.        Spatial Filtering \n    \n                                \n                                \n    \n 3D Reconstruction Object Tracking     \n                                          \n  Depth Estim.        Data Association  \n  Triangulation       Kalman Filtering  \n  Pose Estim.         Multi-Object      \n  Point Clouds        Temporal Smoothing\n    \n                                \n         \n                                                           \n                           \n                        Scene Understanding      Action Planning     \n                                                                     \n                         Semantic Labels         Navigation Goals  \n                         Spatial Relations       Manipulation      \n                         Activity Recog.         Task Sequencing   \n                           \n```\n\n## 7. Isaac ROS Component Interface\n\n### Diagram: isaac-ros-components-interface.svg\n\nThis diagram shows how different Isaac ROS components interface with each other:\n\n```\n\n                        Isaac ROS Component Interface                    \n\n                                                                         \n                          \n    Isaac ROS        Isaac ROS        Isaac ROS                  \n    DetectNet    Visual       Bi3D                       \n                     SLAM                                        \n    Detections      Pose            3D Seg.                   \n    Classes         Map             Depths                    \n    Conf.           Trajectory      Instances                 \n                          \n                                                                     \n                                                                     \n                                                                     \n     \n                  Isaac ROS Common Interface                          \n                                                                    \n    Message Definitions (SRDI)                                     \n    Parameter Definitions                                          \n    Service Definitions                                            \n    Action Definitions                                             \n    Logger Interface                                               \n    Clock Interface                                                \n     \n                                                                     \n                                                                     \n                                                                     \n                          \n    Isaac ROS        Isaac ROS        Isaac ROS                  \n    Apriltag         AprilTag         AprilTag                   \n                     Fiducial          Pose                       \n    Tag Detections  Tag Pose        6DOF Pose                 \n    Tag IDs         Tag Info        Covariance                \n    Tag Images      Tag Map         Timestamp                 \n                          \n\n```\n\n## 8. Performance Pipeline\n\n### Diagram: performance-optimization-pipeline.svg\n\nThis diagram shows the performance optimization pipeline:\n\n```\n        \n Original Model   TensorRT Conversion  Optimized Model     \n                                                                   \n  PyTorch             Quantization            TensorRT Engine   \n  ONNX                Pruning                 INT8 Precision    \n  TensorFlow          Fusion                  GPU Optimized     \n        \n                                                          \n                                                          \n        \n Benchmarking     Profiling Tools      Optimization Report \n                                                                   \n  Inference           Nsight Systems          Bottleneck        \n  Latency             Nsight Graphics         Suggestions       \n  Throughput          Timeline View           Performance       \n        \n                                                          \n         \n                                 \n                       \n                        Deployment Pipeline \n                                            \n                         Containerization  \n                         CI/CD Integration \n                         Auto-scaling      \n                       \n```\n\n## 9. Error Handling and Recovery Pipeline\n\n### Diagram: error-handling-recovery-pipeline.svg\n\nThis diagram shows the error handling and recovery mechanisms:\n\n```\n        \n Normal Operation Anomaly Detection    Error Classification\n                                                                   \n  Perception          Data Quality            Sensor Failure  \n  Navigation          Performance             Algorithm Error \n  Control             Consistency             Communication   \n        \n                                                          \n                                                          \n        \n Fallback System  Recovery Behavior    Safe State          \n Activation           Selection                Transition          \n                                                                   \n  Reduced Mode        Spin Recovery           Stop Motion     \n  Manual Control      Backup Planning         Emergency Stop  \n  Safe Landing        Path Replanning         Return Home     \n        \n                                                          \n         \n                                 \n                       \n                        System Recovery     \n                                            \n                         State Restoration \n                         Calibration       \n                         Resume Operation  \n                       \n```\n\n## 10. Humanoid-Specific Perception Challenges\n\n### Diagram: humanoid-perception-challenges.svg\n\nThis diagram illustrates the unique challenges in humanoid perception:\n\n```\n\n                  Humanoid-Specific Perception Challenges                \n\n                                                                         \n           \n   Balance Maint.       Dynamic Perception       Multi-Modal    \n                                                 Integration    \n    COM Tracking        Moving Sensors          Vision       \n    Fall Prevent.       Motion Blur             Propriocep.  \n    Stability           Ego-motion              Touch        \n           \n                                                                     \n                                                                     \n           \n   Locomotion           Social Perception        Self-Modeling  \n                                                                  \n    Step Planning       Human Detection         Body Parts     \n    Gait Control        Intention Recog.        Configuration  \n    Terrain Adap.       Gesture Recog.          Occlusion      \n           \n\n```\n\n## Implementation Notes\n\nThese diagrams should be implemented as SVG files in the `static/img/` directory with the following characteristics:\n\n1. **Scalability**: Use vector graphics (SVG) for clarity at any size\n2. **Color Scheme**: Use consistent colors for different components:\n   - Blue: Data processing components\n   - Green: Input/output interfaces\n   - Orange: AI/ML components\n   - Red: Error/recovery components\n   - Gray: Supporting infrastructure\n\n3. **Clarity**: Use clear labels and arrows to show data flow\n4. **Consistency**: Maintain consistent styling across all diagrams\n5. **Interactivity**: Consider adding tooltips with more detailed information\n\n## Usage in Documentation\n\nThese diagrams should be referenced in the appropriate sections of the documentation:\n\n- Use overview diagrams in introduction sections\n- Include detailed pipeline diagrams in technical implementation sections\n- Add error handling diagrams in troubleshooting sections\n- Reference performance diagrams in optimization sections\n\nEach diagram should have:\n- A clear title and legend\n- Proper attribution to Isaac ROS/NVIDIA\n- Brief explanation of key components\n- Links to relevant documentation sections",
    "path": "module-3-ai-perception\\perception-navigation-pipeline-diagrams.md",
    "description": ""
  },
  "module-3-ai-perception\\practical-exercises-isaac-ai": {
    "title": "module-3-ai-perception\\practical-exercises-isaac-ai",
    "content": "# Practical Exercises with Isaac AI Components\n\nThis section provides hands-on exercises to reinforce the concepts learned about Isaac AI components for perception and navigation. These exercises will help you gain practical experience with NVIDIA Isaac technologies.\n\n## Exercise 1: Isaac ROS DetectNet Integration\n\n### Objective\nIntegrate Isaac ROS DetectNet with a humanoid robot to detect and classify objects in real-time.\n\n### Prerequisites\n- Isaac Sim installed and running\n- Isaac ROS packages installed\n- ROS 2 Humble\n- Compatible NVIDIA GPU with TensorRT\n\n### Steps\n\n#### 1. Set up the environment\n```bash\n# Source ROS 2 and Isaac ROS\nsource /opt/ros/humble/setup.bash\nsource ~/isaac_ros_ws/install/setup.bash\n\n# Launch Isaac Sim with a simple scene\n# (We'll use a script to automate this in the exercise)\n```\n\n#### 2. Create a custom DetectNet configuration\n```yaml\n# config/detectnet_config.yaml\n---\nlog_level: info\nmodel_name: \"ssd_mobilenet_v2_coco\"\ninput_topic: \"/camera/image_rect_color\"\noutput_topic: \"/detectnet/detections\"\nconfidence_threshold: 0.7\nmax_objects: 10\n```\n\n#### 3. Launch the DetectNet pipeline\n```bash\n# Create launch file: launch/detectnet_humanoid.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config = os.path.join(\n        get_package_share_directory('your_robot_perception'),\n        'config',\n        'detectnet_config.yaml'\n    )\n\n    detectnet_node = Node(\n        package='isaac_ros_detectnet',\n        executable='isaac_ros_detectnet',\n        name='detectnet',\n        parameters=[config],\n        remappings=[\n            ('/image_input', '/camera/image_rect_color'),\n            ('/detectnet/detections', '/detections')\n        ]\n    )\n\n    return LaunchDescription([\n        detectnet_node\n    ])\n```\n\n#### 4. Create a perception processing node\n```cpp\n// src/object_tracker_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectTrackerNode : public rclcpp::Node\n{\npublic:\n    ObjectTrackerNode() : Node(\"object_tracker_node\"), tf_buffer_(this->get_clock())\n    {\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            \"detections\", 10,\n            std::bind(&ObjectTrackerNode::detectionCallback, this, std::placeholders::_1)\n        );\n\n        object_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            \"tracked_object_position\", 10\n        );\n\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(tf_buffer_);\n    }\n\nprivate:\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr msg)\n    {\n        if (msg->detections.empty()) {\n            return;\n        }\n\n        // Process the highest confidence detection\n        auto best_detection = std::max_element(\n            msg->detections.begin(),\n            msg->detections.end(),\n            [](const auto& a, const auto& b) {\n                return a.confidence < b.confidence;\n            }\n        );\n\n        if (best_detection->confidence > 0.7) {\n            // Calculate 3D position from 2D detection\n            geometry_msgs::msg::PointStamped object_pos;\n            object_pos.header = msg->header;\n\n            // Convert 2D bounding box center to 3D position\n            // This requires depth information which we'll simulate\n            object_pos.point.x = (best_detection->bbox.center.x - 320.0) * 0.001; // Simplified\n            object_pos.point.y = (best_detection->bbox.center.y - 240.0) * 0.001; // Simplified\n            object_pos.point.z = 1.0; // Fixed depth for simulation\n\n            object_pub_->publish(object_pos);\n\n            RCLCPP_INFO(\n                this->get_logger(),\n                \"Detected %s with confidence %.2f at (%.2f, %.2f, %.2f)\",\n                best_detection->label.c_str(),\n                best_detection->confidence,\n                object_pos.point.x,\n                object_pos.point.y,\n                object_pos.point.z\n            );\n        }\n    }\n\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pub_;\n\n    tf2_ros::Buffer tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ObjectTrackerNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n```\n\n#### 5. Build and run the exercise\n```bash\n# Build the package\ncd ~/isaac_ros_ws\nsource install/setup.bash\ncolcon build --packages-select your_robot_perception\n\n# Run the nodes\nros2 launch your_robot_perception detectnet_humanoid.launch.py\n```\n\n### Expected Outcome\nA running perception pipeline that detects objects and publishes their 3D positions.\n\n## Exercise 2: Isaac ROS Visual SLAM Integration\n\n### Objective\nSet up and run Isaac ROS Visual SLAM to create a map of the environment and localize the robot.\n\n### Steps\n\n#### 1. Create SLAM launch file\n```python\n# launch/visual_slam.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='isaac_ros_visual_slam',\n        parameters=[{\n            'enable_occupancy_grid': True,\n            'enable_diagnostics': False,\n            'occupancy_grid_resolution': 0.05,\n            'frame_id': 'oak-d_frame',\n            'base_frame': 'base_link',\n            'odom_frame': 'odom',\n            'enable_slam_visualization': True,\n            'enable_landmarks_view': True,\n            'enable_observations_view': True,\n            'calibration_file': '/tmp/calibration.json',\n            'rescale_threshold': 2.0\n        }],\n        remappings=[\n            ('/stereo_camera/left/image', '/camera/left/image_rect_color'),\n            ('/stereo_camera/right/image', '/camera/right/image_rect_color'),\n            ('/stereo_camera/left/camera_info', '/camera/left/camera_info'),\n            ('/stereo_camera/right/camera_info', '/camera/right/camera_info'),\n            ('/visual_slam/imu', '/imu/data'),\n        ]\n    )\n\n    return LaunchDescription([\n        visual_slam_node\n    ])\n```\n\n#### 2. Create a SLAM evaluation node\n```cpp\n// src/slam_evaluator_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass SlamevaluatorNode : public rclcpp::Node\n{\npublic:\n    SlamevaluatorNode() : Node(\"slam_evaluator_node\")\n    {\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            \"visual_slam/odometry\", 10,\n            std::bind(&SlamevaluatorNode::odometryCallback, this, std::placeholders::_1)\n        );\n\n        pose_pub_ = this->create_publisher<geometry_msgs::msg::PoseStamped>(\n            \"estimated_pose\", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"SLAM Evaluator Node initialized\");\n    }\n\nprivate:\n    void odometryCallback(const nav_msgs::msg::Odometry::SharedPtr msg)\n    {\n        // Extract position and orientation from odometry\n        geometry_msgs::msg::PoseStamped pose_msg;\n        pose_msg.header = msg->header;\n        pose_msg.pose = msg->pose.pose;\n\n        // Publish the estimated pose\n        pose_pub_->publish(pose_msg);\n\n        // Calculate and log trajectory metrics\n        if (has_previous_pose_) {\n            double distance = calculateDistance(previous_pose_, pose_msg.pose);\n            total_distance_ += distance;\n\n            RCLCPP_INFO(\n                this->get_logger(),\n                \"SLAM Position: (%.2f, %.2f, %.2f), Total distance: %.2f\",\n                pose_msg.pose.position.x,\n                pose_msg.pose.position.y,\n                pose_msg.pose.position.z,\n                total_distance_\n            );\n        }\n\n        previous_pose_ = pose_msg.pose;\n        has_previous_pose_ = true;\n    }\n\n    double calculateDistance(const geometry_msgs::msg::Pose& p1, const geometry_msgs::msg::Pose& p2)\n    {\n        double dx = p1.position.x - p2.position.x;\n        double dy = p1.position.y - p2.position.y;\n        double dz = p1.position.z - p2.position.z;\n        return sqrt(dx*dx + dy*dy + dz*dz);\n    }\n\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_pub_;\n\n    geometry_msgs::msg::Pose previous_pose_;\n    bool has_previous_pose_ = false;\n    double total_distance_ = 0.0;\n};\n```\n\n#### 3. Run the SLAM exercise\n```bash\n# Launch SLAM\nros2 launch your_robot_perception visual_slam.launch.py\n\n# Visualize results\nros2 run rviz2 rviz2 -d /path/to/slam_config.rviz\n```\n\n### Expected Outcome\nA real-time map of the environment with the robot's estimated position and trajectory.\n\n## Exercise 3: Isaac ROS Bi3D Integration\n\n### Objective\nUse Isaac ROS Bi3D for 3D object detection and segmentation.\n\n### Steps\n\n#### 1. Create Bi3D launch configuration\n```python\n# launch/bi3d_segmentation.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    bi3d_node = Node(\n        package='isaac_ros_bi3d',\n        executable='isaac_ros_bi3d',\n        parameters=[{\n            'engine_file_path': '/path/to/bi3d_plan_engine.plan',\n            'input_tensor_names': ['input_tensor'],\n            'output_tensor_names': ['output_tensor'],\n            'network_input_height': 512,\n            'network_input_width': 512,\n            'num_classes': 256,\n            'mask_threshold': 0.8\n        }],\n        remappings=[\n            ('/image', '/camera/image_rect_color'),\n            ('/segmentation', '/bi3d_segmentation')\n        ]\n    )\n\n    return LaunchDescription([\n        bi3d_node\n    ])\n```\n\n#### 2. Create a 3D object extraction node\n```cpp\n// src/bi3d_processor_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass Bi3DProcessorNode : public rclcpp::Node\n{\npublic:\n    Bi3DProcessorNode() : Node(\"bi3d_processor_node\")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/image_rect_color\", 10,\n            std::bind(&Bi3DProcessorNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        segmentation_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"bi3d_segmentation\", 10,\n            std::bind(&Bi3DProcessorNode::segmentationCallback, this, std::placeholders::_1)\n        );\n\n        object_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            \"extracted_3d_objects\", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Bi3D Processor Node initialized\");\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Store image for processing with segmentation\n        latest_image_ = image_msg;\n    }\n\n    void segmentationCallback(const sensor_msgs::msg::Image::SharedPtr seg_msg)\n    {\n        if (!latest_image_) {\n            return;  // Wait for image\n        }\n\n        try {\n            // Convert segmentation image to OpenCV\n            cv_bridge::CvImagePtr seg_cv_ptr = cv_bridge::toCvCopy(seg_msg, sensor_msgs::image_encodings::TYPE_32SC1);\n\n            // Process segmentation to extract 3D objects\n            std::vector<DetectedObject> objects = extractObjects(seg_cv_ptr->image);\n\n            // Publish each detected object\n            for (const auto& obj : objects) {\n                geometry_msgs::msg::PointStamped obj_pos;\n                obj_pos.header = seg_msg->header;\n                obj_pos.point = obj.centroid;\n\n                object_pub_->publish(obj_pos);\n\n                RCLCPP_INFO(\n                    this->get_logger(),\n                    \"3D Object: %s at (%.2f, %.2f, %.2f), pixels: %d\",\n                    obj.label.c_str(),\n                    obj.centroid.x,\n                    obj.centroid.y,\n                    obj.centroid.z,\n                    obj.pixel_count\n                );\n            }\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n        }\n    }\n\n    struct DetectedObject {\n        std::string label;\n        geometry_msgs::msg::Point centroid;\n        int pixel_count;\n        cv::Rect bounding_box;\n    };\n\n    std::vector<DetectedObject> extractObjects(const cv::Mat& segmentation_mask)\n    {\n        std::vector<DetectedObject> objects;\n\n        // Find contours for each class in the segmentation\n        for (int class_id = 1; class_id < 256; ++class_id) {  // Skip background (0)\n            cv::Mat class_mask = (segmentation_mask == class_id);\n\n            std::vector<std::vector<cv::Point>> contours;\n            cv::findContours(class_mask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);\n\n            for (const auto& contour : contours) {\n                if (contour.size() < 50) continue;  // Filter small regions\n\n                DetectedObject obj;\n                obj.pixel_count = contour.size();\n\n                // Calculate centroid\n                cv::Moments moments = cv::moments(contour);\n                if (moments.m00 != 0) {\n                    int cx = static_cast<int>(moments.m10 / moments.m00);\n                    int cy = static_cast<int>(moments.m01 / moments.m00);\n\n                    // Estimate 3D position (simplified)\n                    obj.centroid.x = (cx - 320.0) * 0.002;  // Approximate conversion\n                    obj.centroid.y = (cy - 240.0) * 0.002;\n                    obj.centroid.z = 1.0;  // Estimated depth\n\n                    obj.bounding_box = cv::boundingRect(contour);\n\n                    // Assign label based on class ID (in real system, use a mapping)\n                    obj.label = \"object_\" + std::to_string(class_id);\n\n                    objects.push_back(obj);\n                }\n            }\n        }\n\n        return objects;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr segmentation_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pub_;\n\n    sensor_msgs::msg::Image::SharedPtr latest_image_;\n};\n```\n\n### Expected Outcome\nA system that segments the scene into 3D objects and publishes their positions.\n\n## Exercise 4: Isaac Sim Perception Pipeline\n\n### Objective\nCreate a complete perception pipeline in Isaac Sim that integrates multiple Isaac ROS components.\n\n### Steps\n\n#### 1. Create Isaac Sim perception scene\n```python\n# scripts/setup_perception_scene.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass PerceptionSceneSetup:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_scene()\n\n    def setup_scene(self):\n        # Get assets root path\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets\")\n            return\n\n        # Add a robot with perception sensors\n        robot_path = assets_root_path + \"/Isaac/Robots/Carter/carter_navigate.usd\"\n        add_reference_to_stage(usd_path=robot_path, prim_path=\"/World/Carter\")\n\n        # Add a camera sensor\n        self.camera = Camera(\n            prim_path=\"/World/Carter/chassis/camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add a LIDAR sensor\n        self.lidar = RotatingLidarPhysX(\n            prim_path=\"/World/Carter/chassis/lidar\",\n            translation=np.array([0.0, 0.0, 0.25]),\n            config=\"Carter\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add some objects to detect\n        cube_path = assets_root_path + \"/Isaac/Props/Blocks/block_instanceable.usd\"\n        add_reference_to_stage(usd_path=cube_path, prim_path=\"/World/Cube1\")\n        from pxr import Gf\n        from omni.isaac.core.utils.prims import set_targets\n        from omni.isaac.core.utils.transformations import quat_from_euler_angles\n\n        # Position the cube in front of the robot\n        cube_prim = self.world.scene.add_static_object(\n            prim_path=\"/World/Cube1\",\n            usd_path=cube_path,\n            position=[2.0, 0.0, 0.5],\n            orientation=quat_from_euler_angles([0, 0, 0])\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_perception_pipeline(self):\n        \"\"\"Run the perception pipeline\"\"\"\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get sensor data\n            camera_data = self.camera.get_rgb()\n            lidar_data = self.lidar.get_linear_depth_data()\n\n            # Process perception data (placeholder)\n            self.process_perception_data(camera_data, lidar_data)\n\n    def process_perception_data(self, camera_data, lidar_data):\n        \"\"\"Process perception data\"\"\"\n        print(f\"Camera data shape: {camera_data.shape if hasattr(camera_data, 'shape') else 'N/A'}\")\n        print(f\"LIDAR data points: {len(lidar_data) if hasattr(lidar_data, '__len__') else 'N/A'}\")\n\n# Run the scene setup\nif __name__ == \"__main__\":\n    scene_setup = PerceptionSceneSetup()\n    scene_setup.run_perception_pipeline()\n```\n\n#### 2. Connect Isaac Sim to ROS\n```python\n# scripts/isaac_sim_ros_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacSimRosBridge(Node):\n    def __init__(self):\n        super().__init__('isaac_sim_ros_bridge')\n\n        # ROS publishers\n        self.image_pub = self.create_publisher(Image, '/camera/image_raw', 10)\n        self.scan_pub = self.create_publisher(LaserScan, '/scan', 10)\n\n        # ROS subscriber for robot commands\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, '/cmd_vel', self.cmd_vel_callback, 10\n        )\n\n        self.bridge = CvBridge()\n\n        # Timer to publish sensor data\n        self.timer = self.create_timer(0.1, self.publish_sensor_data)\n\n        # Store robot commands\n        self.last_cmd_vel = None\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Store command velocity for Isaac Sim\"\"\"\n        self.last_cmd_vel = msg\n        # In a real implementation, this would send commands to Isaac Sim\n\n    def publish_sensor_data(self):\n        \"\"\"Publish sensor data from Isaac Sim\"\"\"\n        # This would normally connect to Isaac Sim\n        # For this exercise, we'll simulate data\n\n        # Publish a simulated image\n        sim_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        image_msg = self.bridge.cv2_to_imgmsg(sim_image, encoding=\"bgr8\")\n        image_msg.header.stamp = self.get_clock().now().to_msg()\n        image_msg.header.frame_id = \"camera_link\"\n        self.image_pub.publish(image_msg)\n\n        # Publish a simulated scan\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = \"lidar_link\"\n        scan_msg.angle_min = -np.pi / 2\n        scan_msg.angle_max = np.pi / 2\n        scan_msg.angle_increment = np.pi / 180  # 1 degree\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 10.0\n        scan_msg.ranges = [5.0] * 180  # Simulated ranges\n\n        self.scan_pub.publish(scan_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = IsaacSimRosBridge()\n\n    try:\n        rclpy.spin(bridge)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n### Expected Outcome\nA complete perception pipeline running in Isaac Sim that connects to ROS and processes data from multiple sensors.\n\n## Exercise 5: Integrated Perception and Navigation\n\n### Objective\nCombine perception and navigation systems to create a complete autonomous robot behavior.\n\n### Steps\n\n#### 1. Create integrated launch file\n```python\n# launch/integrated_perception_navigation.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Include perception pipeline\n    perception_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                get_package_share_directory('your_robot_perception'),\n                'launch',\n                'detectnet_humanoid.launch.py'\n            ])\n        ])\n    )\n\n    # Include navigation stack\n    navigation_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                get_package_share_directory('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': 'true'\n        }.items()\n    )\n\n    # Object avoidance node\n    object_avoidance_node = Node(\n        package='your_robot_perception',\n        executable='object_avoidance_node',\n        name='object_avoidance',\n        parameters=[\n            {'safety_distance': 0.5},\n            {'avoidance_strength': 1.0}\n        ],\n        remappings=[\n            ('/detected_objects', '/tracked_object_position'),\n            ('/cmd_vel_safe', '/cmd_vel_filtered'),\n            ('/cmd_vel_in', '/cmd_vel')\n        ]\n    )\n\n    return LaunchDescription([\n        perception_launch,\n        navigation_launch,\n        object_avoidance_node\n    ])\n```\n\n#### 2. Create object avoidance controller\n```cpp\n// src/object_avoidance_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectAvoidanceNode : public rclcpp::Node\n{\npublic:\n    ObjectAvoidanceNode() : Node(\"object_avoidance_node\")\n    {\n        cmd_vel_in_sub_ = this->create_subscription<geometry_msgs::msg::Twist>(\n            \"cmd_vel_in\", 10,\n            std::bind(&ObjectAvoidanceNode::cmdVelCallback, this, std::placeholders::_1)\n        );\n\n        detected_objects_sub_ = this->create_subscription<geometry_msgs::msg::PointStamped>(\n            \"detected_objects\", 10,\n            std::bind(&ObjectAvoidanceNode::objectCallback, this, std::placeholders::_1)\n        );\n\n        cmd_vel_out_pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\n            \"cmd_vel_safe\", 10\n        );\n\n        // Get parameters\n        safety_distance_ = this->declare_parameter(\"safety_distance\", 0.5);\n        avoidance_strength_ = this->declare_parameter(\"avoidance_strength\", 1.0);\n\n        RCLCPP_INFO(this->get_logger(), \"Object Avoidance Node initialized\");\n    }\n\nprivate:\n    void cmdVelCallback(const geometry_msgs::msg::Twist::SharedPtr cmd_msg)\n    {\n        latest_cmd_vel_ = *cmd_msg;\n        applyObjectAvoidance();\n    }\n\n    void objectCallback(const geometry_msgs::msg::PointStamped::SharedPtr obj_msg)\n    {\n        // Store object position for avoidance calculation\n        detected_objects_.push_back(*obj_msg);\n\n        // Keep only recent detections (last 1 second)\n        auto current_time = this->now();\n        detected_objects_.erase(\n            std::remove_if(detected_objects_.begin(), detected_objects_.end(),\n                [current_time, this](const geometry_msgs::msg::PointStamped& obj) {\n                    return (current_time - obj.header.stamp).seconds() > 1.0;\n                }),\n            detected_objects_.end()\n        );\n    }\n\n    void applyObjectAvoidance()\n    {\n        if (!latest_cmd_vel_) {\n            return;\n        }\n\n        geometry_msgs::msg::Twist safe_cmd = *latest_cmd_vel_;\n\n        // Check for nearby objects that require avoidance\n        for (const auto& obj : detected_objects_) {\n            // Calculate distance to object in robot's frame\n            double dist_to_obj = sqrt(\n                pow(obj.point.x, 2) +\n                pow(obj.point.y, 2) +\n                pow(obj.point.z, 2)\n            );\n\n            if (dist_to_obj < safety_distance_) {\n                // Calculate avoidance vector\n                double avoidance_x = -obj.point.x * avoidance_strength_ / dist_to_obj;\n                double avoidance_y = -obj.point.y * avoidance_strength_ / dist_to_obj;\n\n                // Apply avoidance to commanded velocity\n                safe_cmd.linear.x += avoidance_x;\n                safe_cmd.linear.y += avoidance_y;\n\n                // Add angular component to turn away from object\n                double angle_to_obj = atan2(obj.point.y, obj.point.x);\n                safe_cmd.angular.z -= angle_to_obj * avoidance_strength_ * 0.5;\n            }\n        }\n\n        // Apply velocity limits\n        safe_cmd.linear.x = std::clamp(safe_cmd.linear.x, -0.5, 0.5);\n        safe_cmd.linear.y = std::clamp(safe_cmd.linear.y, -0.2, 0.2);\n        safe_cmd.angular.z = std::clamp(safe_cmd.angular.z, -0.5, 0.5);\n\n        cmd_vel_out_pub_->publish(safe_cmd);\n    }\n\n    rclcpp::Subscription<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_in_sub_;\n    rclcpp::Subscription<geometry_msgs::msg::PointStamped>::SharedPtr detected_objects_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_out_pub_;\n\n    std::optional<geometry_msgs::msg::Twist> latest_cmd_vel_;\n    std::vector<geometry_msgs::msg::PointStamped> detected_objects_;\n\n    double safety_distance_;\n    double avoidance_strength_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ObjectAvoidanceNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n```\n\n#### 3. Create a mission planner node\n```cpp\n// src/mission_planner_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav2_msgs/action/navigate_to_pose.hpp>\n#include <rclcpp_action/rclcpp_action.hpp>\n\nclass MissionPlannerNode : public rclcpp::Node\n{\npublic:\n    MissionPlannerNode() : Node(\"mission_planner_node\")\n    {\n        nav_client_ = rclcpp_action::create_client<nav2_msgs::action::NavigateToPose>(\n            this, \"navigate_to_pose\"\n        );\n\n        // Define a simple mission: visit multiple waypoints\n        waypoints_ = {\n            createPose(1.0, 1.0, 0.0),\n            createPose(2.0, 0.0, 1.57),\n            createPose(1.0, -1.0, 3.14),\n            createPose(0.0, 0.0, 0.0)  // Return to start\n        };\n\n        mission_timer_ = this->create_wall_timer(\n            std::chrono::seconds(5),\n            std::bind(&MissionPlannerNode::executeNextWaypoint, this)\n        );\n\n        current_waypoint_ = 0;\n        mission_active_ = false;\n\n        RCLCPP_INFO(this->get_logger(), \"Mission Planner Node initialized\");\n    }\n\nprivate:\n    geometry_msgs::msg::PoseStamped createPose(double x, double y, double theta)\n    {\n        geometry_msgs::msg::PoseStamped pose;\n        pose.header.frame_id = \"map\";\n        pose.pose.position.x = x;\n        pose.pose.position.y = y;\n        pose.pose.position.z = 0.0;\n\n        tf2::Quaternion q;\n        q.setRPY(0, 0, theta);\n        pose.pose.orientation = tf2::toMsg(q);\n\n        return pose;\n    }\n\n    void executeNextWaypoint()\n    {\n        if (mission_active_ || current_waypoint_ >= waypoints_.size()) {\n            return;  // Wait for current navigation to complete\n        }\n\n        if (!nav_client_->wait_for_action_server(std::chrono::seconds(5))) {\n            RCLCPP_ERROR(this->get_logger(), \"Navigation action server not available\");\n            return;\n        }\n\n        auto goal = nav2_msgs::action::NavigateToPose::Goal();\n        goal.pose = waypoints_[current_waypoint_];\n\n        auto send_goal_options = rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SendGoalOptions();\n        send_goal_options.result_callback =\n            [this](const rclcpp_action::ClientGoalHandle<nav2_msgs::action::NavigateToPose>::WrappedResult& result) {\n                if (result.code == rclcpp_action::ResultCode::SUCCEEDED) {\n                    RCLCPP_INFO(this->get_logger(), \"Waypoint %d reached!\", current_waypoint_);\n                    current_waypoint_++;\n                    mission_active_ = false;\n\n                    if (current_waypoint_ >= waypoints_.size()) {\n                        RCLCPP_INFO(this->get_logger(), \"Mission completed!\");\n                    }\n                } else {\n                    RCLCPP_ERROR(this->get_logger(), \"Failed to reach waypoint %d\", current_waypoint_);\n                    mission_active_ = false;\n                }\n            };\n\n        mission_active_ = true;\n        nav_client_->async_send_goal(goal, send_goal_options);\n    }\n\n    rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SharedPtr nav_client_;\n    rclcpp::TimerBase::SharedPtr mission_timer_;\n\n    std::vector<geometry_msgs::msg::PoseStamped> waypoints_;\n    size_t current_waypoint_;\n    bool mission_active_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<MissionPlannerNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n```\n\n### Expected Outcome\nA complete system that performs perception, navigation, and mission planning with object avoidance capabilities.\n\n## Exercise 6: Performance Evaluation and Optimization\n\n### Objective\nEvaluate and optimize the performance of your Isaac AI perception system.\n\n### Steps\n\n#### 1. Create performance monitoring node\n```cpp\n// src/performance_monitor_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <std_msgs/msg/string.hpp>\n#include <chrono>\n\nclass PerformanceMonitorNode : public rclcpp::Node\n{\npublic:\n    PerformanceMonitorNode() : Node(\"performance_monitor_node\")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/image_raw\", 10,\n            std::bind(&PerformanceMonitorNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        stats_timer_ = this->create_wall_timer(\n            std::chrono::seconds(1),\n            std::bind(&PerformanceMonitorNode::printStats, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Performance Monitor Node initialized\");\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        auto current_time = std::chrono::steady_clock::now();\n\n        // Calculate frame rate\n        if (last_frame_time_.has_value()) {\n            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(\n                current_time - last_frame_time_.value()\n            ).count();\n            frame_times_.push_back(duration);\n        }\n\n        last_frame_time_ = current_time;\n\n        // Calculate bandwidth\n        size_t msg_size = msg->data.size();\n        total_bytes_processed_ += msg_size;\n        messages_processed_++;\n    }\n\n    void printStats()\n    {\n        if (frame_times_.empty()) return;\n\n        // Calculate average frame time\n        double avg_frame_time = 0.0;\n        for (auto time : frame_times_) {\n            avg_frame_time += time;\n        }\n        avg_frame_time /= frame_times_.size();\n\n        // Calculate frame rate\n        double avg_fps = 1e6 / avg_frame_time;  // microseconds to seconds\n\n        // Calculate bandwidth\n        double bandwidth = total_bytes_processed_ / 1e6;  // MB\n        double avg_msg_size = (messages_processed_ > 0) ?\n            static_cast<double>(total_bytes_processed_) / messages_processed_ : 0;\n\n        RCLCPP_INFO(\n            this->get_logger(),\n            \"Performance Stats - FPS: %.2f, Avg Frame Time: %.2f ms, \"\n            \"Avg Msg Size: %.2f KB, Total Processed: %.2f MB\",\n            avg_fps, avg_frame_time / 1000.0, avg_msg_size / 1024.0, bandwidth\n        );\n\n        // Clear for next interval\n        frame_times_.clear();\n        total_bytes_processed_ = 0;\n        messages_processed_ = 0;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::TimerBase::SharedPtr stats_timer_;\n\n    std::optional<std::chrono::steady_clock::time_point> last_frame_time_;\n    std::vector<long> frame_times_;  // in microseconds\n    size_t total_bytes_processed_ = 0;\n    size_t messages_processed_ = 0;\n};\n```\n\n#### 2. Create optimization report\n```cpp\n// scripts/generate_performance_report.py\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nfrom datetime import datetime\n\nclass PerformanceAnalyzer:\n    def __init__(self):\n        self.metrics = []\n\n    def load_metrics(self, metrics_file):\n        \"\"\"Load performance metrics from file\"\"\"\n        with open(metrics_file, 'r') as f:\n            self.metrics = json.load(f)\n\n    def generate_report(self):\n        \"\"\"Generate performance analysis report\"\"\"\n        if not self.metrics:\n            print(\"No metrics loaded\")\n            return\n\n        df = pd.DataFrame(self.metrics)\n\n        # Create visualizations\n        self.create_visualizations(df)\n\n        # Generate recommendations\n        self.generate_recommendations(df)\n\n        # Export detailed report\n        self.export_report(df)\n\n    def create_visualizations(self, df):\n        \"\"\"Create performance visualization plots\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Frame rate over time\n        axes[0, 0].plot(df['timestamp'], df['fps'])\n        axes[0, 0].set_title('Frame Rate Over Time')\n        axes[0, 0].set_xlabel('Time')\n        axes[0, 0].set_ylabel('FPS')\n\n        # Processing time distribution\n        axes[0, 1].hist(df['avg_frame_time_ms'], bins=30)\n        axes[0, 1].set_title('Frame Processing Time Distribution')\n        axes[0, 1].set_xlabel('Processing Time (ms)')\n        axes[0, 1].set_ylabel('Frequency')\n\n        # Bandwidth usage\n        axes[1, 0].plot(df['timestamp'], df['bandwidth_mb'])\n        axes[1, 0].set_title('Bandwidth Usage Over Time')\n        axes[1, 0].set_xlabel('Time')\n        axes[1, 0].set_ylabel('Bandwidth (MB)')\n\n        # Correlation heatmap\n        correlation_data = df[['fps', 'avg_frame_time_ms', 'bandwidth_mb']].corr()\n        im = axes[1, 1].imshow(correlation_data, cmap='coolwarm', aspect='auto')\n        axes[1, 1].set_xticks(range(len(correlation_data.columns)))\n        axes[1, 1].set_yticks(range(len(correlation_data.columns)))\n        axes[1, 1].set_xticklabels(correlation_data.columns, rotation=45)\n        axes[1, 1].set_yticklabels(correlation_data.columns)\n        axes[1, 1].set_title('Performance Metric Correlations')\n\n        # Add colorbar\n        plt.colorbar(im, ax=axes[1, 1])\n\n        plt.tight_layout()\n        plt.savefig('performance_analysis.png')\n        plt.show()\n\n    def generate_recommendations(self, df):\n        \"\"\"Generate optimization recommendations\"\"\"\n        avg_fps = df['fps'].mean()\n        avg_time = df['avg_frame_time_ms'].mean()\n        avg_bandwidth = df['bandwidth_mb'].mean()\n\n        print(\"PERFORMANCE ANALYSIS REPORT\")\n        print(\"=\" * 50)\n        print(f\"Average Frame Rate: {avg_fps:.2f} FPS\")\n        print(f\"Average Processing Time: {avg_time:.2f} ms\")\n        print(f\"Average Bandwidth: {avg_bandwidth:.2f} MB\")\n        print()\n\n        recommendations = []\n\n        if avg_fps < 15:\n            recommendations.append(\"LOW FPS: Consider reducing image resolution or using lighter models\")\n        if avg_time > 50:\n            recommendations.append(\"HIGH PROCESSING TIME: Optimize algorithms or use TensorRT\")\n        if avg_bandwidth > 100:\n            recommendations.append(\"HIGH BANDWIDTH: Compress images or reduce frequency\")\n\n        if not recommendations:\n            recommendations.append(\"Performance looks good! Consider profiling specific bottlenecks.\")\n\n        print(\"OPTIMIZATION RECOMMENDATIONS:\")\n        for rec in recommendations:\n            print(f\"- {rec}\")\n        print()\n\n    def export_report(self, df):\n        \"\"\"Export detailed report\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        report_filename = f\"performance_report_{timestamp}.txt\"\n\n        with open(report_filename, 'w') as f:\n            f.write(\"Isaac AI Performance Report\\n\")\n            f.write(f\"Generated: {datetime.now()}\\n\\n\")\n            f.write(f\"Summary Statistics:\\n{df.describe()}\\n\\n\")\n\n        print(f\"Detailed report exported to {report_filename}\")\n\nif __name__ == \"__main__\":\n    analyzer = PerformanceAnalyzer()\n    # analyzer.load_metrics('performance_metrics.json')  # Load your metrics\n    # analyzer.generate_report()\n```\n\n### Expected Outcome\nA complete system for monitoring, analyzing, and optimizing Isaac AI component performance with actionable insights.\n\n## Troubleshooting Common Issues\n\n### 1. TensorRT Optimization Issues\n**Problem**: Models not running at expected speeds\n**Solutions**:\n- Verify TensorRT engine files are properly built\n- Check GPU memory availability\n- Use appropriate batch sizes\n- Profile with NSight Systems\n\n### 2. Memory Issues\n**Problem**: GPU memory exhaustion\n**Solutions**:\n- Reduce model input resolution\n- Use model quantization\n- Implement memory pooling\n- Monitor memory usage during runtime\n\n### 3. Synchronization Issues\n**Problem**: Sensor data timing problems\n**Solutions**:\n- Use message filters for time synchronization\n- Implement proper buffer sizes\n- Use reliable QoS policies\n- Add timestamps to messages\n\n## Best Practices\n\n### 1. Modular Design\n- Separate perception, planning, and control components\n- Use standard ROS interfaces\n- Implement proper error handling\n- Design for easy testing and debugging\n\n### 2. Performance Optimization\n- Use TensorRT for inference acceleration\n- Implement efficient data pipelines\n- Use multi-threading where appropriate\n- Profile regularly and optimize bottlenecks\n\n### 3. Robustness\n- Handle sensor failures gracefully\n- Implement fallback behaviors\n- Validate inputs and outputs\n- Test in diverse conditions\n\n## Exercise Completion Checklist\n\nAfter completing these exercises, you should be able to:\n\n- [ ] Set up Isaac ROS perception components\n- [ ] Integrate multiple Isaac AI modules\n- [ ] Process and interpret perception data\n- [ ] Implement perception-driven navigation\n- [ ] Evaluate and optimize performance\n- [ ] Troubleshoot common issues\n- [ ] Design robust perception systems\n\nSuccessfully completing these exercises will provide you with hands-on experience with NVIDIA Isaac AI components and prepare you for implementing perception and navigation systems on real humanoid robots.",
    "path": "module-3-ai-perception\\practical-exercises-isaac-ai.md",
    "description": ""
  },
  "module-3-ai-perception\\vslam-navigation": {
    "title": "module-3-ai-perception\\vslam-navigation",
    "content": "# VSLAM and Navigation\n\nVisual Simultaneous Localization and Mapping (VSLAM) is a critical technology for autonomous robots, enabling them to understand their environment and navigate without prior knowledge. This section covers VSLAM concepts and navigation techniques for humanoid robots using Isaac ROS.\n\n## Introduction to VSLAM\n\nVSLAM combines computer vision and sensor data to:\n- **Localize** the robot in its environment\n- **Map** the environment in real-time\n- **Navigate** safely through the mapped space\n\n### Key Components of VSLAM\n- **Feature Detection**: Identify distinctive points in images\n- **Feature Matching**: Match features between frames\n- **Pose Estimation**: Calculate robot position and orientation\n- **Map Building**: Create and update environmental map\n- **Loop Closure**: Recognize previously visited locations\n\n## Visual SLAM Approaches\n\n### 1. Feature-Based VSLAM\nRelies on detecting and tracking distinctive features in the environment.\n\n### 2. Direct VSLAM\nUses pixel intensities directly rather than features.\n\n### 3. Semi-Direct VSLAM (SVO)\nCombines feature-based tracking with direct methods.\n\n## Popular VSLAM Systems\n\n### ORB-SLAM\n- **Features**: Real-time operation, loop closure, relocalization\n- **Strengths**: Robust, well-tested, handles monocular/stereo/RGB-D\n- **Weaknesses**: Requires texture-rich environments\n\n### LSD-SLAM\n- **Features**: Dense reconstruction, direct method\n- **Strengths**: Works in low-texture environments\n- **Weaknesses**: Computationally intensive\n\n### DSO (Direct Sparse Odometry)\n- **Features**: Direct optimization, photometric calibration\n- **Strengths**: Accurate, handles exposure changes\n- **Weaknesses**: Requires good initialization\n\n## Isaac ROS VSLAM Integration\n\n### Isaac ROS Visual SLAM Package\nThe Isaac ROS Visual SLAM package provides optimized VSLAM capabilities:\n\n```yaml\n# Example launch configuration\nvisual_slam_node:\n  ros__parameters:\n    enable_occupancy_grid: true\n    enable_diagnostics: false\n    occupancy_grid_resolution: 0.05\n    frame_id: \"oak-d_frame\"\n    base_frame: \"base_link\"\n    odom_frame: \"odom\"\n    enable_slam_visualization: true\n    enable_landmarks_view: true\n    enable_observations_view: true\n    calibration_file: \"/tmp/calibration.json\"\n    rescale_threshold: 2.0\n```\n\n### Integration with ROS 2\n\n```xml\n<!-- Launch file for VSLAM system -->\n<launch>\n  <!-- Camera driver -->\n  <node pkg=\"camera_driver\" exec=\"camera_node\" name=\"camera\">\n    <param name=\"camera_info_url\" value=\"file://$(find-pkg-share robot_description)/config/camera.yaml\"/>\n  </node>\n\n  <!-- Isaac ROS Visual SLAM -->\n  <node pkg=\"isaac_ros_visual_slam\" exec=\"isaac_ros_visual_slam\" name=\"visual_slam\">\n    <param name=\"enable_occupancy_grid\" value=\"true\"/>\n    <param name=\"occupancy_grid_resolution\" value=\"0.05\"/>\n    <param name=\"frame_id\" value=\"camera_link\"/>\n    <param name=\"base_frame\" value=\"base_link\"/>\n  </node>\n\n  <!-- Robot state publisher -->\n  <node pkg=\"robot_state_publisher\" exec=\"robot_state_publisher\" name=\"robot_state_publisher\">\n    <param name=\"robot_description\" value=\"$(var robot_description)\"/>\n  </node>\n</launch>\n```\n\n## Navigation Stack Integration\n\n### Nav2 Architecture with VSLAM\n\nNav2 (Navigation 2) is the ROS 2 navigation stack that works with VSLAM:\n\n```\nNav2 Stack\n Global Planner (NavFn, A*, etc.)\n Local Planner (DWA, TEB, etc.)\n Controller (PID, MPC, etc.)\n Recovery Behaviors\n Costmap (Static & Local)\n Behavior Trees (for task orchestration)\n```\n\n### Navigation Configuration with VSLAM\n\n```yaml\n# nav2_params.yaml with VSLAM integration\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: \"base_link\"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: \"map\"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: \"likelihood_field\"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: \"odom\"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: \"nav2_amcl::DifferentialMotionModel\"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.2\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_through_poses_bt_xml: nav2_bt_navigator/navigate_through_poses_w_replanning_and_recovery.xml\n    default_nav_to_pose_bt_xml: nav2_bt_navigator/navigate_to_pose_w_replanning_and_recovery.xml\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_compute_path_through_poses_action_bt_node\n    - nav2_smooth_path_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_assisted_teleop_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_drive_on_heading_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_globally_consistent_condition_bt_node\n    - nav2_is_path_valid_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_truncate_path_local_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n    - nav2_controller_cancel_bt_node\n    - nav2_path_longer_on_approach_bt_node\n    - nav2_wait_cancel_bt_node\n    - nav2_spin_cancel_bt_node\n    - nav2_back_up_cancel_bt_node\n    - nav2_assisted_teleop_cancel_bt_node\n    - nav2_drive_on_heading_cancel_bt_node\n```\n\n### Navigation with VSLAM\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav2_msgs/action/navigate_to_pose.hpp>\n#include <rclcpp_action/rclcpp_action.hpp>\n\nclass NavigationWithVSLAM : public rclcpp::Node\n{\npublic:\n    NavigationWithVSLAM() : Node(\"nav_with_vslam\")\n    {\n        // Create action client for navigation\n        nav_client_ = rclcpp_action::create_client<nav2_msgs::action::NavigateToPose>(\n            this, \"navigate_to_pose\"\n        );\n\n        // Subscribe to VSLAM pose\n        vslam_sub_ = this->create_subscription<geometry_msgs::msg::PoseStamped>(\n            \"visual_slam/pose\", 10,\n            std::bind(&NavigationWithVSLAM::vslamPoseCallback, this, std::placeholders::_1)\n        );\n    }\n\n    void navigateToGoal(double x, double y, double theta)\n    {\n        // Wait for action server\n        if (!nav_client_->wait_for_action_server(std::chrono::seconds(5))) {\n            RCLCPP_ERROR(this->get_logger(), \"Navigation action server not available\");\n            return;\n        }\n\n        // Create goal\n        auto goal = nav2_msgs::action::NavigateToPose::Goal();\n        goal.pose.header.frame_id = \"map\";\n        goal.pose.header.stamp = this->now();\n        goal.pose.pose.position.x = x;\n        goal.pose.pose.position.y = y;\n        goal.pose.pose.position.z = 0.0;\n\n        // Convert theta to quaternion\n        double s = sin(theta/2);\n        double c = cos(theta/2);\n        goal.pose.pose.orientation.x = 0.0;\n        goal.pose.pose.orientation.y = 0.0;\n        goal.pose.pose.orientation.z = s;\n        goal.pose.pose.orientation.w = c;\n\n        // Send goal\n        auto send_goal_options = rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SendGoalOptions();\n        send_goal_options.result_callback =\n            [this](const rclcpp_action::ClientGoalHandle<nav2_msgs::action::NavigateToPose>::WrappedResult& result) {\n                if (result.code == rclcpp_action::ResultCode::SUCCEEDED) {\n                    RCLCPP_INFO(this->get_logger(), \"Navigation succeeded!\");\n                } else {\n                    RCLCPP_ERROR(this->get_logger(), \"Navigation failed!\");\n                }\n            };\n\n        nav_client_->async_send_goal(goal, send_goal_options);\n    }\n\nprivate:\n    void vslamPoseCallback(const geometry_msgs::msg::PoseStamped::SharedPtr msg)\n    {\n        // Update robot's pose in the navigation system\n        current_pose_ = *msg;\n\n        // This pose can be used for localization in the navigation stack\n        RCLCPP_DEBUG(this->get_logger(),\n            \"Received VSLAM pose: (%.2f, %.2f)\",\n            msg->pose.position.x, msg->pose.position.y);\n    }\n\n    rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SharedPtr nav_client_;\n    rclcpp::Subscription<geometry_msgs::msg::PoseStamped>::SharedPtr vslam_sub_;\n    geometry_msgs::msg::PoseStamped current_pose_;\n};\n```\n\n## Isaac ROS Integration\n\n### Isaac ROS VSLAM Packages\n\nNVIDIA Isaac ROS provides optimized VSLAM implementations:\n\n```yaml\n# Isaac ROS VSLAM launch\nlaunch:\n  - package: \"isaac_ros_visual_slam\"\n    executable: \"isaac_ros_visual_slam\"\n    name: \"visual_slam\"\n    parameters:\n      - \"enable_occupancy_grid\": True\n      - \"occupancy_grid_resolution\": 0.05\n      - \"frame_id\": \"camera_link\"\n      - \"base_frame\": \"base_link\"\n      - \"enable_slam_visualization\": True\n```\n\n### VSLAM Performance Optimization\n\n```cpp\n// Optimized VSLAM node with performance considerations\nclass OptimizedVSLAMNode : public rclcpp::Node\n{\npublic:\n    OptimizedVSLAMNode() : Node(\"optimized_vslam\")\n    {\n        // Use intra-process communication when possible\n        rclcpp::QoS qos(10);\n        qos.best_effort();\n\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/image_raw\", qos,\n            std::bind(&OptimizedVSLAMNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        // Throttle processing if needed\n        processing_rate_ = this->declare_parameter(\"processing_rate\", 10.0);\n        timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(static_cast<int>(1000.0 / processing_rate_)),\n            std::bind(&OptimizedVSLAMNode::processCallback, this)\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Store image for processing at fixed rate\n        latest_image_ = msg;\n        image_available_ = true;\n    }\n\n    void processCallback()\n    {\n        if (!image_available_) return;\n\n        // Process with VSLAM\n        processVSLAM(latest_image_);\n        image_available_ = false;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n    sensor_msgs::msg::Image::SharedPtr latest_image_;\n    bool image_available_ = false;\n    double processing_rate_;\n};\n```\n\n## Humanoid Navigation Challenges\n\n### 3D Navigation\nHumanoid robots require 3D navigation capabilities:\n\n```cpp\n// 3D navigation for humanoid robots\nclass Humanoid3DNavigator\n{\npublic:\n    void navigate3D(const geometry_msgs::msg::Pose& target)\n    {\n        // Plan 3D path considering robot's height and step capabilities\n        auto path3d = plan3DPath(current_pose_, target);\n\n        // Execute path with balance considerations\n        executePathWithBalance(path3d);\n    }\n\nprivate:\n    std::vector<geometry_msgs::msg::Pose> plan3DPath(\n        const geometry_msgs::msg::Pose& start,\n        const geometry_msgs::msg::Pose& goal)\n    {\n        // Implement 3D path planning considering:\n        // - Robot's height and reach\n        // - Stair navigation\n        // - Obstacle avoidance in 3D space\n        // - Balance constraints\n    }\n\n    void executePathWithBalance(const std::vector<geometry_msgs::msg::Pose>& path)\n    {\n        // Execute path while maintaining balance\n        // This involves:\n        // - Walking pattern generation\n        // - Balance control\n        // - Step planning\n    }\n};\n```\n\n### Multi-Modal Navigation\nCombine different navigation modes:\n\n```cpp\nenum NavigationMode {\n    WALKING,\n    CLIMBING,\n    CRAWLING,\n    MANIPULATION_ASSISTED\n};\n\nclass MultiModalNavigator\n{\npublic:\n    void navigateWithMode(const geometry_msgs::msg::Pose& target, NavigationMode mode)\n    {\n        switch (mode) {\n            case WALKING:\n                executeWalkingNavigation(target);\n                break;\n            case CLIMBING:\n                executeClimbingNavigation(target);\n                break;\n            case CRAWLING:\n                executeCrawlingNavigation(target);\n                break;\n            case MANIPULATION_ASSISTED:\n                executeManipulationAssistedNavigation(target);\n                break;\n        }\n    }\n\nprivate:\n    void executeWalkingNavigation(const geometry_msgs::msg::Pose& target);\n    void executeClimbingNavigation(const geometry_msgs::msg::Pose& target);\n    void executeCrawlingNavigation(const geometry_msgs::msg::Pose& target);\n    void executeManipulationAssistedNavigation(const geometry_msgs::msg::Pose& target);\n};\n```\n\n## Performance Evaluation\n\n### VSLAM Metrics\nEvaluate VSLAM performance with:\n\n- **Absolute Trajectory Error (ATE)**: Difference between estimated and ground truth trajectory\n- **Relative Pose Error (RPE)**: Error in relative motion estimates\n- **Processing Time**: Real-time performance metrics\n- **Map Accuracy**: Quality of reconstructed environment\n\n### Navigation Metrics\nFor navigation performance:\n\n- **Success Rate**: Percentage of successful goal reaches\n- **Path Efficiency**: Actual path length vs optimal path\n- **Time to Goal**: Navigation completion time\n- **Safety**: Number of collisions or near-misses\n\n## Troubleshooting VSLAM Issues\n\n### Common Problems and Solutions\n\n#### 1. Drift\n**Problem**: Accumulated pose errors over time\n**Solutions**:\n- Implement loop closure detection\n- Use sensor fusion with IMU/odometry\n- Regular relocalization against known features\n\n#### 2. Low Texture Environments\n**Problem**: Insufficient features for tracking\n**Solutions**:\n- Use direct methods (LSD-SLAM, DSO)\n- Add artificial markers or fiducials\n- Combine with other sensors (LIDAR, IMU)\n\n#### 3. Dynamic Objects\n**Problem**: Moving objects affecting map/pose estimation\n**Solutions**:\n- Implement dynamic object detection and filtering\n- Use semantic segmentation to identify static objects\n- Temporal consistency checks\n\n## Best Practices\n\n### 1. Robust Initialization\n- Ensure good initial pose estimate\n- Verify camera calibration\n- Check lighting conditions\n\n### 2. Parameter Tuning\n- Adjust parameters based on environment\n- Monitor performance metrics\n- Use adaptive parameters when possible\n\n### 3. Sensor Fusion\n- Combine VSLAM with other sensors\n- Use IMU for motion prediction\n- Integrate with wheel odometry\n\n### 4. Computational Efficiency\n- Optimize feature detection and matching\n- Use appropriate image resolution\n- Implement multi-threading where possible\n\nIsaac ROS Visual SLAM provides a powerful foundation for robot localization and mapping, especially when combined with other Isaac ROS perception packages for a complete AI-powered robotics solution.",
    "path": "module-3-ai-perception\\vslam-navigation.md",
    "description": ""
  },
  "module-4-vla\\humanoid-locomotion-control": {
    "title": "module-4-vla\\humanoid-locomotion-control",
    "content": "# Humanoid Locomotion and Control\n\nHumanoid locomotion represents one of the most challenging problems in robotics, requiring sophisticated control systems to achieve stable, efficient, and human-like movement. This section covers the principles and implementation of humanoid locomotion using Isaac Sim and Isaac ROS.\n\n## Introduction to Humanoid Locomotion\n\nHumanoid locomotion involves the complex control of multi-degree-of-freedom systems to achieve stable movement patterns similar to human walking. Unlike wheeled robots, humanoid robots must manage balance, foot placement, and dynamic stability simultaneously.\n\n### Key Challenges in Humanoid Locomotion\n\n1. **Balance Control**: Maintaining center of mass within support polygon\n2. **Step Planning**: Determining optimal foot placement\n3. **Gait Generation**: Creating stable walking patterns\n4. **Terrain Adaptation**: Adjusting to different surfaces and obstacles\n5. **Dynamic Stability**: Managing momentum during movement\n\n### Types of Humanoid Gait\n\n- **Static Gait**: Center of mass always within support polygon\n- **Dynamic Gait**: Uses momentum to maintain balance\n- **Passive Dynamic**: Exploits mechanical dynamics for efficient walking\n- **Adaptive Gait**: Adjusts to terrain and disturbances in real-time\n\n## Center of Mass (CoM) Control\n\n### Zero Moment Point (ZMP) Theory\n\nThe Zero Moment Point is a critical concept in humanoid locomotion:\n\n```\nZMP = ((mi * (xi * g - x''i * mi)) / (mi * g - z''i * mi), (mi * (yi * g - y''i * mi)) / (mi * g - z''i * mi))\n```\n\nWhere:\n- mi = mass of point i\n- xi, yi = position coordinates\n- x''i, y''i = acceleration\n- g = gravitational acceleration\n- z''i = vertical acceleration\n\n### Center of Pressure (CoP) vs ZMP\n\n- **Center of Pressure (CoP)**: Point where the ground reaction force acts\n- **Zero Moment Point (ZMP)**: Point where net moment of active forces equals moment of passive forces\n\nFor stable walking, ZMP must remain within the support polygon defined by the feet.\n\n## Walking Pattern Generation\n\n### Preview Control Method\n\nPreview control uses future reference trajectories to generate stable walking patterns:\n\n```python\nimport numpy as np\nfrom scipy.linalg import solve_continuous_are\nfrom scipy.integrate import solve_ivp\n\nclass PreviewController:\n    def __init__(self, dt=0.01, preview_horizon=2.0):\n        self.dt = dt\n        self.preview_horizon = preview_horizon\n        self.preview_steps = int(preview_horizon / dt)\n\n    def compute_reference_trajectory(self, start_pos, goal_pos, walk_speed=0.3):\n        \"\"\"Compute reference trajectory for walking\"\"\"\n        # Calculate distance to goal\n        dist = np.sqrt((goal_pos[0] - start_pos[0])**2 + (goal_pos[1] - start_pos[1])**2)\n\n        # Generate trajectory points\n        steps = int(dist / (walk_speed * self.dt))\n        x_traj = np.linspace(start_pos[0], goal_pos[0], steps)\n        y_traj = np.linspace(start_pos[1], goal_pos[1], steps)\n\n        # Add small sinusoidal variation for natural movement\n        t = np.arange(len(x_traj)) * self.dt\n        y_variation = 0.02 * np.sin(2 * np.pi * t * 0.5)  # Small vertical movement\n\n        return np.column_stack([x_traj, y_traj, y_variation])\n\n    def generate_footsteps(self, com_trajectory, step_length=0.3, step_width=0.2):\n        \"\"\"Generate footstep locations based on CoM trajectory\"\"\"\n        footsteps = []\n        current_left = [0, step_width/2, 0]  # Start with left foot\n        current_right = [0, -step_width/2, 0]  # Start with right foot\n\n        for i, com_pos in enumerate(com_trajectory):\n            # Alternate steps based on phase\n            if i % (int(0.8 / self.dt) * 2) < int(0.8 / self.dt):  # Left foot phase\n                # Place left foot\n                target_x = com_pos[0] + step_length / 2\n                target_y = com_pos[1] + step_width / 2\n                current_left = [target_x, target_y, 0]\n            else:  # Right foot phase\n                # Place right foot\n                target_x = com_pos[0] + step_length / 2\n                target_y = com_pos[1] - step_width / 2\n                current_right = [target_x, target_y, 0]\n\n            # Add footstep to trajectory\n            if i % int(0.8 / self.dt) == 0:  # Add step every half cycle\n                if i % (int(0.8 / self.dt) * 2) < int(0.8 / self.dt):\n                    footsteps.append(('left', current_left))\n                else:\n                    footsteps.append(('right', current_right))\n\n        return footsteps\n```\n\n### Linear Inverted Pendulum Model (LIPM)\n\nThe Linear Inverted Pendulum Model simplifies humanoid balance:\n\n```python\nclass LIPMController:\n    def __init__(self, com_height=0.8, gravity=9.81):\n        self.com_height = com_height\n        self.gravity = gravity\n        self.omega = np.sqrt(gravity / com_height)\n\n    def compute_zmp_from_com(self, com_pos, com_vel, com_acc):\n        \"\"\"Compute ZMP from CoM position, velocity, and acceleration\"\"\"\n        zmp_x = com_pos[0] - (com_acc[0] / self.gravity) * self.com_height\n        zmp_y = com_pos[1] - (com_acc[1] / self.gravity) * self.com_height\n        return [zmp_x, zmp_y, 0]\n\n    def compute_com_from_zmp(self, zmp_pos, com_pos_prev, com_vel_prev):\n        \"\"\"Compute CoM position from desired ZMP using LIPM\"\"\"\n        dt = 0.01  # Control timestep\n\n        # LIPM dynamics: com_ddot = omega^2 * (com - zmp)\n        com_acc = self.omega**2 * (np.array(com_pos_prev[:2]) - np.array(zmp_pos[:2]))\n\n        # Integrate to get new CoM position and velocity\n        new_com_vel = com_vel_prev[:2] + com_acc * dt\n        new_com_pos = com_pos_prev[:2] + new_com_vel * dt + 0.5 * com_acc * dt**2\n\n        return [new_com_pos[0], new_com_pos[1], self.com_height], new_com_vel\n```\n\n## Isaac ROS Humanoid Control Integration\n\n### Isaac ROS Control Packages\n\nIsaac ROS provides specialized packages for humanoid control:\n\n```yaml\n# Isaac ROS Control configuration\nisaac_ros_control:\n  ros__parameters:\n    update_rate: 100  # Hz\n    controller_manager:\n      ros__parameters:\n        use_sim_time: true\n        controller_names:\n          - joint_state_broadcaster\n          - left_leg_controller\n          - right_leg_controller\n          - torso_controller\n          - head_controller\n\nleft_leg_controller:\n  ros__parameters:\n    type: position_controllers/JointGroupPositionController\n    joints:\n      - left_hip_roll\n      - left_hip_yaw\n      - left_hip_pitch\n      - left_knee\n      - left_ankle_pitch\n      - left_ankle_roll\n\nright_leg_controller:\n  ros__parameters:\n    type: position_controllers/JointGroupPositionController\n    joints:\n      - right_hip_roll\n      - right_hip_yaw\n      - right_hip_pitch\n      - right_knee\n      - right_ankle_pitch\n      - right_ankle_roll\n```\n\n### Humanoid Balance Controller\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/joint_state.hpp>\n#include <geometry_msgs/msg/vector3_stamped.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n#include <Eigen/Dense>\n\nclass HumanoidBalanceController : public rclcpp::Node\n{\npublic:\n    HumanoidBalanceController() : Node(\"humanoid_balance_controller\")\n    {\n        // Subscriptions\n        joint_state_sub_ = this->create_subscription<sensor_msgs::msg::JointState>(\n            \"joint_states\", 10,\n            std::bind(&HumanoidBalanceController::jointStateCallback, this, std::placeholders::_1)\n        );\n\n        imu_sub_ = this->create_subscription<sensor_msgs::msg::Imu>(\n            \"imu/data\", 10,\n            std::bind(&HumanoidBalanceController::imuCallback, this, std::placeholders::_1)\n        );\n\n        // Publishers\n        target_joints_pub_ = this->create_publisher<sensor_msgs::msg::JointState>(\n            \"target_joint_positions\", 10\n        );\n\n        // Timer for control loop\n        control_timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(10),  // 100 Hz control\n            std::bind(&HumanoidBalanceController::controlLoop, this)\n        );\n\n        // Initialize balance controller parameters\n        com_height_ = this->declare_parameter(\"com_height\", 0.8);\n        control_gain_ = this->declare_parameter(\"control_gain\", 10.0);\n        max_correction_ = this->declare_parameter(\"max_correction\", 0.1);\n\n        RCLCPP_INFO(this->get_logger(), \"Humanoid Balance Controller initialized\");\n    }\n\nprivate:\n    void jointStateCallback(const sensor_msgs::msg::JointState::SharedPtr msg)\n    {\n        // Store current joint states\n        current_joint_positions_ = msg->position;\n        current_joint_velocities_ = msg->velocity;\n        current_joint_efforts_ = msg->effort;\n        has_joint_state_ = true;\n    }\n\n    void imuCallback(const sensor_msgs::msg::Imu::SharedPtr msg)\n    {\n        // Store IMU data for balance feedback\n        imu_orientation_ = msg->orientation;\n        imu_angular_velocity_ = msg->angular_velocity;\n        imu_linear_acceleration_ = msg->linear_acceleration;\n        has_imu_data_ = true;\n    }\n\n    void controlLoop()\n    {\n        if (!has_joint_state_ || !has_imu_data_) {\n            return;  // Wait for sensor data\n        }\n\n        // Calculate current center of mass\n        auto com_pos = calculateCenterOfMass(current_joint_positions_);\n        auto com_vel = calculateCenterOfMassVelocity(current_joint_positions_, current_joint_velocities_);\n\n        // Calculate desired ZMP based on target trajectory\n        auto desired_zmp = calculateDesiredZMP();\n\n        // Calculate current ZMP from IMU and kinematics\n        auto current_zmp = calculateCurrentZMP(com_pos, com_vel, imu_linear_acceleration_);\n\n        // Compute balance error\n        double x_error = desired_zmp.x - current_zmp.x;\n        double y_error = desired_zmp.y - current_zmp.y;\n\n        // Generate corrective joint commands using PID control\n        auto correction_commands = computeBalanceCorrection(x_error, y_error);\n\n        // Apply corrections to joint targets\n        auto target_joints = applyBalanceCorrections(current_joint_positions_, correction_commands);\n\n        // Publish target joint positions\n        publishTargetJoints(target_joints);\n    }\n\n    geometry_msgs::msg::Point calculateCenterOfMass(const std::vector<double>& joint_positions)\n    {\n        // Calculate CoM based on joint positions and link masses\n        // This is a simplified implementation - in practice, use URDF info\n        geometry_msgs::msg::Point com;\n\n        // For a simplified model, assume CoM is at fixed height\n        // and calculate horizontal position based on joint angles\n        double com_x = 0.0, com_y = 0.0;\n\n        // Simplified CoM calculation (would use full kinematic model in practice)\n        for (size_t i = 0; i < joint_positions.size(); ++i) {\n            // Weight each joint contribution based on its position in the kinematic chain\n            com_x += joint_positions[i] * 0.01;  // Simplified weighting\n            com_y += joint_positions[i] * 0.01;  // Simplified weighting\n        }\n\n        com.x = com_x;\n        com.y = com_y;\n        com.z = com_height_;\n\n        return com;\n    }\n\n    geometry_msgs::msg::Point calculateCurrentZMP(\n        const geometry_msgs::msg::Point& com_pos,\n        const geometry_msgs::msg::Point& com_vel,\n        const geometry_msgs::msg::Vector3& linear_acc)\n    {\n        // Calculate ZMP from CoM and acceleration data\n        // ZMP_x = CoM_x - (CoM_z * CoM_acc_x) / g\n        // ZMP_y = CoM_y - (CoM_z * CoM_acc_y) / g\n\n        geometry_msgs::msg::Point zmp;\n        zmp.x = com_pos.x - (com_pos.z * linear_acc.x) / gravity_;\n        zmp.y = com_pos.y - (com_pos.z * linear_acc.y) / gravity_;\n        zmp.z = 0.0;  // ZMP is on the ground plane\n\n        return zmp;\n    }\n\n    std::vector<double> computeBalanceCorrection(double x_error, double y_error)\n    {\n        // Simple PD controller for balance correction\n        static double prev_x_error = 0, prev_y_error = 0;\n        static double integral_x_error = 0, integral_y_error = 0;\n\n        // PID parameters\n        double kp = 100.0;  // Proportional gain\n        double ki = 10.0;   // Integral gain\n        double kd = 50.0;   // Derivative gain\n\n        // Update error integrals\n        integral_x_error += x_error * dt_;\n        integral_y_error += y_error * dt_;\n\n        // Calculate derivatives\n        double dx_error = (x_error - prev_x_error) / dt_;\n        double dy_error = (y_error - prev_y_error) / dt_;\n\n        // Compute control outputs\n        double x_control = kp * x_error + ki * integral_x_error + kd * dx_error;\n        double y_control = kp * y_error + ki * integral_y_error + kd * dy_error;\n\n        // Limit control outputs\n        x_control = std::max(-max_correction_, std::min(max_correction_, x_control));\n        y_control = std::max(-max_correction_, std::min(max_correction_, y_control));\n\n        // Convert to joint space corrections\n        // This would involve inverse kinematics in a real implementation\n        std::vector<double> corrections(num_joints_, 0.0);\n\n        // Simplified mapping - in practice, use full inverse kinematics\n        corrections[left_hip_roll_idx_] = x_control * 0.1;\n        corrections[right_hip_roll_idx_] = -x_control * 0.1;\n        corrections[left_ankle_roll_idx_] = -x_control * 0.2;\n        corrections[right_ankle_roll_idx_] = x_control * 0.2;\n\n        corrections[left_hip_pitch_idx_] = y_control * 0.1;\n        corrections[right_hip_pitch_idx_] = y_control * 0.1;\n        corrections[left_ankle_pitch_idx_] = -y_control * 0.2;\n        corrections[right_ankle_pitch_idx_] = -y_control * 0.2;\n\n        prev_x_error = x_error;\n        prev_y_error = y_error;\n\n        return corrections;\n    }\n\n    void publishTargetJoints(const std::vector<double>& target_positions)\n    {\n        auto msg = sensor_msgs::msg::JointState();\n        msg.header.stamp = this->now();\n        msg.name = joint_names_;  // Would be initialized with actual joint names\n        msg.position = target_positions;\n\n        target_joints_pub_->publish(msg);\n    }\n\n    // Subscriptions\n    rclcpp::Subscription<sensor_msgs::msg::JointState>::SharedPtr joint_state_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Imu>::SharedPtr imu_sub_;\n\n    // Publishers\n    rclcpp::Publisher<sensor_msgs::msg::JointState>::SharedPtr target_joints_pub_;\n\n    // Timer\n    rclcpp::TimerBase::SharedPtr control_timer_;\n\n    // State variables\n    std::vector<double> current_joint_positions_;\n    std::vector<double> current_joint_velocities_;\n    std::vector<double> current_joint_efforts_;\n    geometry_msgs::msg::Quaternion imu_orientation_;\n    geometry_msgs::msg::Vector3 imu_angular_velocity_;\n    geometry_msgs::msg::Vector3 imu_linear_acceleration_;\n\n    bool has_joint_state_ = false;\n    bool has_imu_data_ = false;\n\n    // Balance control parameters\n    double com_height_;\n    double control_gain_;\n    double max_correction_;\n    double gravity_ = 9.81;\n    double dt_ = 0.01;  // Control timestep\n\n    // Joint indices (would be initialized based on actual robot)\n    int left_hip_roll_idx_ = 0;\n    int right_hip_roll_idx_ = 1;\n    // ... other joint indices\n};\n```\n\n## Isaac Sim Humanoid Simulation\n\n### Creating Humanoid Robots in Isaac Sim\n\n```python\n# Isaac Sim humanoid robot setup\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.articulations import ArticulationView\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np\n\nclass IsaacHumanoidRobot:\n    def __init__(self, prim_path=\"/World/HumanoidRobot\", name=\"humanoid\"):\n        self.prim_path = prim_path\n        self.name = name\n        self.world = World()\n\n        # Add humanoid robot to stage\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Use a humanoid robot asset (example with Carter robot - replace with actual humanoid)\n        robot_asset_path = assets_root_path + \"/Isaac/Robots/Humanoid/humanoid_instanceable.usd\"\n        add_reference_to_stage(usd_path=robot_asset_path, prim_path=prim_path)\n\n        # Create articulation view for the robot\n        self.robot_articulation = ArticulationView(prim_path=prim_path + \"/base_link\", name=name + \"_view\")\n        self.world.add_articulation(self.robot_articulation)\n\n    def initialize_robot(self):\n        \"\"\"Initialize robot in simulation\"\"\"\n        self.world.reset()\n\n        # Set default joint positions for standing pose\n        default_positions = np.array([\n            0.0,  # left_hip_roll\n            0.0,  # left_hip_yaw\n            0.0,  # left_hip_pitch\n            0.0,  # left_knee\n            0.0,  # left_ankle_pitch\n            0.0,  # left_ankle_roll\n            0.0,  # right_hip_roll\n            0.0,  # right_hip_yaw\n            0.0,  # right_hip_pitch\n            0.0,  # right_knee\n            0.0,  # right_ankle_pitch\n            0.0,  # right_ankle_roll\n            # ... add other joints\n        ])\n\n        self.robot_articulation.set_joint_positions(default_positions)\n\n    def move_to_standing_pose(self):\n        \"\"\"Move robot to neutral standing pose\"\"\"\n        standing_positions = np.array([\n            0.0,   # left_hip_roll\n            0.0,   # left_hip_yaw\n            0.0,   # left_hip_pitch (neutral)\n            0.0,   # left_knee (straight)\n            0.0,   # left_ankle_pitch\n            0.0,   # left_ankle_roll\n            0.0,   # right_hip_roll\n            0.0,   # right_hip_yaw\n            0.0,   # right_hip_pitch (neutral)\n            0.0,   # right_knee (straight)\n            0.0,   # right_ankle_pitch\n            0.0,   # right_ankle_roll\n        ])\n\n        self.robot_articulation.set_joint_positions(standing_positions)\n\n    def execute_walk_cycle(self, step_phase, step_length=0.3, step_height=0.05):\n        \"\"\"Execute a single step in walking cycle\"\"\"\n        # Calculate joint positions based on step phase (0 to 1)\n        # This is a simplified walking pattern - in practice, use proper gait generation\n\n        # Left leg trajectory\n        left_knee_angle = np.sin(step_phase * 2 * np.pi) * 0.3  # Knee bend\n        left_ankle_pitch = -left_knee_angle * 0.5  # Compensate for knee movement\n\n        # Right leg trajectory (opposite phase)\n        right_phase = (step_phase + 0.5) % 1.0\n        right_knee_angle = np.sin(right_phase * 2 * np.pi) * 0.3\n        right_ankle_pitch = -right_knee_angle * 0.5\n\n        # Hip adjustments for balance\n        pelvis_roll = np.sin(step_phase * 2 * np.pi) * 0.1  # Shift weight\n\n        target_positions = np.array([\n            pelvis_roll,  # left_hip_roll\n            0.0,          # left_hip_yaw\n            0.0,          # left_hip_pitch\n            left_knee_angle,      # left_knee\n            left_ankle_pitch,     # left_ankle_pitch\n            0.0,          # left_ankle_roll\n            -pelvis_roll, # right_hip_roll (opposite to maintain balance)\n            0.0,          # right_hip_yaw\n            0.0,          # right_hip_pitch\n            right_knee_angle,     # right_knee\n            right_ankle_pitch,    # right_ankle_pitch\n            0.0,          # right_ankle_roll\n        ])\n\n        self.robot_articulation.set_joint_positions(target_positions)\n\n    def get_robot_state(self):\n        \"\"\"Get current robot state including joint positions, velocities, and CoM\"\"\"\n        joint_positions = self.robot_articulation.get_joint_positions()\n        joint_velocities = self.robot_articulation.get_joint_velocities()\n\n        # Calculate center of mass position and velocity\n        com_position = self.calculate_center_of_mass(joint_positions)\n        com_velocity = self.calculate_center_of_mass_velocity(joint_positions, joint_velocities)\n\n        robot_state = {\n            'joint_positions': joint_positions,\n            'joint_velocities': joint_velocities,\n            'com_position': com_position,\n            'com_velocity': com_velocity,\n            'base_position': self.robot_articulation.get_world_poses()[0][0],\n            'base_orientation': self.robot_articulation.get_world_poses()[1][0]\n        }\n\n        return robot_state\n\n    def calculate_center_of_mass(self, joint_positions):\n        \"\"\"Calculate center of mass position from joint configuration\"\"\"\n        # Simplified CoM calculation - in practice, use URDF mass properties\n        # This would involve forward kinematics and mass-weighted averaging\n\n        # For now, return a simplified estimate\n        return np.array([0.0, 0.0, 0.8])  # Approximate CoM height for humanoid\n\n    def calculate_center_of_mass_velocity(self, joint_positions, joint_velocities):\n        \"\"\"Calculate center of mass velocity\"\"\"\n        # Simplified CoM velocity calculation\n        return np.array([0.0, 0.0, 0.0])\n```\n\n## Gait Planning and Execution\n\n### Walking Pattern Generator\n\n```python\nclass WalkingPatternGenerator:\n    def __init__(self, step_length=0.3, step_width=0.2, step_height=0.05, step_duration=0.8):\n        self.step_length = step_length\n        self.step_width = step_width\n        self.step_height = step_height\n        self.step_duration = step_duration\n        self.dt = 0.01  # Control timestep\n\n    def generate_walk_trajectory(self, distance, direction='forward'):\n        \"\"\"Generate complete walk trajectory for given distance\"\"\"\n        # Calculate number of steps needed\n        step_count = int(distance / self.step_length)\n\n        trajectory = []\n\n        for step_num in range(step_count):\n            # Generate single step trajectory\n            step_trajectory = self.generate_single_step(step_num % 2 == 0)  # Alternate feet\n            trajectory.extend(step_trajectory)\n\n        return trajectory\n\n    def generate_single_step(self, use_left_foot=True):\n        \"\"\"Generate trajectory for a single step\"\"\"\n        steps_per_phase = int(self.step_duration / self.dt / 4)  # 4 phases per step\n\n        trajectory = []\n\n        # Phase 1: Preparation (lift foot slightly)\n        for i in range(steps_per_phase):\n            t = i / steps_per_phase\n            lift_amount = self.step_height * 0.3 * t  # Gentle lift\n\n            joint_positions = self.calculate_step_joints(\n                phase=t,\n                foot_lift=lift_amount,\n                swing_foot=use_left_foot\n            )\n            trajectory.append(joint_positions)\n\n        # Phase 2: Swing (move foot forward)\n        for i in range(steps_per_phase):\n            t = i / steps_per_phase\n            forward_progress = self.step_length * t\n            foot_lift = self.step_height * (1 - (t - 0.5)**2)  # Parabolic lift\n\n            joint_positions = self.calculate_step_joints(\n                phase=t,\n                forward_progress=forward_progress,\n                foot_lift=foot_lift,\n                swing_foot=use_left_foot\n            )\n            trajectory.append(joint_positions)\n\n        # Phase 3: Landing (lower foot)\n        for i in range(steps_per_phase):\n            t = i / steps_per_phase\n            foot_lift = self.step_height * (1 - t)  # Lower foot\n            forward_progress = self.step_length\n\n            joint_positions = self.calculate_step_joints(\n                phase=t,\n                forward_progress=forward_progress,\n                foot_lift=foot_lift,\n                swing_foot=use_left_foot\n            )\n            trajectory.append(joint_positions)\n\n        # Phase 4: Stabilization (adjust for balance)\n        for i in range(steps_per_phase):\n            t = i / steps_per_phase\n            joint_positions = self.calculate_stance_joints(use_left_foot)\n            trajectory.append(joint_positions)\n\n        return trajectory\n\n    def calculate_step_joints(self, phase, forward_progress=0, foot_lift=0, swing_foot=True):\n        \"\"\"Calculate joint positions for step execution\"\"\"\n        # This would implement inverse kinematics for foot placement\n        # For simplicity, return a basic pattern\n\n        if swing_foot:  # Left foot is swinging\n            # Move left foot forward and lift it\n            left_knee = np.clip(foot_lift * 3, 0, 0.5)  # Knee bend for lifting\n            left_ankle = -foot_lift  # Compensate ankle for lift\n            right_knee = 0.0  # Right leg straight\n            right_ankle = 0.0\n        else:  # Right foot is swinging\n            right_knee = np.clip(foot_lift * 3, 0, 0.5)\n            right_ankle = -foot_lift\n            left_knee = 0.0\n            left_ankle = 0.0\n\n        # Hip adjustments for balance during step\n        hip_adjustment = foot_lift * 0.2 if foot_lift > 0 else 0.0\n\n        joint_positions = np.zeros(12)  # Assuming 12 leg joints\n        joint_positions[3] = left_knee    # left_knee\n        joint_positions[4] = left_ankle   # left_ankle_pitch\n        joint_positions[9] = right_knee   # right_knee\n        joint_positions[10] = right_ankle # right_ankle_pitch\n\n        if swing_foot:\n            joint_positions[2] = hip_adjustment   # left_hip_pitch (raise opposite hip)\n            joint_positions[8] = -hip_adjustment  # right_hip_pitch (lower swing hip)\n        else:\n            joint_positions[2] = -hip_adjustment  # left_hip_pitch (lower swing hip)\n            joint_positions[8] = hip_adjustment   # right_hip_pitch (raise opposite hip)\n\n        return joint_positions\n\n    def calculate_stance_joints(self, stance_foot_is_left):\n        \"\"\"Calculate joint positions for stable stance\"\"\"\n        # Return neutral standing position with slight knee bend\n        joint_positions = np.zeros(12)\n        joint_positions[3] = 0.1  # Slight knee bend for stability\n        joint_positions[9] = 0.1  # Slight knee bend for stability\n        return joint_positions\n```\n\n## ROS 2 Integration for Humanoid Control\n\n### Humanoid Controller Manager\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <std_msgs/msg/float64_multi_array.hpp>\n#include <sensor_msgs/msg/joint_state.hpp>\n#include <geometry_msgs/msg/twist.h>\n#include <control_msgs/msg/joint_trajectory_controller_state.hpp>\n\nclass HumanoidControllerManager : public rclcpp::Node\n{\npublic:\n    HumanoidControllerManager() : Node(\"humanoid_controller_manager\")\n    {\n        // Initialize humanoid-specific controllers\n        initializeControllers();\n\n        // Create subscribers for different command types\n        cmd_vel_sub_ = this->create_subscription<geometry_msgs::msg::Twist>(\n            \"cmd_vel\", 10,\n            std::bind(&HumanoidControllerManager::cmdVelCallback, this, std::placeholders::_1)\n        );\n\n        joint_command_sub_ = this->create_subscription<std_msgs::msg::Float64MultiArray>(\n            \"joint_group_position_controller/commands\", 10,\n            std::bind(&HumanoidControllerManager::jointCommandCallback, this, std::placeholders::_1)\n        );\n\n        // Publishers for robot state\n        robot_state_pub_ = this->create_publisher<sensor_msgs::msg::JointState>(\n            \"robot_state\", 10\n        );\n\n        // Timer for control loop\n        control_timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(10),  // 100 Hz\n            std::bind(&HumanoidControllerManager::controlLoop, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Humanoid Controller Manager initialized\");\n    }\n\nprivate:\n    void cmdVelCallback(const geometry_msgs::msg::Twist::SharedPtr msg)\n    {\n        // Convert velocity command to walking pattern\n        requested_linear_vel_ = msg->linear.x;\n        requested_angular_vel_ = msg->angular.z;\n\n        // Generate appropriate gait pattern based on velocity request\n        generateWalkPattern(requested_linear_vel_, requested_angular_vel_);\n    }\n\n    void jointCommandCallback(const std_msgs::msg::Float64MultiArray::SharedPtr msg)\n    {\n        // Handle direct joint position commands\n        target_joint_positions_ = msg->data;\n        control_mode_ = JOINT_POSITION_CONTROL;\n    }\n\n    void controlLoop()\n    {\n        // Get current robot state\n        auto current_state = getCurrentRobotState();\n\n        // Apply appropriate control based on mode\n        std::vector<double> commands;\n\n        switch (control_mode_) {\n            case WALK_CONTROL:\n                commands = generateWalkCommands(current_state);\n                break;\n            case JOINT_POSITION_CONTROL:\n                commands = generateJointPositionCommands(current_state);\n                break;\n            case BALANCE_CONTROL:\n                commands = generateBalanceCommands(current_state);\n                break;\n            default:\n                commands = std::vector<double>(num_joints_, 0.0);\n                break;\n        }\n\n        // Apply commands to robot\n        sendJointCommands(commands);\n\n        // Publish robot state\n        publishRobotState(current_state);\n    }\n\n    std::vector<double> generateWalkCommands(const RobotState& state)\n    {\n        // Generate walking pattern based on requested velocities\n        std::vector<double> commands(num_joints_, 0.0);\n\n        // Calculate step parameters based on requested velocities\n        double step_frequency = calculateStepFrequency(requested_linear_vel_);\n        double step_length = calculateStepLength(requested_linear_vel_);\n        double turn_compensation = calculateTurnCompensation(requested_angular_vel_);\n\n        // Generate walking pattern\n        auto walk_pattern = walking_generator_.generateWalkingPattern(\n            step_frequency, step_length, turn_compensation, state\n        );\n\n        // Convert to joint commands\n        commands = walk_pattern.toJointCommands();\n\n        return commands;\n    }\n\n    void initializeControllers()\n    {\n        // Initialize different controller types:\n        // - Joint position controllers for each limb\n        // - Balance controller for CoM management\n        // - Walking pattern generator\n        // - Impedance controllers for compliant behavior\n\n        walking_generator_.initialize();\n        balance_controller_.initialize();\n        impedance_controllers_.initialize();\n    }\n\n    enum ControlMode {\n        WALK_CONTROL,\n        JOINT_POSITION_CONTROL,\n        BALANCE_CONTROL,\n        TRAJECTORY_CONTROL\n    };\n\n    ControlMode control_mode_ = WALK_CONTROL;\n\n    // Subscriptions\n    rclcpp::Subscription<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_sub_;\n    rclcpp::Subscription<std_msgs::msg::Float64MultiArray>::SharedPtr joint_command_sub_;\n\n    // Publishers\n    rclcpp::Publisher<sensor_msgs::msg::JointState>::SharedPtr robot_state_pub_;\n\n    // Timer\n    rclcpp::TimerBase::SharedPtr control_timer_;\n\n    // State variables\n    double requested_linear_vel_ = 0.0;\n    double requested_angular_vel_ = 0.0;\n    std::vector<double> target_joint_positions_;\n    std::vector<std::string> joint_names_;\n\n    // Controllers\n    WalkingPatternGenerator walking_generator_;\n    BalanceController balance_controller_;\n    ImpedanceControllerManager impedance_controllers_;\n\n    // Constants\n    const size_t num_joints_ = 28;  // Typical humanoid has ~28+ joints\n};\n```\n\n## Advanced Locomotion Patterns\n\n### Different Gait Types\n\n```python\nclass GaitController:\n    def __init__(self):\n        self.current_gait = \"walking\"\n        self.gait_parameters = {\n            \"walking\": {\n                \"step_length\": 0.3,\n                \"step_height\": 0.05,\n                \"step_duration\": 0.8,\n                \"stance_ratio\": 0.6\n            },\n            \"trotting\": {\n                \"step_length\": 0.4,\n                \"step_height\": 0.1,\n                \"step_duration\": 0.4,\n                \"stance_ratio\": 0.4\n            },\n            \"crawling\": {\n                \"step_length\": 0.1,\n                \"step_height\": 0.02,\n                \"step_duration\": 1.0,\n                \"stance_ratio\": 0.8\n            }\n        }\n\n    def switch_gait(self, gait_type):\n        \"\"\"Switch between different locomotion gaits\"\"\"\n        if gait_type in self.gait_parameters:\n            self.current_gait = gait_type\n            self.update_gait_parameters(self.gait_parameters[gait_type])\n            return True\n        return False\n\n    def generate_gait_pattern(self, velocity, terrain_type=\"flat\"):\n        \"\"\"Generate gait pattern based on velocity and terrain\"\"\"\n        gait_params = self.gait_parameters[self.current_gait]\n\n        if terrain_type == \"rough\":\n            # Adjust parameters for rough terrain\n            gait_params[\"step_height\"] *= 1.5\n            gait_params[\"step_length\"] *= 0.7\n            gait_params[\"step_duration\"] *= 1.2\n\n        elif terrain_type == \"stairs\":\n            # Special pattern for stair climbing\n            return self.generate_stair_climbing_pattern(velocity)\n\n        elif terrain_type == \"narrow\":\n            # Adjust for narrow passages\n            gait_params[\"step_width\"] = 0.1  # Narrower steps\n\n        # Generate appropriate pattern based on current gait\n        if self.current_gait == \"walking\":\n            return self.generate_walking_pattern(velocity, gait_params)\n        elif self.current_gait == \"trotting\":\n            return self.generate_trotting_pattern(velocity, gait_params)\n        elif self.current_gait == \"crawling\":\n            return self.generate_crawling_pattern(velocity, gait_params)\n\n    def generate_stair_climbing_pattern(self, velocity):\n        \"\"\"Generate special pattern for stair climbing\"\"\"\n        # Stair climbing requires specific foot placement and balance\n        # This is a simplified implementation\n        pattern = []\n\n        # Approach step\n        approach_joints = self.calculate_stance_joints()\n        pattern.extend([approach_joints] * 10)  # Hold approach position\n\n        # Lift swing foot to step height\n        for i in range(20):  # 0.2 seconds at 100Hz\n            t = i / 20.0\n            lift_amount = 0.15 * np.sin(np.pi * t)  # Lift to step height\n            joints = self.calculate_stance_joints()\n            # Apply lift to swing foot (simplified)\n            pattern.append(joints)\n\n        # Move swing foot forward over step\n        for i in range(20):\n            t = i / 20.0\n            forward_amount = 0.3 * t  # Move forward by step depth\n            joints = self.calculate_stance_joints()\n            pattern.append(joints)\n\n        # Lower swing foot to next step\n        for i in range(20):\n            t = i / 20.0\n            lower_amount = 0.15 * (1 - np.cos(np.pi * t))  # Smooth lowering\n            joints = self.calculate_stance_joints()\n            pattern.append(joints)\n\n        return pattern\n\n    def adapt_to_terrain(self, terrain_analysis):\n        \"\"\"Adapt gait based on terrain analysis from perception system\"\"\"\n        # Analyze terrain characteristics\n        slope = terrain_analysis.get('slope', 0)\n        step_height = terrain_analysis.get('step_height', 0)\n        surface_roughness = terrain_analysis.get('roughness', 0)\n        obstacles = terrain_analysis.get('obstacles', [])\n\n        # Select appropriate gait based on terrain\n        if slope > 15:  # Steep incline\n            self.switch_gait(\"crawling\")\n        elif step_height > 0.1:  # Significant height changes\n            # Use special climbing gait\n            pass\n        elif surface_roughness > 0.5:  # Very rough terrain\n            self.switch_gait(\"crawling\")\n            self.reduce_step_length(0.5)\n        else:\n            self.switch_gait(\"walking\")\n\n        # Adjust parameters based on terrain\n        if obstacles:\n            self.enable_obstacle_aware_navigation(obstacles)\n```\n\n## Balance Recovery Behaviors\n\n### Fall Prevention and Recovery\n\n```cpp\nclass BalanceRecoverySystem\n{\npublic:\n    BalanceRecoverySystem() {\n        state_ = BALANCED;\n        recovery_threshold_ = 0.3;  // ZMP outside support polygon threshold\n        fall_threshold_ = 0.5;      // Critical imbalance threshold\n    }\n\n    BalanceState assessBalance(const RobotState& state)\n    {\n        // Calculate ZMP and compare to support polygon\n        auto zmp = calculateZMP(state);\n        auto support_polygon = calculateSupportPolygon(state);\n\n        if (isOutsideSupportPolygon(zmp, support_polygon)) {\n            double distance = distanceToSupportPolygon(zmp, support_polygon);\n\n            if (distance > fall_threshold_) {\n                state_ = FALLING;\n                return state_;\n            } else if (distance > recovery_threshold_) {\n                state_ = UNBALANCED;\n                initiateRecovery(state);\n                return state_;\n            }\n        }\n\n        state_ = BALANCED;\n        return state_;\n    }\n\n    void initiateRecovery(const RobotState& state)\n    {\n        // Choose appropriate recovery action based on situation\n        if (fabs(state.com_velocity.z) > 0.5) {\n            // Falling - use protective landing\n            executeProtectiveLanding(state);\n        } else if (state.foot_contacts[LEFT_FOOT] && state.foot_contacts[RIGHT_FOOT]) {\n            // Stable stance - use ankle strategy\n            executeAnkleStrategy(state);\n        } else {\n            // Single support - use hip/stepping strategy\n            executeHipSteppingStrategy(state);\n        }\n    }\n\n    void executeAnkleStrategy(const RobotState& state)\n    {\n        // Ankle strategy: use ankle torques to shift CoM back to support\n        double x_error = calculateZMPErrors(state).x;\n        double y_error = calculateZMPErrors(state).y;\n\n        // Simple PD control on ankle joints\n        double ankle_roll_command = -kp_ankle_roll_ * x_error - kd_ankle_roll_ * state.ankle_velocities[ROLL];\n        double ankle_pitch_command = -kp_ankle_pitch_ * y_error - kd_ankle_pitch_ * state.ankle_velocities[PITCH];\n\n        // Apply commands\n        setAnkleTorques(ankle_roll_command, ankle_pitch_command);\n    }\n\n    void executeHipSteppingStrategy(const RobotState& state)\n    {\n        // Hip strategy: use hip torques and stepping to recover balance\n        auto zmp_error = calculateZMPErrors(state);\n\n        // Decide if to step or use hip torques based on severity\n        if (fabs(zmp_error.x) > 0.2 || fabs(zmp_error.y) > 0.2) {\n            // Severe imbalance - plan emergency step\n            planEmergencyStep(state, zmp_error);\n        } else {\n            // Moderate imbalance - use hip torques\n            executeHipStrategy(state, zmp_error);\n        }\n    }\n\nprivate:\n    BalanceState state_;\n    double recovery_threshold_;\n    double fall_threshold_;\n\n    // Control gains\n    double kp_ankle_roll_ = 50.0;\n    double kd_ankle_roll_ = 10.0;\n    double kp_ankle_pitch_ = 50.0;\n    double kd_ankle_pitch_ = 10.0;\n    double kp_hip_roll_ = 100.0;\n    double kd_hip_roll_ = 20.0;\n    double kp_hip_pitch_ = 100.0;\n    double kd_hip_pitch_ = 20.0;\n};\n```\n\n## Performance Evaluation\n\n### Metrics for Humanoid Locomotion\n\n```python\nclass LocomotionEvaluator:\n    def __init__(self):\n        self.metrics = {\n            'stability': [],\n            'efficiency': [],\n            'balance': [],\n            'trajectory_accuracy': [],\n            'energy_consumption': []\n        }\n\n    def evaluate_locomotion_performance(self, robot_state, reference_trajectory):\n        \"\"\"Evaluate humanoid locomotion performance\"\"\"\n\n        # Stability metrics\n        zmp_stability = self.calculate_zmp_stability(robot_state)\n        self.metrics['stability'].append(zmp_stability)\n\n        # Balance metrics\n        com_deviation = self.calculate_com_deviation(robot_state)\n        self.metrics['balance'].append(com_deviation)\n\n        # Efficiency metrics\n        energy_used = self.calculate_energy_consumption(robot_state)\n        self.metrics['efficiency'].append(energy_used)\n\n        # Trajectory tracking accuracy\n        tracking_error = self.calculate_trajectory_error(robot_state, reference_trajectory)\n        self.metrics['trajectory_accuracy'].append(tracking_error)\n\n        return {\n            'stability_score': self.calculate_average('stability'),\n            'balance_score': self.calculate_average('balance'),\n            'efficiency_score': self.calculate_average('efficiency'),\n            'accuracy_score': self.calculate_average('trajectory_accuracy')\n        }\n\n    def calculate_zmp_stability(self, state):\n        \"\"\"Calculate ZMP-based stability metric\"\"\"\n        zmp = state['zmp']\n        support_polygon = state['support_polygon']\n\n        # Calculate distance from ZMP to edge of support polygon\n        distance_to_edge = self.distance_to_polygon_edge(zmp, support_polygon)\n\n        # Normalize by support polygon area\n        support_area = self.calculate_polygon_area(support_polygon)\n        stability_metric = distance_to_edge / np.sqrt(support_area)\n\n        return stability_metric\n\n    def calculate_energy_consumption(self, state):\n        \"\"\"Calculate energy consumption based on joint torques and velocities\"\"\"\n        total_energy = 0.0\n\n        for i, (torque, velocity) in enumerate(zip(state['joint_torques'], state['joint_velocities'])):\n            # Energy = integral of torque * velocity over time\n            instantaneous_power = abs(torque * velocity)\n            total_energy += instantaneous_power * self.dt  # dt = control timestep\n\n        return total_energy\n\n    def calculate_trajectory_error(self, robot_state, reference_trajectory):\n        \"\"\"Calculate error between robot position and reference trajectory\"\"\"\n        robot_pos = np.array([robot_state['position']['x'], robot_state['position']['y']])\n\n        # Find closest point on reference trajectory\n        min_distance = float('inf')\n        for ref_point in reference_trajectory:\n            dist = np.linalg.norm(robot_pos - np.array([ref_point['x'], ref_point['y']]))\n            min_distance = min(min_distance, dist)\n\n        return min_distance\n```\n\n## Troubleshooting Common Issues\n\n### 1. Instability and Falls\n**Problem**: Robot falls during walking\n**Solutions**:\n- Check CoM height parameter in controller\n- Verify joint limits and physical properties\n- Adjust balance controller gains\n- Ensure proper initial standing pose\n\n### 2. Joint Limit Violations\n**Problem**: Joints reaching limits during walking\n**Solutions**:\n- Check URDF joint limits\n- Adjust gait parameters (step height/length)\n- Implement joint limit checking in controller\n- Use trajectory optimization to respect limits\n\n### 3. ZMP Outside Support Polygon\n**Problem**: Balance errors during locomotion\n**Solutions**:\n- Reduce walking speed\n- Increase step width\n- Adjust CoM height\n- Improve sensor feedback quality\n\n### 4. Phase Synchronization Issues\n**Problem**: Legs getting out of sync during walking\n**Solutions**:\n- Verify gait phase calculation\n- Check timing synchronization\n- Ensure proper step sequencing\n- Use phase oscillator models\n\n## Best Practices\n\n### 1. Gradual Complexity Increase\n- Start with standing balance control\n- Progress to simple stepping\n- Add forward walking\n- Introduce turning and complex maneuvers\n\n### 2. Safety First Approach\n- Implement soft limits and safety controllers\n- Use simulation extensively before real robot testing\n- Have emergency stop mechanisms\n- Monitor robot state continuously\n\n### 3. Parameter Tuning\n- Start with conservative parameters\n- Gradually increase aggressiveness\n- Test on various terrains\n- Validate with multiple scenarios\n\n### 4. Sensor Fusion\n- Combine multiple sensor inputs\n- Use IMU for orientation feedback\n- Implement sensor validation\n- Handle sensor failures gracefully\n\n## Exercise\n\nCreate a complete humanoid locomotion system that includes:\n\n1. Implement a ZMP-based balance controller\n2. Create a walking pattern generator with adjustable parameters\n3. Integrate with Isaac Sim for humanoid robot simulation\n4. Implement gait switching capabilities (walking, crawling, stair climbing)\n5. Add balance recovery behaviors for disturbance rejection\n6. Create a navigation system that uses the locomotion controller\n7. Evaluate the system's performance with stability and efficiency metrics\n\nTest your system with various scenarios including:\n- Straight line walking\n- Turning maneuvers\n- Walking on uneven terrain\n- Disturbance rejection (external pushes)\n- Stair climbing (simulation)\n- Obstacle avoidance while walking\n\nEvaluate the system's performance using the metrics discussed in this section.",
    "path": "module-4-vla\\humanoid-locomotion-control.md",
    "description": ""
  },
  "module-4-vla\\index": {
    "title": "Module 4 VLA",
    "content": "\n# Module 4: Vision-Language-Action (VLA) Tasks\n\nWelcome to Module 4 of the Physical AI & Humanoid Robotics book! This module focuses on Vision-Language-Action (VLA) systems that enable humanoid robots to understand natural language commands and execute complex physical tasks in the real world.\n\n## Overview\n\nIn this module, you'll learn to integrate vision, language, and action systems to create robots that can understand and respond to natural language commands. This represents the cutting edge of AI-powered robotics, combining computer vision, natural language processing, and robotic control.\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n\n- Integrate Whisper for speech-to-text processing in robotic systems\n- Connect Large Language Models (LLMs) for natural language understanding and planning\n- Implement multimodal perception systems that combine vision and language\n- Create voice-to-action pipelines for humanoid robots\n- Design action planning systems that translate language commands to robotic actions\n- Implement multimodal interaction between vision, language, and robotic control\n- Build end-to-end VLA systems for complex task execution\n\n## Prerequisites\n\nBefore starting this module, ensure you have:\n\n- Completed Modules 1-3 (ROS 2, Digital Twin, AI Perception)\n- Basic understanding of neural networks and deep learning\n- Access to OpenAI API key or local LLM (e.g., Llama models)\n- Microphone and audio processing capabilities\n- Understanding of computer vision concepts from Module 3\n\n## Module Structure\n\nThis module is organized into the following sections:\n\n1. **Introduction to VLA** - Core concepts and architecture\n2. **Whisper Speech Processing** - Speech-to-text implementation\n3. **LLM Planning** - Language understanding and action planning\n4. **Multimodal Perception** - Combining vision and language\n5. **Voice-to-Action Pipeline** - Complete integration\n6. **Practical Exercises** - Hands-on VLA applications\n7. **System Integration** - Full VLA system implementation\n\n## Vision-Language-Action Architecture\n\nThe VLA system combines three key components:\n\n```\nVoice Command\n     \nSpeech Recognition (Whisper)\n     \nNatural Language Understanding (LLM)\n     \nAction Planning & Reasoning (LLM)\n     \nAction Execution (Robot Control)\n     \nPhysical Action in Environment\n```\n\n## Key Technologies Covered\n\n### Speech Processing\n- **Whisper**: OpenAI's speech recognition model\n- **Audio preprocessing**: Noise reduction, normalization\n- **Real-time processing**: Streaming audio processing\n- **Localization**: Multi-language support\n\n### Language Models\n- **OpenAI GPT models**: For language understanding and planning\n- **Open-source alternatives**: Llama, Mistral, or other local models\n- **Prompt engineering**: Techniques for robotic task planning\n- **Function calling**: Connecting LLMs to robotic APIs\n\n### Vision Integration\n- **Multimodal models**: CLIP, BLIP for vision-language understanding\n- **Object detection**: Connecting vision to language understanding\n- **Scene understanding**: Interpreting visual context for commands\n- **Visual grounding**: Connecting language to visual elements\n\n## Integration with Previous Modules\n\nThis module builds on all previous modules by:\n- Using ROS 2 communication patterns from Module 1\n- Leveraging digital twin simulation from Module 2\n- Incorporating perception systems from Module 3\n- Creating the ultimate integration of all components\n- Preparing for the capstone project in Module 5\n\n## VLA Pipeline Architecture\n\nThe complete VLA pipeline includes:\n\n1. **Input Processing**: Audio capture and preprocessing\n2. **Speech Recognition**: Converting speech to text\n3. **Language Understanding**: Parsing commands and intent\n4. **Perception Integration**: Combining vision and language\n5. **Action Planning**: Generating robot action sequences\n6. **Execution**: Sending commands to robot control systems\n7. **Feedback**: Processing results and reporting to user\n\n## Next Steps\n\nBegin with the Whisper speech processing section to establish your audio input pipeline, then proceed through the sections in order to build up your understanding of the complete VLA system. Each section builds on the previous one, so follow the sequence for the best learning experience.",
    "path": "module-4-vla\\index.md",
    "description": ""
  },
  "module-4-vla\\isaac-ros-integration": {
    "title": "module-4-vla\\isaac-ros-integration",
    "content": "# Isaac ROS Integration\n\nIsaac ROS is NVIDIA's collection of hardware-accelerated perception and navigation packages that bridge the gap between NVIDIA's GPU-accelerated AI capabilities and the ROS 2 robotics framework. This section covers how to integrate Isaac ROS packages with humanoid robotics systems.\n\n## Introduction to Isaac ROS\n\nIsaac ROS provides optimized implementations of common robotics algorithms leveraging NVIDIA's GPU acceleration:\n\n- **Hardware Acceleration**: Leverages TensorRT and CUDA for accelerated inference\n- **ROS 2 Compatibility**: Full integration with ROS 2 ecosystem\n- **Modular Design**: Standalone packages that can be combined\n- **Production Ready**: Optimized for real-world deployment\n\n### Isaac ROS Package Categories\n\n1. **Perception**: Object detection, segmentation, SLAM\n2. **Navigation**: Path planning, localization, obstacle avoidance\n3. **Manipulation**: Grasping, trajectory planning\n4. **Simulation**: Isaac Sim integration\n\n## Isaac ROS Perception Integration\n\n### Isaac ROS Visual SLAM\n\nThe Isaac ROS Visual SLAM package provides hardware-accelerated visual SLAM capabilities:\n\n```yaml\n# Isaac ROS Visual SLAM configuration\nisaac_ros_visual_slam:\n  ros__parameters:\n    enable_occupancy_grid: true\n    enable_diagnostics: false\n    occupancy_grid_resolution: 0.05\n    frame_id: \"oak-d_frame\"\n    base_frame: \"base_link\"\n    odom_frame: \"odom\"\n    enable_slam_visualization: true\n    enable_landmarks_view: true\n    enable_observations_view: true\n    calibration_file: \"/tmp/calibration.json\"\n    rescale_threshold: 2.0\n```\n\n### Isaac ROS DetectNet\n\nObject detection with NVIDIA's DetectNet:\n\n```yaml\n# Isaac ROS DetectNet configuration\nisaac_ros_detectnet:\n  ros__parameters:\n    input_topic: \"/camera/image_rect_color\"\n    output_topic: \"/detectnet/detections\"\n    model_name: \"ssd_mobilenet_v2_coco\"\n    confidence_threshold: 0.7\n    enable_bbox: true\n    enable_mask: false\n    mask_overlay_alpha: 0.5\n```\n\n### Isaac ROS Bi3D\n\n3D segmentation and depth estimation:\n\n```yaml\n# Isaac ROS Bi3D configuration\nisaac_ros_bi3d:\n  ros__parameters:\n    input_topic: \"/camera/image_rect_color\"\n    output_topic: \"/bi3d/segmentation\"\n    model_name: \"Bi3D_Stereo\"\n    max_disparity: 64.0\n    disparity_shift: 0.0\n    enable_depth_viz: true\n```\n\n## Isaac ROS Navigation Integration\n\n### Isaac ROS Navigation Stack\n\n```yaml\n# Isaac ROS Navigation configuration\nisaac_ros_navigation:\n  ros__parameters:\n    # Global planner settings\n    global_planner:\n      plugin: \"nav2_navfn_planner/NavfnPlanner\"\n      tolerance: 0.5\n      use_astar: false\n      allow_unknown: true\n\n    # Local planner settings\n    local_planner:\n      plugin: \"nav2_dwb_controller/DWBLocalPlanner\"\n      sim_time: 1.7\n      linear_vel_limits: [-0.5, 0.5, 2.5]\n      angular_vel_limits: [-1.0, 1.0, 3.2]\n      linear_accel_limits: [-2.5, 2.5]\n      angular_accel_limits: [-3.2, 3.2]\n\n    # Costmap settings\n    local_costmap:\n      plugins: [\"obstacle_layer\", \"inflation_layer\"]\n      obstacle_layer:\n        enabled: true\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: true\n          marking: true\n          data_type: \"LaserScan\"\n      inflation_layer:\n        enabled: true\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n```\n\n## Isaac ROS Package Architecture\n\n### Core Isaac ROS Components\n\n```cpp\n// Isaac ROS Node Base Class\n#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_nitros/nitros_node.hpp>\n#include <isaac_ros_nitros/types/type_adapter_nitros_context.hpp>\n\nclass IsaacROSBaseNode : public nitros::NitrosNode\n{\npublic:\n    explicit IsaacROSBaseNode(const std::string & name)\n    : NitrosNode(\n        name,\n        \"\",\n        {\n          .type = nitros::SerializerType::kJson,\n          .transport_type = nitros::TransportType::kUDP\n        },\n        {\n          .enable = true,\n          .path = \"/tmp/isaac_ros_logs\"\n        }\n      )\n    {\n        registerSupportedType<nitros::NitrosPublisher, nitros::MsgType::kRgb8, nitros::TransportType::kTCP>();\n        registerSupportedType<nitros::NitrosPublisher, nitros::MsgType::kBgr8, nitros::TransportType::kTCP>();\n        registerSupportedType<nitros::NitrosPublisher, nitros::MsgType::kPointCloud2, nitros::TransportType::kUDP>();\n    }\n\nprotected:\n    void registerSupportedType(\n        const nitros::TransportType transport_type,\n        const nitros::MsgType msg_type,\n        const nitros::SerializerType serializer_type = nitros::SerializerType::kJson)\n    {\n        // Register supported message types for Nitros transport\n    }\n\n    void setupTransport(\n        const std::string & transport_name,\n        const nitros::TransportType transport_type,\n        const nitros::MsgType msg_type)\n    {\n        // Setup Nitros transport for optimized message passing\n    }\n};\n```\n\n### Isaac ROS Visual SLAM Node Implementation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass IsaacVSLAMNode : public rclcpp::Node\n{\npublic:\n    IsaacVSLAMNode() : Node(\"isaac_vslam_node\")\n    {\n        // Create subscription for stereo images\n        left_image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"left/image_rect\", 10,\n            std::bind(&IsaacVSLAMNode::leftImageCallback, this, std::placeholders::_1)\n        );\n\n        right_image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"right/image_rect\", 10,\n            std::bind(&IsaacVSLAMNode::rightImageCallback, this, std::placeholders::_1)\n        );\n\n        // Publishers\n        pose_pub_ = this->create_publisher<geometry_msgs::msg::PoseStamped>(\"visual_slam/pose\", 10);\n        map_pub_ = this->create_publisher<nav_msgs::msg::OccupancyGrid>(\"visual_slam/grid_map\", 10);\n\n        // Initialize VSLAM algorithm (would interface with Isaac's optimized VSLAM)\n        initializeVSLAM();\n\n        RCLCPP_INFO(this->get_logger(), \"Isaac VSLAM Node initialized\");\n    }\n\nprivate:\n    void leftImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        if (!has_right_image_) {\n            latest_left_image_ = msg;\n            return;\n        }\n\n        // Process stereo pair for VSLAM\n        processStereoImages(latest_left_image_, latest_right_image_);\n\n        // Clear flags\n        has_right_image_ = false;\n    }\n\n    void rightImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        if (!has_left_image_) {\n            latest_right_image_ = msg;\n            return;\n        }\n\n        // Process stereo pair for VSLAM\n        processStereoImages(latest_left_image_, msg);\n\n        // Clear flags\n        has_left_image_ = false;\n    }\n\n    void processStereoImages(\n        const sensor_msgs::msg::Image::SharedPtr left_msg,\n        const sensor_msgs::msg::Image::SharedPtr right_msg)\n    {\n        try {\n            // Convert ROS images to OpenCV\n            cv_bridge::CvImagePtr left_cv_ptr = cv_bridge::toCvCopy(left_msg, \"bgr8\");\n            cv_bridge::CvImagePtr right_cv_ptr = cv_bridge::toCvCopy(right_msg, \"bgr8\");\n\n            // Perform stereo processing using Isaac's optimized algorithms\n            auto pose_estimate = runVSLAM(left_cv_ptr->image, right_cv_ptr->image);\n\n            if (pose_estimate.has_value()) {\n                // Publish pose estimate\n                auto pose_msg = geometry_msgs::msg::PoseStamped();\n                pose_msg.header = left_msg->header;\n                pose_msg.pose = pose_estimate.value();\n                pose_pub_->publish(pose_msg);\n\n                // Update and publish map\n                auto occupancy_grid = buildOccupancyGrid(pose_msg.pose);\n                map_msg.header = left_msg->header;\n                map_pub_->publish(occupancy_grid);\n            }\n\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n        }\n    }\n\n    std::optional<geometry_msgs::msg::Pose> runVSLAM(const cv::Mat& left_image, const cv::Mat& right_image)\n    {\n        // This would interface with Isaac's optimized VSLAM implementation\n        // For this example, we'll simulate the output\n\n        static double sim_x = 0.0, sim_y = 0.0, sim_theta = 0.0;\n\n        // Simulate pose update based on visual odometry\n        // In real implementation, this would call Isaac's VSLAM algorithms\n        if (!first_frame_) {\n            // Calculate displacement from previous frame using feature matching\n            double dx = 0.01;  // Simulated forward movement\n            double dtheta = 0.001;  // Simulated rotation\n\n            sim_x += dx * cos(sim_theta) - dy * sin(sim_theta);\n            sim_y += dx * sin(sim_theta) + dy * cos(sim_theta);\n            sim_theta += dtheta;\n\n            // Apply noise to simulate real sensor imperfections\n            sim_x += (static_cast<double>(rand()) / RAND_MAX - 0.5) * 0.001;\n            sim_y += (static_cast<double>(rand()) / RAND_MAX - 0.5) * 0.001;\n        }\n\n        first_frame_ = false;\n\n        geometry_msgs::msg::Pose pose;\n        pose.position.x = sim_x;\n        pose.position.y = sim_y;\n        pose.position.z = 0.0;\n\n        // Convert angle to quaternion\n        tf2::Quaternion q;\n        q.setRPY(0, 0, sim_theta);\n        pose.orientation = tf2::toMsg(q);\n\n        return pose;\n    }\n\n    nav_msgs::msg::OccupancyGrid buildOccupancyGrid(const geometry_msgs::msg::Pose& robot_pose)\n    {\n        nav_msgs::msg::OccupancyGrid grid;\n        grid.info.resolution = 0.05;  // 5cm resolution\n        grid.info.width = 200;        // 10m x 10m map\n        grid.info.height = 200;\n        grid.info.origin.position.x = robot_pose.position.x - 5.0;  // Center map around robot\n        grid.info.origin.position.y = robot_pose.position.y - 5.0;\n        grid.info.origin.position.z = 0.0;\n        grid.info.origin.orientation.w = 1.0;\n\n        // Initialize with unknown (-1)\n        grid.data.resize(grid.info.width * grid.info.height, -1);\n\n        // In a real implementation, this would populate the grid based on\n        // SLAM map building from visual features and depth information\n\n        return grid;\n    }\n\n    void initializeVSLAM()\n    {\n        // Initialize Isaac's VSLAM algorithm with optimized parameters\n        // This would typically involve loading pre-trained models and\n        // setting up GPU acceleration\n\n        first_frame_ = true;\n        has_left_image_ = false;\n        has_right_image_ = false;\n    }\n\n    // Subscriptions\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr left_image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr right_image_sub_;\n\n    // Publishers\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_pub_;\n    rclcpp::Publisher<nav_msgs::msg::OccupancyGrid>::SharedPtr map_pub_;\n\n    // Storage for stereo pair synchronization\n    sensor_msgs::msg::Image::SharedPtr latest_left_image_;\n    sensor_msgs::msg::Image::SharedPtr latest_right_image_;\n    bool has_left_image_ = false;\n    bool has_right_image_ = false;\n\n    // VSLAM state\n    bool first_frame_;\n    double last_timestamp_;\n};\n```\n\n## Isaac ROS Hardware Acceleration\n\n### TensorRT Integration\n\n```cpp\n#include <NvInfer.h>\n#include <cuda_runtime_api.h>\n#include <rclcpp/rclcpp.hpp>\n\nclass TensorRTInferenceNode : public rclcpp::Node\n{\npublic:\n    TensorRTInferenceNode() : Node(\"tensorrt_inference_node\")\n    {\n        // Initialize TensorRT engine\n        initializeTensorRTEngine();\n\n        // Create subscription for inference input\n        input_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"inference_input\", 10,\n            std::bind(&TensorRTInferenceNode::inferenceCallback, this, std::placeholders::_1)\n        );\n\n        // Create publisher for inference output\n        output_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            \"inference_output\", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"TensorRT Inference Node initialized\");\n    }\n\nprivate:\n    void initializeTensorRTEngine()\n    {\n        // Create TensorRT runtime\n        trt_runtime_ = nvinfer1::createInferRuntime(trt_logger_);\n\n        // Load serialized engine from file\n        std::ifstream engine_file(\"model.plan\", std::ios::binary);\n        if (!engine_file) {\n            throw std::runtime_error(\"Could not open engine file\");\n        }\n\n        // Read engine data\n        std::vector<char> engine_data;\n        engine_data.assign(\n            std::istreambuf_iterator<char>(engine_file),\n            std::istreambuf_iterator<char>()\n        );\n\n        // Create execution engine\n        trt_engine_ = std::shared_ptr<nvinfer1::ICudaEngine>(\n            trt_runtime_->deserializeCudaEngine(engine_data.data(), engine_data.size()),\n            [this](nvinfer1::ICudaEngine* engine) {\n                if (engine) engine->destroy();\n            }\n        );\n\n        if (!trt_engine_) {\n            throw std::runtime_error(\"Could not create TensorRT engine\");\n        }\n\n        // Create execution context\n        trt_context_ = std::shared_ptr<nvinfer1::IExecutionContext>(\n            trt_engine_->createExecutionContext(),\n            [](nvinfer1::IExecutionContext* context) {\n                if (context) context->destroy();\n            }\n        );\n\n        // Allocate GPU memory for inputs/outputs\n        allocateGPUBuffers();\n    }\n\n    void inferenceCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        try {\n            // Preprocess input image\n            cv_bridge::CvImagePtr cv_ptr = cv_bridge::toCvCopy(msg, \"bgr8\");\n            auto preprocessed = preprocessImage(cv_ptr->image);\n\n            // Copy input to GPU\n            cudaMemcpy(input_buffer_, preprocessed.ptr(), input_size_, cudaMemcpyHostToDevice);\n\n            // Run inference\n            bool success = trt_context_->enqueueV2(\n                gpu_buffers_.data(),\n                cuda_stream_,\n                nullptr\n            );\n\n            if (!success) {\n                RCLCPP_ERROR(this->get_logger(), \"TensorRT inference failed\");\n                return;\n            }\n\n            // Copy output from GPU\n            cudaMemcpy(output_buffer_, gpu_buffers_[output_binding_index_], output_size_, cudaMemcpyDeviceToHost);\n\n            // Post-process output\n            auto result = postprocessOutput(output_buffer_);\n\n            // Publish result\n            publishResult(result, msg->header);\n\n        } catch (const std::exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"Inference error: %s\", e.what());\n        }\n    }\n\n    std::vector<float> preprocessImage(const cv::Mat& image)\n    {\n        // Resize and normalize image for model input\n        cv::Mat resized;\n        cv::resize(image, resized, cv::Size(input_width_, input_height_));\n\n        // Convert BGR to RGB and normalize to [0,1]\n        cv::Mat normalized;\n        resized.convertTo(normalized, CV_32F, 1.0/255.0);\n\n        // Rearrange from HWC to CHW (channel-first format)\n        std::vector<cv::Mat> channels;\n        cv::split(normalized, channels);\n\n        std::vector<float> input_data;\n        input_data.reserve(input_size_);\n        for (const auto& channel : channels) {\n            input_data.insert(input_data.end(), channel.begin<float>(), channel.end<float>());\n        }\n\n        return input_data;\n    }\n\n    void allocateGPUBuffers()\n    {\n        // Get binding information\n        int num_bindings = trt_engine_->getNbBindings();\n        gpu_buffers_.resize(num_bindings);\n\n        for (int i = 0; i < num_bindings; ++i) {\n            auto dims = trt_engine_->getBindingDimensions(i);\n            size_t binding_size = getBindingSize(dims, trt_engine_->getBindingDataType(i));\n\n            if (trt_engine_->bindingIsInput(i)) {\n                input_size_ = binding_size;\n                input_binding_index_ = i;\n                cudaMalloc(&gpu_buffers_[i], binding_size);\n            } else {\n                output_size_ = binding_size;\n                output_binding_index_ = i;\n                cudaMalloc(&gpu_buffers_[i], binding_size);\n            }\n        }\n\n        // Create CUDA stream\n        cudaStreamCreate(&cuda_stream_);\n\n        // Allocate host buffers for async memory transfer\n        cudaMallocHost(&input_buffer_, input_size_);\n        cudaMallocHost(&output_buffer_, output_size_);\n    }\n\n    size_t getBindingSize(const nvinfer1::Dims& dims, nvinfer1::DataType dtype)\n    {\n        size_t size = 1;\n        for (int i = 0; i < dims.nbDims; ++i) {\n            size *= dims.d[i];\n        }\n\n        size_t element_size = 0;\n        switch (dtype) {\n            case nvinfer1::DataType::kFLOAT:\n                element_size = sizeof(float);\n                break;\n            case nvinfer1::DataType::kHALF:\n                element_size = sizeof(half);\n                break;\n            case nvinfer1::DataType::kINT8:\n                element_size = sizeof(int8_t);\n                break;\n            case nvinfer1::DataType::kINT32:\n                element_size = sizeof(int32_t);\n                break;\n        }\n\n        return size * element_size;\n    }\n\n    // TensorRT components\n    nvinfer1::IRuntime* trt_runtime_;\n    std::shared_ptr<nvinfer1::ICudaEngine> trt_engine_;\n    std::shared_ptr<nvinfer1::IExecutionContext> trt_context_;\n    nvinfer1::ILogger trt_logger_;\n\n    // CUDA components\n    cudaStream_t cuda_stream_;\n    std::vector<void*> gpu_buffers_;\n    void* input_buffer_;\n    void* output_buffer_;\n    size_t input_size_;\n    size_t output_size_;\n    int input_binding_index_ = -1;\n    int output_binding_index_ = -1;\n\n    // Model parameters\n    int input_width_ = 224;\n    int input_height_ = 224;\n\n    // Subscriptions and publishers\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr input_sub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr output_pub_;\n};\n```\n\n## Isaac ROS Manipulation Integration\n\n### Isaac ROS Manipulation Stack\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <sensor_msgs/msg/joint_state.hpp>\n#include <control_msgs/msg/joint_trajectory_controller_state.hpp>\n#include <moveit_msgs/msg/planning_scene.h>\n#include <moveit_msgs/srv/get_planning_scene.h>\n\nclass IsaacManipulationController : public rclcpp::Node\n{\npublic:\n    IsaacManipulationController() : Node(\"isaac_manipulation_controller\")\n    {\n        // Initialize Isaac-specific manipulation components\n        initializeManipulationPipeline();\n\n        // Subscriptions\n        target_pose_sub_ = this->create_subscription<geometry_msgs::msg::PoseStamped>(\n            \"manipulation_target\", 10,\n            std::bind(&IsaacManipulationController::targetPoseCallback, this, std::placeholders::_1)\n        );\n\n        joint_state_sub_ = this->create_subscription<sensor_msgs::msg::JointState>(\n            \"joint_states\", 10,\n            std::bind(&IsaacManipulationController::jointStateCallback, this, std::placeholders::_1)\n        );\n\n        // Publishers\n        trajectory_pub_ = this->create_publisher<trajectory_msgs::msg::JointTrajectory>(\n            \"joint_trajectory_controller/joint_trajectory\", 10\n        );\n\n        planning_scene_pub_ = this->create_publisher<moveit_msgs::msg::PlanningScene>(\n            \"planning_scene\", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Isaac Manipulation Controller initialized\");\n    }\n\nprivate:\n    void targetPoseCallback(const geometry_msgs::msg::PoseStamped::SharedPtr msg)\n    {\n        // Plan and execute manipulation to target pose\n        auto trajectory = planArmTrajectoryToPose(msg->pose);\n\n        if (!trajectory.points.empty()) {\n            trajectory_pub_->publish(trajectory);\n        } else {\n            RCLCPP_ERROR(this->get_logger(), \"Failed to plan trajectory to target pose\");\n        }\n    }\n\n    trajectory_msgs::msg::JointTrajectory planArmTrajectoryToPose(const geometry_msgs::msg::Pose& target_pose)\n    {\n        trajectory_msgs::msg::JointTrajectory trajectory;\n\n        // In a real implementation, this would use Isaac's optimized motion planning\n        // For this example, we'll create a simple trajectory\n\n        // Set joint names\n        trajectory.joint_names = {\"joint1\", \"joint2\", \"joint3\", \"joint4\", \"joint5\", \"joint6\"};\n\n        // Create trajectory points\n        trajectory.points.resize(10);  // 10 intermediate points\n\n        // Calculate intermediate poses\n        geometry_msgs::msg::Pose start_pose = getCurrentEndEffectorPose();\n\n        for (int i = 0; i <= 10; ++i) {\n            double t = static_cast<double>(i) / 10.0;  // Interpolation factor [0,1]\n\n            trajectory_msgs::msg::JointTrajectoryPoint point;\n            point.positions.resize(6);\n\n            // Linear interpolation between start and target poses\n            geometry_msgs::msg::Pose interpolated_pose;\n            interpolated_pose.position.x = start_pose.position.x + t * (target_pose.position.x - start_pose.position.x);\n            interpolated_pose.position.y = start_pose.position.y + t * (target_pose.position.y - start_pose.position.y);\n            interpolated_pose.position.z = start_pose.position.z + t * (target_pose.position.z - start_pose.position.z);\n\n            // Convert pose to joint positions using inverse kinematics\n            auto joint_positions = inverseKinematics(interpolated_pose);\n\n            point.positions = joint_positions;\n\n            // Calculate time from start\n            point.time_from_start.sec = 0;\n            point.time_from_start.nanosec = static_cast<uint32_t>(i * 100000000);  // 100ms intervals\n\n            trajectory.points[i] = point;\n        }\n\n        return trajectory;\n    }\n\n    std::vector<double> inverseKinematics(const geometry_msgs::msg::Pose& pose)\n    {\n        // This would interface with Isaac's optimized IK solvers\n        // For this example, return a placeholder solution\n        return std::vector<double>(6, 0.0);  // Placeholder joint positions\n    }\n\n    geometry_msgs::msg::Pose getCurrentEndEffectorPose()\n    {\n        // Get current joint positions and calculate FK\n        // This would use Isaac's optimized FK solvers\n        geometry_msgs::msg::Pose pose;\n        pose.position.x = 0.5;  // Placeholder\n        pose.position.y = 0.0;\n        pose.position.z = 0.8;\n        pose.orientation.w = 1.0;\n        return pose;\n    }\n\n    void initializeManipulationPipeline()\n    {\n        // Initialize Isaac's manipulation pipeline with:\n        // - Optimized inverse kinematics solvers\n        // - Collision checking with TensorRT acceleration\n        // - Motion planning with GPU acceleration\n        // - Grasp planning with neural networks\n    }\n\n    rclcpp::Subscription<geometry_msgs::msg::PoseStamped>::SharedPtr target_pose_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::JointState>::SharedPtr joint_state_sub_;\n    rclcpp::Publisher<trajectory_msgs::msg::JointTrajectory>::SharedPtr trajectory_pub_;\n    rclcpp::Publisher<moveit_msgs::msg::PlanningScene>::SharedPtr planning_scene_pub_;\n\n    std::vector<double> current_joint_positions_;\n    bool has_joint_state_ = false;\n};\n```\n\n## Isaac ROS Navigation with Humanoid Robots\n\n### Humanoid-Specific Navigation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav2_msgs/action/navigate_to_pose.hpp>\n#include <rclcpp_action/rclcpp_action.hpp>\n#include <geometry_msgs/msg/pose_stamped.h>\n#include <nav_msgs/msg/path.h>\n#include <tf2_ros/transform_listener.h>\n\nclass HumanoidNavigationController : public rclcpp::Node\n{\npublic:\n    HumanoidNavigationController() : Node(\"humanoid_navigation_controller\")\n    {\n        // Create action client for navigation\n        nav_client_ = rclcpp_action::create_client<nav2_msgs::action::NavigateToPose>(\n            this, \"navigate_to_pose\"\n        );\n\n        // Create TF buffer and listener\n        tf_buffer_ = std::make_shared<tf2_ros::Buffer>(this->get_clock());\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(*tf_buffer_);\n\n        // Timer for periodic navigation updates\n        nav_timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(100),  // 10 Hz\n            std::bind(&HumanoidNavigationController::navigationUpdate, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Humanoid Navigation Controller initialized\");\n    }\n\n    void navigateToPose(double x, double y, double theta)\n    {\n        if (!nav_client_->wait_for_action_server(std::chrono::seconds(5))) {\n            RCLCPP_ERROR(this->get_logger(), \"Navigation action server not available\");\n            return;\n        }\n\n        auto goal = nav2_msgs::action::NavigateToPose::Goal();\n        goal.pose.header.frame_id = \"map\";\n        goal.pose.header.stamp = this->now();\n        goal.pose.pose.position.x = x;\n        goal.pose.pose.position.y = y;\n        goal.pose.pose.position.z = 0.0;\n\n        // Convert angle to quaternion\n        double s = sin(theta/2);\n        double c = cos(theta/2);\n        goal.pose.pose.orientation.x = 0.0;\n        goal.pose.pose.orientation.y = 0.0;\n        goal.pose.pose.orientation.z = s;\n        goal.pose.pose.orientation.w = c;\n\n        auto send_goal_options = rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SendGoalOptions();\n        send_goal_options.result_callback = [this](const GoalHandle::WrappedResult& result) {\n            switch (result.code) {\n                case rclcpp_action::ResultCode::SUCCEEDED:\n                    RCLCPP_INFO(this->get_logger(), \"Navigation succeeded!\");\n                    break;\n                case rclcpp_action::ResultCode::ABORTED:\n                    RCLCPP_ERROR(this->get_logger(), \"Navigation was aborted\");\n                    break;\n                case rclcpp_action::ResultCode::CANCELED:\n                    RCLCPP_ERROR(this->get_logger(), \"Navigation was canceled\");\n                    break;\n                default:\n                    RCLCPP_ERROR(this->get_logger(), \"Unknown result code\");\n                    break;\n            }\n        };\n\n        nav_client_->async_send_goal(goal, send_goal_options);\n    }\n\nprivate:\n    void navigationUpdate()\n    {\n        // Check navigation status and adjust for humanoid-specific requirements\n        // such as balance maintenance, step planning, etc.\n\n        // For humanoid robots, navigation needs to consider:\n        // - Balance and stability during locomotion\n        // - Step planning for bipedal locomotion\n        // - Dynamic stability margins\n        // - Fall prevention mechanisms\n\n        if (isHumanoidOffBalance()) {\n            // Pause navigation and execute balance recovery\n            executeBalanceRecovery();\n        }\n    }\n\n    bool isHumanoidOffBalance()\n    {\n        // Check if humanoid robot is losing balance\n        // This would interface with balance controller\n        return false;  // Placeholder\n    }\n\n    void executeBalanceRecovery()\n    {\n        // Execute balance recovery behavior\n        // This might involve stepping, crouching, or protective movements\n    }\n\n    rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SharedPtr nav_client_;\n    rclcpp::TimerBase::SharedPtr nav_timer_;\n\n    std::shared_ptr<tf2_ros::Buffer> tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n};\n```\n\n## Isaac Sim Integration\n\n### Isaac Sim ROS Bridge\n\n```python\n# Isaac Sim ROS Bridge Configuration\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.ros_bridge import ROSBridge\n\nclass IsaacSimROSIntegration:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_ros_bridge()\n\n    def setup_ros_bridge(self):\n        \"\"\"Setup ROS bridge for Isaac Sim integration\"\"\"\n        # Enable ROS bridge extension\n        omni.kit.commands.execute(\"RosExtEnable\")\n\n        # Create ROS bridge node in Isaac Sim\n        self.ros_bridge = ROSBridge()\n\n        # Configure ROS bridge parameters\n        self.ros_bridge.set_parameter(\"ros_bridge_rate\", 60.0)  # Hz\n        self.ros_bridge.set_parameter(\"enable_tf_publishing\", True)\n        self.ros_bridge.set_parameter(\"enable_odom_publishing\", True)\n\n    def create_robot_with_sensors(self, robot_name, position):\n        \"\"\"Create robot with ROS-enabled sensors in Isaac Sim\"\"\"\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets\")\n            return\n\n        # Add robot to stage\n        robot_path = assets_root_path + f\"/Isaac/Robots/Humanoid/humanoid_instanceable.usd\"\n        add_reference_to_stage(usd_path=robot_path, prim_path=f\"/World/{robot_name}\")\n\n        # Add sensors with ROS publishing enabled\n        from omni.isaac.range_sensor import RotatingLidarPhysX\n        from omni.isaac.sensor import Camera\n\n        # Add camera sensor\n        camera = Camera(\n            prim_path=f\"/World/{robot_name}/base_link/chassis_camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add LIDAR sensor\n        lidar = RotatingLidarPhysX(\n            prim_path=f\"/World/{robot_name}/base_link/lidar\",\n            translation=np.array([0.0, 0.0, 0.5]),\n            config=\"Carter\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Configure ROS publishing for sensors\n        camera.add_ros_bridge_publisher(\n            topic_name=f\"/{robot_name}/camera/image_rect_color\",\n            message_type=\"sensor_msgs/Image\"\n        )\n\n        lidar.add_ros_bridge_publisher(\n            topic_name=f\"/{robot_name}/scan\",\n            message_type=\"sensor_msgs/LaserScan\"\n        )\n\n    def run_simulation_with_ros(self):\n        \"\"\"Run simulation with ROS integration\"\"\"\n        self.world.reset()\n\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Process ROS callbacks\n            if self.ros_bridge:\n                self.ros_bridge.process_messages()\n\n            # Get simulation data\n            robot_pos = self.get_robot_position()\n            sensor_data = self.get_sensor_data()\n\n            # Process with Isaac ROS components\n            processed_data = self.process_with_isaac_ros(robot_pos, sensor_data)\n\n            # Publish results back to ROS\n            self.publish_ros_results(processed_data)\n```\n\n## Isaac ROS Performance Optimization\n\n### Optimized Pipeline Configuration\n\n```yaml\n# Optimized Isaac ROS pipeline configuration\nisaac_ros_pipeline:\n  ros__parameters:\n    # Enable Nitros transport for optimized message passing\n    enable_nitros: true\n    nitros:\n      transport:\n        type: \"tcp\"  # Use TCP for reliability, UDP for speed\n        compression: \"lz4\"  # Enable compression for large messages\n        serialization: \"json\"  # Use JSON for flexibility\n\n    # Performance parameters\n    processing_rate: 30.0  # Hz\n    max_queue_size: 10\n    use_multithreading: true\n    thread_pool_size: 4\n\n    # Memory optimization\n    enable_memory_pool: true\n    memory_pool_size: 100  # Number of pre-allocated message buffers\n    enable_zero_copy_transport: true  # When supported by transport\n\n    # GPU optimization\n    cuda_device_id: 0\n    enable_tensorrt: true\n    tensorrt_precision: \"fp16\"  # Use FP16 for better performance\n    tensorrt_workspace_size: 1073741824  # 1GB workspace\n\n    # Pipeline optimization\n    enable_pipeline_optimization: true\n    pipeline_batch_size: 1  # Adjust based on GPU memory\n    enable_dynamic_batching: false  # Enable for variable input sizes\n```\n\n## Isaac ROS Diagnostic Tools\n\n### Isaac ROS Diagnostic Node\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <diagnostic_updater/diagnostic_updater.h>\n#include <diagnostic_msgs/msg/diagnostic_array.h>\n\nclass IsaacROSDiagnosticNode : public rclcpp::Node\n{\npublic:\n    IsaacROSDiagnosticNode() : Node(\"isaac_ros_diagnostics\")\n    {\n        // Initialize diagnostic updater\n        diag_updater_.setHardwareID(\"isaac_ros_system\");\n\n        // Add diagnostic checks\n        diag_updater_.add(\"Isaac ROS Health\", this, &IsaacROSDiagnosticNode::checkHealth);\n        diag_updater_.add(\"GPU Utilization\", this, &IsaacROSDiagnosticNode::checkGPUUtilization);\n        diag_updater_.add(\"Memory Usage\", this, &IsaacROSDiagnosticNode::checkMemoryUsage);\n        diag_updater_.add(\"Pipeline Throughput\", this, &IsaacROSDiagnosticNode::checkThroughput);\n\n        // Timer for periodic diagnostics\n        diag_timer_ = this->create_wall_timer(\n            std::chrono::seconds(1),\n            std::bind(&IsaacROSDiagnosticNode::updateDiagnostics, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Isaac ROS Diagnostics initialized\");\n    }\n\nprivate:\n    void updateDiagnostics()\n    {\n        diag_updater_.update();\n    }\n\n    void checkHealth(diagnostic_updater::DiagnosticStatusWrapper& stat)\n    {\n        // Check overall system health\n        if (isSystemHealthy()) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::OK, \"Isaac ROS system healthy\");\n        } else {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::ERROR, \"Isaac ROS system error detected\");\n        }\n\n        // Add additional details\n        stat.add(\"Pipeline Status\", getPipelineStatus());\n        stat.add(\"Component Count\", getActiveComponents());\n    }\n\n    void checkGPUUtilization(diagnostic_updater::DiagnosticStatusWrapper& stat)\n    {\n        // Check GPU utilization\n        auto gpu_usage = getGPUUtilization();\n        stat.addf(\"GPU Utilization (%)\", \"%.2f\", gpu_usage.utilization);\n        stat.addf(\"GPU Memory Used (MB)\", \"%.2f\", gpu_usage.memory_used_mb);\n        stat.addf(\"GPU Memory Total (MB)\", \"%.2f\", gpu_usage.memory_total_mb);\n\n        if (gpu_usage.utilization > 90.0) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::WARN, \"High GPU utilization\");\n        } else if (gpu_usage.utilization < 10.0) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::OK, \"GPU utilization nominal\");\n        } else {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::OK, \"GPU utilization normal\");\n        }\n    }\n\n    void checkMemoryUsage(diagnostic_updater::DiagnosticStatusWrapper& stat)\n    {\n        // Check system memory usage\n        auto mem_usage = getMemoryUsage();\n        stat.addf(\"Memory Used (GB)\", \"%.2f\", mem_usage.used_gb);\n        stat.addf(\"Memory Total (GB)\", \"%.2f\", mem_usage.total_gb);\n        stat.addf(\"Memory Percentage\", \"%.2f%%\", mem_usage.percentage);\n\n        if (mem_usage.percentage > 90.0) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::ERROR, \"High memory usage\");\n        } else if (mem_usage.percentage > 75.0) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::WARN, \"Moderate memory usage\");\n        } else {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::OK, \"Memory usage normal\");\n        }\n    }\n\n    void checkThroughput(diagnostic_updater::DiagnosticStatusWrapper& stat)\n    {\n        // Check pipeline throughput\n        auto throughput = getPipelineThroughput();\n        stat.addf(\"Current Rate (Hz)\", \"%.2f\", throughput.current_rate);\n        stat.addf(\"Target Rate (Hz)\", \"%.2f\", throughput.target_rate);\n        stat.addf(\"Latency (ms)\", \"%.2f\", throughput.latency_ms);\n\n        if (throughput.current_rate < 0.8 * throughput.target_rate) {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::WARN, \"Low pipeline throughput\");\n        } else {\n            stat.summary(diagnostic_msgs::msg::DiagnosticStatus::OK, \"Pipeline throughput normal\");\n        }\n    }\n\n    diagnostic_updater::Updater diag_updater_;\n    rclcpp::TimerBase::SharedPtr diag_timer_;\n\n    struct GPUUsage {\n        double utilization;\n        double memory_used_mb;\n        double memory_total_mb;\n    };\n\n    struct MemoryUsage {\n        double used_gb;\n        double total_gb;\n        double percentage;\n    };\n\n    struct ThroughputInfo {\n        double current_rate;\n        double target_rate;\n        double latency_ms;\n    };\n\n    GPUUsage getGPUUtilization() { /* Implementation */ return {0.0, 0.0, 0.0}; }\n    MemoryUsage getMemoryUsage() { /* Implementation */ return {0.0, 0.0, 0.0}; }\n    ThroughputInfo getPipelineThroughput() { /* Implementation */ return {0.0, 0.0, 0.0}; }\n    bool isSystemHealthy() { /* Implementation */ return true; }\n    std::string getPipelineStatus() { return \"normal\"; }\n    int getActiveComponents() { return 5; }\n};\n```\n\n## Best Practices for Isaac ROS Integration\n\n### 1. Performance Optimization\n- Use Nitros for optimized message transport between Isaac ROS components\n- Enable TensorRT acceleration for deep learning models\n- Configure appropriate batch sizes for GPU utilization\n- Use multi-threading for parallel processing\n\n### 2. Resource Management\n- Monitor GPU memory usage to avoid out-of-memory errors\n- Implement proper cleanup of GPU resources\n- Use memory pools for efficient allocation\n- Configure appropriate queue sizes to avoid message drops\n\n### 3. Error Handling\n- Implement robust error handling for GPU operations\n- Provide fallback mechanisms when acceleration fails\n- Monitor component health and restart if necessary\n- Log diagnostic information for debugging\n\n### 4. System Integration\n- Ensure proper timing synchronization between components\n- Use appropriate QoS settings for different message types\n- Validate data integrity between pipeline stages\n- Implement graceful degradation when components fail\n\n## Troubleshooting Common Issues\n\n### 1. GPU Memory Issues\n**Symptoms**: Out-of-memory errors, slow performance\n**Solutions**:\n- Reduce batch sizes in deep learning models\n- Use FP16 precision instead of FP32\n- Optimize model sizes with TensorRT optimization\n- Monitor memory usage with nvidia-smi\n\n### 2. Message Transport Issues\n**Symptoms**: High latency, dropped messages\n**Solutions**:\n- Use appropriate transport protocols (TCP vs UDP)\n- Optimize message queue sizes\n- Enable Nitros transport for Isaac ROS components\n- Check network configuration for ROS bridge\n\n### 3. Synchronization Problems\n**Symptoms**: Misaligned sensor data, incorrect timing\n**Solutions**:\n- Use message filters for time synchronization\n- Implement proper timestamp handling\n- Verify clock synchronization between components\n- Use appropriate buffer sizes for message synchronization\n\nIsaac ROS provides powerful tools for implementing AI-powered robotics applications with hardware acceleration. When properly configured, it can significantly enhance the performance of perception, navigation, and manipulation tasks in humanoid robotics systems.",
    "path": "module-4-vla\\isaac-ros-integration.md",
    "description": ""
  },
  "module-4-vla\\isaac-sim-fundamentals": {
    "title": "module-4-vla\\isaac-sim-fundamentals",
    "content": "# Isaac Sim Fundamentals\n\nIsaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, designed specifically for developing, testing, and validating AI-based robotics applications. This section covers the core concepts and fundamentals of Isaac Sim.\n\n## Introduction to Isaac Sim\n\nIsaac Sim provides a comprehensive simulation environment that includes:\n- **Photorealistic rendering**: Physically accurate rendering with RTX real-time ray tracing\n- **High-fidelity physics**: Accurate rigid body dynamics, soft body simulation, and fluid dynamics\n- **Synthetic data generation**: High-quality training data for AI models\n- **Sensor simulation**: Realistic camera, LIDAR, RADAR, IMU, and other sensor models\n- **AI integration**: Built-in tools for training and testing AI models\n- **ROS/ROS2 bridge**: Seamless integration with ROS 2 ecosystem\n\n## System Requirements and Setup\n\n### Hardware Requirements\n- **GPU**: NVIDIA RTX 3080/4080/4090 or professional GPU (A40, A6000) with 10GB+ VRAM\n- **CPU**: 8+ core processor (Intel i7 / AMD Ryzen 7 or better recommended)\n- **RAM**: 32GB system memory (64GB+ recommended)\n- **Storage**: 50GB+ free space on SSD (100GB+ recommended)\n- **OS**: Ubuntu 22.04 LTS or Windows 10/11 (64-bit)\n\n### Software Requirements\n- **NVIDIA GPU Drivers**: Version 520+ (535+ recommended)\n- **CUDA Toolkit**: Version 11.8 or 12.x\n- **Isaac Sim**: Latest version from NVIDIA Developer portal\n- **Omniverse Launcher**: For managing Isaac Sim installation\n\n## Installation Methods\n\n### Method 1: Omniverse Launcher (Recommended for Beginners)\n1. Download Omniverse Launcher from NVIDIA Developer website\n2. Install the launcher following the on-screen instructions\n3. Launch Omniverse Launcher\n4. Sign in with your NVIDIA Developer account\n5. Navigate to \"Isaac\" section\n6. Click \"Install\" next to Isaac Sim\n7. Choose installation location (default is recommended)\n\n### Method 2: Containerized Installation (Recommended for Production)\n```bash\n# Pull the Isaac Sim Docker image\ndocker pull nvcr.io/nvidia/isaac-sim:4.0.0\n\n# Create directories for persistent data\nmkdir -p ~/docker/isaac-sim/cache/kit\nmkdir -p ~/docker/isaac-sim/assets\nmkdir -p ~/docker/isaac-sim/home\n\n# Run Isaac Sim container with GUI support\nxhost +local:docker\ndocker run --gpus all -it --rm \\\n  --network=host \\\n  --env \"ACCEPT_EULA=Y\" \\\n  --env \"NVIDIA_VISIBLE_DEVICES=all\" \\\n  --env \"NVIDIA_DRIVER_CAPABILITIES=all\" \\\n  --volume /tmp/.X11-unix:/tmp/.X11-unix:rw \\\n  --volume $HOME/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache:rw \\\n  --volume $HOME/docker/isaac-sim/assets:/isaac-sim/assets:rw \\\n  --volume $HOME/docker/isaac-sim/home:/isaac-sim/home:rw \\\n  --volume $HOME/docker/isaac-sim/logs:/isaac-sim/logs:rw \\\n  --volume $HOME/docker/isaac-sim/config:/isaac-sim/config:rw \\\n  nvcr.io/nvidia/isaac-sim:4.0.0\n```\n\n### Method 3: Native Installation (Advanced Users)\n1. Download Isaac Sim from NVIDIA Developer portal\n2. Extract the archive to your preferred location (e.g., `/opt/isaac-sim`)\n3. Set up environment variables:\n\n```bash\n# Add to ~/.bashrc\necho 'export ISAAC_SIM_PATH=/opt/isaac-sim' >> ~/.bashrc\necho 'export PYTHONPATH=$ISAAC_SIM_PATH/python:$PYTHONPATH' >> ~/.bashrc\necho 'export PATH=$ISAAC_SIM_PATH/python/bin:$PATH' >> ~/.bashrc\nsource ~/.bashrc\n```\n\n## Core Concepts\n\n### Universal Scene Description (USD)\nIsaac Sim uses NVIDIA's Universal Scene Description (USD) format for scene representation:\n- **Hierarchical structure**: Organized in a tree-like structure\n- **Layered composition**: Multiple layers that can be composed together\n- **Schema system**: Standardized object types and properties\n- **Variant sets**: Different configurations of the same object\n\n### Omniverse Nucleus\nThe collaborative platform that enables:\n- Multi-user editing of scenes\n- Asset management and sharing\n- Real-time synchronization across users\n\n### Physics Simulation\nIsaac Sim uses PhysX for physics simulation:\n- **Rigid body dynamics**: Accurate collision detection and response\n- **Soft body simulation**: Deformable objects and cloth simulation\n- **Fluid dynamics**: Liquid and gas simulation\n- **Vehicle dynamics**: Realistic vehicle physics\n\n## Isaac Sim Architecture\n\n### Core Components\n```\nIsaac Sim\n Omniverse Kit (Foundation)\n    Physics Engine (PhysX)\n    Rendering Engine (RTX)\n    USD Scene Management\n    UI Framework\n Isaac Extensions\n    Robotics Extensions\n    Perception Extensions\n    Navigation Extensions\n    Manipulation Extensions\n ROS/ROS2 Bridge\n    Message Conversion\n    Service Integration\n    Action Support\n Synthetic Data Generation\n     Ground Truth\n     Sensor Simulation\n     Annotation Tools\n```\n\n### Extensions System\nIsaac Sim uses an extension-based architecture:\n- **Isaac Sim Robotics**: Core robotics functionality\n- **Isaac Sim Navigation**: Navigation and path planning\n- **Isaac Sim Perception**: Computer vision and sensor simulation\n- **Isaac Sim Manipulation**: Grasping and manipulation\n\n## Getting Started with Isaac Sim\n\n### Launching Isaac Sim\n1. From Omniverse Launcher: Click \"Launch\" next to Isaac Sim\n2. From command line (native installation):\n   ```bash\n   cd /opt/isaac-sim\n   ./isaac-sim.sh\n   ```\n\n### Basic Interface\n- **Viewport**: Main 3D scene view\n- **Stage**: Scene hierarchy panel\n- **Property Panel**: Object properties and settings\n- **Extension Manager**: Manage Isaac Sim extensions\n- **Timeline**: Animation and simulation controls\n\n## Creating Your First Scene\n\n### Basic Scene Setup\n1. Create a new stage (File  New Stage)\n2. Add a ground plane (Create  Ground Plane)\n3. Add a simple robot (Window  Extensions  Isaac Sim  Robotics  Carter)\n\n### Adding Sensors\n```python\n# Python API example for adding sensors\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nimport numpy as np\n\n# Initialize the world\nworld = World(stage_units_in_meters=1.0)\n\n# Add a robot\nassets_root_path = get_assets_root_path()\nif assets_root_path is None:\n    print(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\nelse:\n    # Add a robot to the scene\n    robot_path = assets_root_path + \"/Isaac/Robots/Carter/carter_navigate.usd\"\n    add_reference_to_stage(usd_path=robot_path, prim_path=\"/World/Carter\")\n\n    # Add a camera sensor\n    camera = Camera(\n        prim_path=\"/World/Carter/chassis/camera\",\n        frequency=30,\n        resolution=(640, 480)\n    )\n\n    # Add a LIDAR sensor\n    lidar = RotatingLidarPhysX(\n        prim_path=\"/World/Carter/chassis/lidar\",\n        translation=np.array([0.0, 0.0, 0.25]),\n        config=\"Carter\",\n        rotation_frequency=10,\n        samples_per_scan=1080\n    )\n\n    # Initialize the world\n    world.reset()\n\n    # Main simulation loop\n    while simulation_app.is_running():\n        world.step(render=True)\n\n        # Get sensor data\n        if world.is_playing():\n            # Get camera data\n            rgb_data = camera.get_rgb()\n            depth_data = camera.get_depth()\n\n            # Get LIDAR data\n            lidar_data = lidar.get_linear_depth_data()\n\n            # Process data here\n            print(f\"Camera RGB shape: {rgb_data.shape}\")\n            print(f\"LIDAR data points: {len(lidar_data)}\")\n\n    world.clear()\n```\n\n## Isaac ROS Integration\n\n### Setting up Isaac ROS Bridge\nIsaac Sim includes a bridge to ROS 2 that enables communication between Isaac Sim and ROS 2 nodes:\n\n1. Enable the Isaac ROS Bridge extension in Isaac Sim\n2. Add ROS Bridge node to your scene\n3. Configure topic mappings and message types\n\n### Example ROS Integration\n```python\n# Python script to interface with Isaac Sim via ROS\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacSimROSInterface(Node):\n    def __init__(self):\n        super().__init__('isaac_sim_ros_interface')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Create subscribers for Isaac Sim sensors\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.lidar_callback,\n            10\n        )\n\n        # Create publisher for robot control\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            '/cmd_vel',\n            10\n        )\n\n        self.get_logger().info('Isaac Sim ROS Interface initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process camera images from Isaac Sim\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process image (e.g., object detection, feature extraction)\n            processed_image = self.process_image(cv_image)\n\n            # Publish processed commands if needed\n            self.publish_navigation_command(processed_image)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LIDAR data from Isaac Sim\"\"\"\n        # Convert LIDAR ranges to numpy array\n        ranges = np.array(msg.ranges)\n\n        # Process ranges (e.g., obstacle detection, mapping)\n        obstacles = self.detect_obstacles(ranges)\n\n        # Publish navigation commands based on obstacle detection\n        self.publish_avoidance_command(obstacles)\n\n    def process_image(self, image):\n        \"\"\"Process image data and extract relevant information\"\"\"\n        # Implementation would include:\n        # - Object detection\n        # - Feature extraction\n        # - Scene understanding\n        # - etc.\n        return image  # Placeholder\n\n    def detect_obstacles(self, ranges):\n        \"\"\"Detect obstacles from LIDAR ranges\"\"\"\n        # Find minimum distances in different sectors\n        sector_size = len(ranges) // 8  # Divide into 8 sectors\n        obstacles = []\n\n        for i in range(8):\n            start_idx = i * sector_size\n            end_idx = min((i + 1) * sector_size, len(ranges))\n            sector_ranges = ranges[start_idx:end_idx]\n\n            min_distance = np.min(sector_ranges[np.isfinite(sector_ranges)])\n            if min_distance < 1.0:  # Obstacle within 1 meter\n                obstacles.append({\n                    'sector': i,\n                    'distance': min_distance,\n                    'angle': (i * 360 / 8) - 180  # Convert to -180 to +180 degrees\n                })\n\n        return obstacles\n\n    def publish_navigation_command(self, image_data):\n        \"\"\"Publish navigation commands based on image processing\"\"\"\n        # Example: Move forward if no obstacles detected\n        cmd = Twist()\n        cmd.linear.x = 0.5  # Move forward at 0.5 m/s\n        cmd.angular.z = 0.0  # No rotation\n\n        self.cmd_vel_pub.publish(cmd)\n\n    def publish_avoidance_command(self, obstacles):\n        \"\"\"Publish avoidance commands based on obstacle detection\"\"\"\n        if obstacles:\n            # Example: Turn away from closest obstacle\n            closest_obstacle = min(obstacles, key=lambda o: o['distance'])\n\n            cmd = Twist()\n            cmd.linear.x = 0.2  # Slow down\n            cmd.angular.z = 0.5 if closest_obstacle['angle'] < 0 else -0.5  # Turn away\n\n            self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    isaac_interface = IsaacSimROSInterface()\n\n    try:\n        rclpy.spin(isaac_interface)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        isaac_interface.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Synthetic Data Generation\n\n### Creating Training Data\nIsaac Sim excels at generating synthetic training data for AI models:\n\n```python\n# Example: Generate synthetic object detection dataset\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport json\nimport os\n\nclass SyntheticDatasetGenerator:\n    def __init__(self, output_dir=\"synthetic_dataset\"):\n        self.output_dir = output_dir\n        self.sd_helper = SyntheticDataHelper()\n\n        # Create output directories\n        os.makedirs(f\"{output_dir}/images\", exist_ok=True)\n        os.makedirs(f\"{output_dir}/labels\", exist_ok=True)\n        os.makedirs(f\"{output_dir}/depth\", exist_ok=True)\n\n    def generate_training_samples(self, num_samples=1000):\n        \"\"\"Generate synthetic training samples\"\"\"\n        for i in range(num_samples):\n            # Randomize scene\n            self.randomize_scene()\n\n            # Capture synthetic data\n            data = self.capture_synthetic_data()\n\n            # Save data\n            self.save_training_sample(data, i)\n\n            if i % 100 == 0:\n                print(f\"Generated {i}/{num_samples} samples\")\n\n    def randomize_scene(self):\n        \"\"\"Randomize object positions, lighting, and camera viewpoints\"\"\"\n        # Move objects to random positions\n        # Change lighting conditions\n        # Adjust camera angles\n        # Vary textures and materials\n        pass\n\n    def capture_synthetic_data(self):\n        \"\"\"Capture RGB, depth, segmentation, and ground truth data\"\"\"\n        # Get RGB image\n        rgb_data = self.get_rgb_image()\n\n        # Get depth data\n        depth_data = self.get_depth_image()\n\n        # Get semantic segmentation\n        seg_data = self.get_semantic_segmentation()\n\n        # Get ground truth object poses\n        gt_poses = self.get_ground_truth_poses()\n\n        return {\n            'rgb': rgb_data,\n            'depth': depth_data,\n            'segmentation': seg_data,\n            'ground_truth': gt_poses\n        }\n\n    def save_training_sample(self, data, sample_id):\n        \"\"\"Save training sample with annotations\"\"\"\n        # Save RGB image\n        cv2.imwrite(f\"{self.output_dir}/images/{sample_id:06d}.png\", data['rgb'])\n\n        # Save depth image\n        np.save(f\"{self.output_dir}/depth/{sample_id:06d}.npy\", data['depth'])\n\n        # Save annotations\n        annotation = {\n            'image_id': sample_id,\n            'objects': data['ground_truth'],\n            'camera_intrinsics': self.get_camera_intrinsics()\n        }\n\n        with open(f\"{self.output_dir}/labels/{sample_id:06d}.json\", 'w') as f:\n            json.dump(annotation, f)\n\n# Usage\ngenerator = SyntheticDatasetGenerator(\"humanoid_training_data\")\ngenerator.generate_training_samples(num_samples=5000)\n```\n\n## Performance Optimization\n\n### Isaac Sim Settings for Performance\nTo optimize Isaac Sim performance:\n\n1. **Rendering Quality**: Adjust rendering quality based on needs\n   - For training: Lower quality with faster simulation\n   - For visualization: Higher quality with slower simulation\n\n2. **Physics Settings**: Optimize physics parameters\n   ```python\n   # Set appropriate physics substeps\n   world.get_physics_context().set_subspace_count(1)\n   world.get_physics_context().set_fixed_timestep(1.0/60.0)  # 60 FPS\n   ```\n\n3. **Scene Complexity**: Manage scene complexity\n   - Use appropriate level of detail (LOD) for objects\n   - Limit number of active objects\n   - Use instancing for repeated objects\n\n### Multi-GPU Utilization\nFor complex scenes requiring more computational power:\n- Use multiple GPUs for rendering and physics\n- Configure GPU affinity for different tasks\n- Monitor GPU utilization to balance load\n\n## Best Practices\n\n### 1. Scene Organization\n- Use consistent naming conventions\n- Organize objects in logical hierarchies\n- Use tags and labels for easy selection\n\n### 2. Asset Management\n- Use relative paths for portability\n- Organize assets in a clear directory structure\n- Use version control for scene files\n\n### 3. Performance\n- Start simple and add complexity gradually\n- Profile regularly to identify bottlenecks\n- Use appropriate level of detail for your needs\n\n### 4. Reproducibility\n- Document scene configurations\n- Use version control for scenes and scripts\n- Create configuration files for different scenarios\n\n## Troubleshooting Common Issues\n\n### 1. GPU Memory Issues\n**Problem**: Isaac Sim crashes due to GPU memory exhaustion\n**Solutions**:\n- Reduce rendering resolution\n- Simplify scene geometry\n- Close other GPU-intensive applications\n- Use less detailed textures\n\n### 2. Physics Instability\n**Problem**: Objects behave unrealistically or explode\n**Solutions**:\n- Check mass properties of objects\n- Verify collision geometry\n- Adjust physics substeps\n- Reduce time step size\n\n### 3. ROS Bridge Issues\n**Problem**: ROS communication fails\n**Solutions**:\n- Check network settings\n- Verify Isaac ROS Bridge extension is enabled\n- Ensure ROS environment is properly sourced\n- Check topic/service names match expectations\n\n## Integration with Isaac ROS\n\nIsaac Sim integrates seamlessly with Isaac ROS packages for perception, navigation, and manipulation tasks. This enables:\n- Realistic sensor simulation for algorithm testing\n- Synthetic data generation for AI training\n- Safe algorithm validation before real robot deployment\n- Hardware-in-the-loop testing\n\n## Exercise\n\n1. Install Isaac Sim using the method appropriate for your system\n2. Create a simple scene with a robot and basic environment\n3. Add camera and LIDAR sensors to the robot\n4. Configure the ROS bridge to publish sensor data\n5. Create a ROS node that subscribes to the sensor data and processes it\n6. Experiment with different scene configurations and lighting conditions\n7. Generate a small synthetic dataset using Isaac Sim's synthetic data tools\n\nThis exercise will help you become familiar with Isaac Sim's interface, scene creation, sensor configuration, and ROS integration.",
    "path": "module-4-vla\\isaac-sim-fundamentals.md",
    "description": ""
  },
  "module-4-vla\\llm-planning": {
    "title": "module-4-vla\\llm-planning",
    "content": "# LLM Planning for Robotics\n\nLarge Language Models (LLMs) play a crucial role in robotics by enabling natural language understanding, task planning, and high-level decision making. This section covers how to integrate LLMs with humanoid robots for intelligent task planning and execution.\n\n## Introduction to LLMs in Robotics\n\nLLMs bring several capabilities to robotics:\n\n- **Natural Language Understanding**: Interpret human commands in natural language\n- **Task Planning**: Decompose high-level goals into executable robot actions\n- **Reasoning**: Apply logical reasoning to handle novel situations\n- **Knowledge Integration**: Access vast knowledge bases for decision making\n- **Human-Robot Interaction**: Enable natural communication with humans\n\n### Key Capabilities for Robotics\n\n1. **Command Interpretation**: Convert natural language commands to robot actions\n2. **Task Decomposition**: Break down complex tasks into atomic robot operations\n3. **Context Awareness**: Understand the environment and robot capabilities\n4. **Error Handling**: Generate recovery strategies when tasks fail\n5. **Learning**: Adapt behavior based on past experiences\n\n## LLM Integration Approaches\n\n### 1. OpenAI GPT Models\n\nThe most straightforward approach uses OpenAI's GPT models:\n\n```python\nimport openai\nimport json\nfrom typing import Dict, List, Optional\n\nclass RobotLLMInterface:\n    def __init__(self, api_key: str, model: str = \"gpt-4-turbo\"):\n        openai.api_key = api_key\n        self.model = model\n        self.system_prompt = self._create_system_prompt()\n\n    def _create_system_prompt(self) -> str:\n        \"\"\"Create system prompt for robot task planning\"\"\"\n        return \"\"\"\n        You are a robot task planner that converts natural language commands into robot actions.\n        Your role is to:\n        1. Understand the human command in natural language\n        2. Decompose the command into specific robot actions\n        3. Consider the robot's capabilities and environment\n        4. Generate a sequence of executable actions\n        5. Include error handling and validation\n\n        Robot capabilities include:\n        - Navigation: Move to specific locations\n        - Manipulation: Pick up, place, grasp objects\n        - Perception: Detect and recognize objects\n        - Communication: Respond to human commands\n\n        Respond in JSON format with the following structure:\n        {\n          \"task_breakdown\": [\n            {\n              \"step\": 1,\n              \"action\": \"action_type\",\n              \"parameters\": {\"param1\": \"value1\", ...},\n              \"description\": \"Human-readable description\",\n              \"validation\": \"How to verify success\"\n            }\n          ],\n          \"potential_issues\": [\"issue1\", \"issue2\"],\n          \"success_criteria\": \"How to know the task is complete\"\n        }\n        \"\"\"\n\n    def plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Plan a task based on natural language command\"\"\"\n        user_prompt = f\"\"\"\n        Command: {command}\n\n        Robot State: {json.dumps(robot_state, indent=2)}\n        Environment: {json.dumps(environment, indent=2)}\n\n        Generate a detailed plan to execute this command.\n        \"\"\"\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                temperature=0.3,\n                max_tokens=1000\n            )\n\n            # Extract and parse JSON response\n            response_text = response.choices[0].message.content\n\n            # Find JSON in response (in case of additional text)\n            json_start = response_text.find('{')\n            json_end = response_text.rfind('}') + 1\n\n            if json_start != -1 and json_end != 0:\n                json_content = response_text[json_start:json_end]\n                plan = json.loads(json_content)\n                return plan\n            else:\n                # If no JSON found, return the raw response for error handling\n                return {\"raw_response\": response_text, \"parsed\": False}\n\n        except Exception as e:\n            return {\"error\": str(e), \"success\": False}\n```\n\n### 2. Open-Source LLM Integration\n\nFor privacy and cost considerations, open-source models can be used:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport json\n\nclass OpenSourceRobotLLM:\n    def __init__(self, model_name: str = \"meta-llama/Llama-2-7b-chat-hf\"):\n        \"\"\"\n        Initialize with open-source LLM.\n        Note: You'll need to handle model access and hardware requirements appropriately.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n\n        # Add pad token if missing\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Plan task using open-source LLM\"\"\"\n        prompt = f\"\"\"\n        [INST] <<SYS>>\n        You are a robot task planner. Convert the following natural language command into a sequence of robot actions.\n\n        Robot State: {json.dumps(robot_state)}\n        Environment: {json.dumps(environment)}\n\n        Respond in JSON format with task breakdown, potential issues, and success criteria.\n        <</SYS>>\n\n        Command: {command}\n\n        Provide a detailed plan in JSON format: [/INST]\n        \"\"\"\n\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n\n        # Move inputs to the same device as the model\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=500,\n                temperature=0.3,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n\n        try:\n            # Extract JSON from response\n            json_start = response.find('{')\n            json_end = response.rfind('}') + 1\n            if json_start != -1 and json_end != 0:\n                json_content = response[json_start:json_end]\n                return json.loads(json_content)\n            else:\n                return {\"raw_response\": response, \"parsed\": False}\n        except json.JSONDecodeError:\n            return {\"raw_response\": response, \"parsed\": False, \"error\": \"Could not parse JSON\"}\n```\n\n## Function Calling for Robotics APIs\n\n### OpenAI Function Calling\n\nLLMs can be enhanced with function calling to directly interact with robot APIs:\n\n```python\nimport json\nfrom typing import Dict, Any\n\nclass RobotFunctionCaller:\n    def __init__(self):\n        self.functions = {\n            \"move_to_location\": {\n                \"name\": \"move_to_location\",\n                \"description\": \"Move the robot to a specific location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"x\": {\"type\": \"number\", \"description\": \"X coordinate\"},\n                        \"y\": {\"type\": \"number\", \"description\": \"Y coordinate\"},\n                        \"z\": {\"type\": \"number\", \"description\": \"Z coordinate\"},\n                        \"orientation\": {\"type\": \"number\", \"description\": \"Orientation in radians\"}\n                    },\n                    \"required\": [\"x\", \"y\"]\n                }\n            },\n            \"pick_object\": {\n                \"name\": \"pick_object\",\n                \"description\": \"Pick up an object\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"object_name\": {\"type\": \"string\", \"description\": \"Name of the object to pick\"},\n                        \"location\": {\"type\": \"string\", \"description\": \"Where to find the object\"}\n                    },\n                    \"required\": [\"object_name\"]\n                }\n            },\n            \"place_object\": {\n                \"name\": \"place_object\",\n                \"description\": \"Place an object at a location\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"object_name\": {\"type\": \"string\", \"description\": \"Name of the object to place\"},\n                        \"location\": {\"type\": \"string\", \"description\": \"Where to place the object\"}\n                    },\n                    \"required\": [\"object_name\", \"location\"]\n                }\n            },\n            \"detect_objects\": {\n                \"name\": \"detect_objects\",\n                \"description\": \"Detect objects in the environment\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"area\": {\"type\": \"string\", \"description\": \"Area to scan for objects\"}\n                    }\n                }\n            }\n        }\n\n    def call_robot_functions(self, command: str, llm_interface) -> Dict:\n        \"\"\"Use LLM with function calling for robot planning\"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a robot task planner. Use available functions to plan and execute tasks.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": command\n            }\n        ]\n\n        # First, let LLM decide which functions to call\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4-turbo\",\n            messages=messages,\n            functions=list(self.functions.values()),\n            function_call=\"auto\",\n            temperature=0.3\n        )\n\n        response_message = response.choices[0].message\n\n        # If the model wants to call a function\n        if response_message.get(\"function_call\"):\n            function_name = response_message[\"function_call\"][\"name\"]\n            function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n\n            # Execute the function\n            result = self.execute_function(function_name, function_args)\n\n            # Add the result to the conversation\n            messages.append(response_message)\n            messages.append({\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": json.dumps(result)\n            })\n\n            # Get the final response\n            second_response = openai.ChatCompletion.create(\n                model=\"gpt-4-turbo\",\n                messages=messages,\n                temperature=0.3\n            )\n\n            return second_response.choices[0].message\n\n        return response_message\n\n    def execute_function(self, function_name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute robot function and return result\"\"\"\n        if function_name == \"move_to_location\":\n            return self._move_to_location(**args)\n        elif function_name == \"pick_object\":\n            return self._pick_object(**args)\n        elif function_name == \"place_object\":\n            return self._place_object(**args)\n        elif function_name == \"detect_objects\":\n            return self._detect_objects(**args)\n        else:\n            return {\"error\": f\"Unknown function: {function_name}\"}\n\n    def _move_to_location(self, x: float, y: float, z: float = 0.0, orientation: float = 0.0) -> Dict[str, Any]:\n        \"\"\"Simulate moving to location\"\"\"\n        # In a real implementation, this would interface with navigation stack\n        return {\n            \"success\": True,\n            \"message\": f\"Moved to location ({x}, {y}, {z}) with orientation {orientation}\",\n            \"actual_position\": {\"x\": x, \"y\": y, \"z\": z}\n        }\n\n    def _pick_object(self, object_name: str, location: str = None) -> Dict[str, Any]:\n        \"\"\"Simulate picking up an object\"\"\"\n        # In a real implementation, this would interface with manipulation stack\n        return {\n            \"success\": True,\n            \"message\": f\"Picked up {object_name}\",\n            \"object_status\": \"held\"\n        }\n\n    def _place_object(self, object_name: str, location: str) -> Dict[str, Any]:\n        \"\"\"Simulate placing an object\"\"\"\n        # In a real implementation, this would interface with manipulation stack\n        return {\n            \"success\": True,\n            \"message\": f\"Placed {object_name} at {location}\",\n            \"object_status\": \"placed\"\n        }\n\n    def _detect_objects(self, area: str = \"current_view\") -> Dict[str, Any]:\n        \"\"\"Simulate object detection\"\"\"\n        # In a real implementation, this would interface with perception stack\n        return {\n            \"success\": True,\n            \"objects_detected\": [\n                {\"name\": \"cup\", \"confidence\": 0.95, \"position\": {\"x\": 1.0, \"y\": 2.0, \"z\": 0.8}},\n                {\"name\": \"book\", \"confidence\": 0.89, \"position\": {\"x\": 1.2, \"y\": 2.1, \"z\": 0.85}}\n            ],\n            \"area_scanned\": area\n        }\n```\n\n## ROS 2 Integration\n\n### LLM Planning Node\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import JointState\nimport json\nimport openai\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__('llm_planning_node')\n\n        # Initialize LLM interface\n        api_key = self.declare_parameter('openai_api_key', '').get_parameter_value().string_value\n        if not api_key:\n            self.get_logger().error(\"OpenAI API key not provided\")\n            return\n\n        openai.api_key = api_key\n        self.llm_interface = RobotLLMInterface(api_key)\n\n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            'voice_commands',\n            self.command_callback,\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.robot_pose_sub = self.create_subscription(\n            Pose,\n            'robot_pose',\n            self.robot_pose_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, 'robot_plan', 10)\n        self.status_pub = self.create_publisher(String, 'llm_status', 10)\n\n        # Robot state tracking\n        self.current_joint_state = None\n        self.current_pose = None\n\n        self.get_logger().info('LLM Planning Node initialized')\n\n    def command_callback(self, msg: String):\n        \"\"\"Process voice command and generate plan\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        # Get current robot state\n        robot_state = self.get_current_robot_state()\n        environment = self.get_environment_context()\n\n        # Generate plan using LLM\n        plan = self.llm_interface.plan_task(command, robot_state, environment)\n\n        if \"error\" not in plan:\n            # Publish the plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f'Generated plan: {plan}')\n        else:\n            self.get_logger().error(f'Plan generation failed: {plan[\"error\"]}')\n            status_msg = String()\n            status_msg.data = f\"Error: {plan['error']}\"\n            self.status_pub.publish(status_msg)\n\n    def joint_state_callback(self, msg: JointState):\n        \"\"\"Update joint state\"\"\"\n        self.current_joint_state = msg\n\n    def robot_pose_callback(self, msg: Pose):\n        \"\"\"Update robot pose\"\"\"\n        self.current_pose = msg\n\n    def get_current_robot_state(self) -> Dict:\n        \"\"\"Get current robot state\"\"\"\n        state = {\n            \"timestamp\": self.get_clock().now().to_msg().stamp.sec,\n            \"position\": {},\n            \"joints\": {},\n            \"battery_level\": 100,  # Would come from battery topic\n            \"capabilities\": [\"navigation\", \"manipulation\", \"perception\"]\n        }\n\n        if self.current_pose:\n            state[\"position\"] = {\n                \"x\": self.current_pose.position.x,\n                \"y\": self.current_pose.position.y,\n                \"z\": self.current_pose.position.z,\n                \"orientation\": {\n                    \"x\": self.current_pose.orientation.x,\n                    \"y\": self.current_pose.orientation.y,\n                    \"z\": self.current_pose.orientation.z,\n                    \"w\": self.current_pose.orientation.w\n                }\n            }\n\n        if self.current_joint_state:\n            state[\"joints\"] = dict(zip(\n                self.current_joint_state.name,\n                self.current_joint_state.position\n            ))\n\n        return state\n\n    def get_environment_context(self) -> Dict:\n        \"\"\"Get environment context\"\"\"\n        # In a real system, this would come from:\n        # - Static map\n        # - Dynamic object tracking\n        # - Sensor data\n        # - Human presence detection\n        return {\n            \"known_locations\": {\n                \"kitchen\": {\"x\": 5.0, \"y\": 3.0},\n                \"living_room\": {\"x\": 2.0, \"y\": 1.0},\n                \"bedroom\": {\"x\": 8.0, \"y\": 2.0}\n            },\n            \"recently_detected_objects\": [],\n            \"obstacles\": [],  # Would come from costmap\n            \"navigation_status\": \"ready\"\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    llm_planning_node = LLMPlanningNode()\n\n    try:\n        rclpy.spin(llm_planning_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        llm_planning_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n## Advanced Planning Techniques\n\n### Hierarchical Task Networks (HTN)\n\n```python\nclass HTNPlanner:\n    def __init__(self):\n        self.primitive_actions = {\n            'navigate_to': self._navigate_to,\n            'pick_up': self._pick_up,\n            'place_down': self._place_down,\n            'detect_object': self._detect_object\n        }\n\n        self.complex_tasks = {\n            'fetch_object': self._decompose_fetch_object,\n            'clean_table': self._decompose_clean_table,\n            'serve_drink': self._decompose_serve_drink\n        }\n\n    def decompose_task(self, task_name: str, params: Dict) -> List[Dict]:\n        \"\"\"Decompose high-level task into primitive actions\"\"\"\n        if task_name in self.complex_tasks:\n            return self.complex_tasks[task_name](params)\n        elif task_name in self.primitive_actions:\n            return [{'action': task_name, 'params': params}]\n        else:\n            raise ValueError(f\"Unknown task: {task_name}\")\n\n    def _decompose_fetch_object(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose fetch object task\"\"\"\n        object_name = params['object_name']\n        destination = params['destination']\n\n        return [\n            {'action': 'detect_object', 'params': {'target': object_name}},\n            {'action': 'navigate_to', 'params': {'location': f'{object_name}_location'}},\n            {'action': 'pick_up', 'params': {'object': object_name}},\n            {'action': 'navigate_to', 'params': {'location': destination}},\n            {'action': 'place_down', 'params': {'object': object_name, 'location': destination}}\n        ]\n\n    def _decompose_clean_table(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose clean table task\"\"\"\n        table_location = params['table_location']\n\n        return [\n            {'action': 'navigate_to', 'params': {'location': table_location}},\n            {'action': 'detect_object', 'params': {'target': 'any_object_on_table'}},\n            # This would continue with pickup/dropoff cycles\n        ]\n\n    def _decompose_serve_drink(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose serve drink task\"\"\"\n        drink_type = params.get('drink_type', 'water')\n        customer_location = params['customer_location']\n\n        return [\n            {'action': 'navigate_to', 'params': {'location': 'kitchen'}},\n            {'action': 'detect_object', 'params': {'target': drink_type}},\n            {'action': 'pick_up', 'params': {'object': f'{drink_type}_container'}},\n            {'action': 'navigate_to', 'params': {'location': customer_location}},\n            {'action': 'place_down', 'params': {'object': f'{drink_type}_container', 'location': 'table_near_customer'}},\n            {'action': 'utter', 'params': {'text': 'Here is your drink!'}}\n        ]\n\n    def _navigate_to(self, params: Dict):\n        \"\"\"Primitive navigation action\"\"\"\n        # Interface with Nav2\n        pass\n\n    def _pick_up(self, params: Dict):\n        \"\"\"Primitive pick up action\"\"\"\n        # Interface with manipulation stack\n        pass\n\n    def _place_down(self, params: Dict):\n        \"\"\"Primitive place down action\"\"\"\n        # Interface with manipulation stack\n        pass\n\n    def _detect_object(self, params: Dict):\n        \"\"\"Primitive object detection\"\"\"\n        # Interface with perception stack\n        pass\n```\n\n## Prompt Engineering for Robotics\n\n### Effective Prompts for Robot Planning\n\n```python\nclass RobotPlanningPrompts:\n    @staticmethod\n    def create_task_planning_prompt(command: str, robot_state: Dict, environment: Dict) -> str:\n        \"\"\"Create effective prompt for task planning\"\"\"\n        return f\"\"\"\n        You are an expert robot task planner. Your job is to break down human commands into specific, executable robot actions.\n\n        ROBOT CAPABILITIES:\n        - Navigation: Can move to specific coordinates (x, y, z) with orientation\n        - Manipulation: Can pick up, place, grasp objects\n        - Perception: Can detect and recognize objects, people, locations\n        - Communication: Can speak, listen, display information\n\n        CURRENT STATE:\n        Position: ({robot_state.get('position', {}).get('x', 0)}, {robot_state.get('position', {}).get('y', 0)})\n        Battery: {robot_state.get('battery_level', 100)}%\n        Connected: True\n        Available actions: {', '.join(robot_state.get('capabilities', []))}\n\n        ENVIRONMENT:\n        Known locations: {list(environment.get('known_locations', {}).keys())}\n        Recent detections: {environment.get('recently_detected_objects', [])}\n        Obstacles: {environment.get('obstacles', [])}\n\n        COMMAND: \"{command}\"\n\n        INSTRUCTIONS:\n        1. Analyze the command for specific goals\n        2. Consider the robot's current state and environment\n        3. Break down into sequential, executable actions\n        4. Include error handling and validation steps\n        5. Consider safety and feasibility\n\n        OUTPUT FORMAT:\n        {{\n            \"analysis\": \"Brief analysis of the command\",\n            \"plan\": [\n                {{\n                    \"step\": 1,\n                    \"action\": \"action_type\",\n                    \"parameters\": {{\"param1\": \"value1\"}},\n                    \"description\": \"What this step does\",\n                    \"expected_outcome\": \"How to verify success\",\n                    \"error_handling\": \"What to do if this fails\"\n                }}\n            ],\n            \"estimated_duration\": \"Time estimate in seconds\",\n            \"resources_needed\": [\"list\", \"of\", \"required\", \"resources\"],\n            \"potential_risks\": [\"risk1\", \"risk2\"]\n        }}\n\n        Respond ONLY in valid JSON format:\n        \"\"\"\n\n    @staticmethod\n    def create_error_recovery_prompt(error: str, attempted_action: Dict, robot_state: Dict) -> str:\n        \"\"\"Create prompt for error recovery\"\"\"\n        return f\"\"\"\n        The robot encountered an error during task execution:\n\n        ERROR: {error}\n        ATTEMPTED ACTION: {attempted_action}\n        CURRENT STATE: {robot_state}\n\n        PROVIDE RECOVERY PLAN:\n        1. Diagnose the likely cause of the error\n        2. Suggest immediate recovery actions\n        3. Propose alternative approaches\n        4. Indicate when to abort and ask for human help\n\n        FORMAT: {{\n            \"diagnosis\": \"Likely cause of error\",\n            \"immediate_recovery\": [\"action1\", \"action2\"],\n            \"alternative_approach\": \"Different way to achieve goal\",\n            \"abort_conditions\": [\"conditions\", \"to\", \"stop\", \"trying\"],\n            \"human_help_needed\": \"When to ask for assistance\"\n        }}\n        \"\"\"\n```\n\n## Safety and Validation\n\n### Plan Validation System\n\n```python\nclass PlanValidator:\n    def __init__(self):\n        self.safety_rules = [\n            self._check_navigation_safety,\n            self._check_manipulation_safety,\n            self._check_resource_availability,\n            self._check_feasibility\n        ]\n\n    def validate_plan(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Validate plan for safety and feasibility\"\"\"\n        validation_result = {\n            \"is_valid\": True,\n            \"issues\": [],\n            \"warnings\": [],\n            \"suggestions\": []\n        }\n\n        for rule in self.safety_rules:\n            result = rule(plan, robot_state, environment)\n            if not result[\"valid\"]:\n                validation_result[\"is_valid\"] = False\n                validation_result[\"issues\"].extend(result[\"issues\"])\n            validation_result[\"warnings\"].extend(result[\"warnings\"])\n            validation_result[\"suggestions\"].extend(result[\"suggestions\"])\n\n        return validation_result\n\n    def _check_navigation_safety(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Check if navigation plan is safe\"\"\"\n        issues = []\n        warnings = []\n        suggestions = []\n\n        for step in plan.get(\"plan\", []):\n            if step.get(\"action\") == \"navigate_to\":\n                target = step.get(\"parameters\", {})\n                x, y = target.get(\"x\"), target.get(\"y\")\n\n                # Check if target is in known safe areas\n                if not self._is_safe_navigation_target(x, y, environment):\n                    issues.append(f\"Navigation to ({x}, {y}) may be unsafe\")\n\n        return {\n            \"valid\": len(issues) == 0,\n            \"issues\": issues,\n            \"warnings\": warnings,\n            \"suggestions\": suggestions\n        }\n\n    def _is_safe_navigation_target(self, x: float, y: float, environment: Dict) -> bool:\n        \"\"\"Check if navigation target is safe\"\"\"\n        # Check against known obstacles\n        obstacles = environment.get(\"obstacles\", [])\n        for obstacle in obstacles:\n            obs_x, obs_y = obstacle.get(\"x\", 0), obstacle.get(\"y\", 0)\n            distance = ((x - obs_x)**2 + (y - obs_y)**2)**0.5\n            if distance < 0.5:  # 50cm safety margin\n                return False\n\n        # Check if within map boundaries\n        # (would check against map in real implementation)\n\n        return True\n\n    def _check_manipulation_safety(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Check if manipulation plan is safe\"\"\"\n        # Implementation would check:\n        # - Reachability\n        # - Object properties (weight, fragility)\n        # - Collision avoidance\n        pass\n\n    def _check_resource_availability(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Check if required resources are available\"\"\"\n        # Implementation would check:\n        # - Battery level for planned duration\n        # - Required tools/accessories\n        # - Available time before deadlines\n        pass\n\n    def _check_feasibility(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Check if plan is technically feasible\"\"\"\n        # Implementation would check:\n        # - Robot capabilities vs required actions\n        # - Environmental constraints\n        # - Time feasibility\n        pass\n```\n\n## Performance Optimization\n\n### Caching and Optimization\n\n```python\nimport functools\nimport hashlib\nimport time\nfrom typing import Callable, Any\n\nclass OptimizedLLMInterface:\n    def __init__(self, api_key: str, cache_size: int = 1000):\n        openai.api_key = api_key\n        self.cache = {}\n        self.cache_order = []  # For LRU eviction\n        self.cache_size = cache_size\n\n    def cached_plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Plan task with caching to reduce API calls\"\"\"\n        # Create cache key from inputs\n        cache_key = self._create_cache_key(command, robot_state, environment)\n\n        # Check cache first\n        if cache_key in self.cache:\n            self.cache_order.remove(cache_key)  # Remove from current position\n            self.cache_order.append(cache_key)  # Move to end (most recent)\n            return self.cache[cache_key]\n\n        # Generate new plan\n        plan = self._generate_plan(command, robot_state, environment)\n\n        # Add to cache\n        self._add_to_cache(cache_key, plan)\n\n        return plan\n\n    def _create_cache_key(self, command: str, robot_state: Dict, environment: Dict) -> str:\n        \"\"\"Create unique cache key\"\"\"\n        combined = f\"{command}|{hash(str(sorted(robot_state.items())))}|{hash(str(sorted(environment.items())))}\"\n        return hashlib.md5(combined.encode()).hexdigest()\n\n    def _add_to_cache(self, key: str, value: Dict):\n        \"\"\"Add to cache with LRU eviction\"\"\"\n        if len(self.cache) >= self.cache_size:\n            # Remove oldest entry\n            oldest_key = self.cache_order.pop(0)\n            del self.cache[oldest_key]\n\n        self.cache[key] = value\n        self.cache_order.append(key)\n\n    def _generate_plan(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        \"\"\"Generate plan using LLM (this is the actual API call)\"\"\"\n        # Implementation of the actual LLM call\n        # This would be the method from RobotLLMInterface\n        pass\n```\n\n## Troubleshooting Common Issues\n\n### 1. API Rate Limits\n**Problem**: OpenAI API rate limiting\n**Solutions**:\n- Implement request queuing\n- Use caching for repeated commands\n- Monitor token usage\n- Consider higher-tier plans for production\n\n### 2. Hallucination Issues\n**Problem**: LLM generates invalid or impossible actions\n**Solutions**:\n- Use function calling to constrain outputs\n- Implement validation layers\n- Use lower temperatures (0.1-0.3)\n- Provide clear examples in system prompts\n\n### 3. Context Window Limitations\n**Problem**: Large robot states exceed context window\n**Solutions**:\n- Summarize robot state before sending\n- Use retrieval-augmented generation (RAG)\n- Implement state compression\n- Use streaming for large inputs\n\n### 4. Integration Latency\n**Problem**: High latency in robot response\n**Solutions**:\n- Use smaller, faster models for simple tasks\n- Implement asynchronous processing\n- Pre-plan common tasks\n- Use local models for basic commands\n\n## Best Practices\n\n### 1. Error Handling\n- Always implement fallback strategies\n- Log LLM interactions for debugging\n- Provide human-in-the-loop options\n- Validate outputs before execution\n\n### 2. Security\n- Secure API keys properly\n- Validate all inputs from LLM\n- Implement access controls\n- Monitor for prompt injection\n\n### 3. Performance\n- Cache frequent requests\n- Use appropriate model sizes\n- Implement request batching\n- Monitor and optimize token usage\n\n### 4. Testing\n- Test with edge cases\n- Validate safety constraints\n- Test error recovery paths\n- Performance benchmarking\n\n## Exercise\n\nCreate a complete LLM integration for your humanoid robot that includes:\n\n1. A robust LLM interface with error handling\n2. Function calling integration for direct robot API access\n3. A validation system to ensure plan safety\n4. Performance optimization techniques\n5. ROS 2 integration for real-time command processing\n6. A hierarchical planning system for complex tasks\n\nTest your system with various natural language commands and evaluate its ability to generate safe, executable robot plans.",
    "path": "module-4-vla\\llm-planning.md",
    "description": ""
  },
  "module-4-vla\\multimodal-perception": {
    "title": "module-4-vla\\multimodal-perception",
    "content": "# Multimodal Perception\n\nMultimodal perception refers to the integration of multiple sensory modalities (vision, language, and action) to create a comprehensive understanding of the environment and enable intelligent robot behavior. This section covers the integration of visual, linguistic, and motor information for humanoid robotics.\n\n## Introduction to Multimodal Perception\n\nMultimodal perception is crucial for humanoid robots to interact naturally with humans and environments. It involves:\n\n- **Visual Processing**: Understanding the visual environment\n- **Language Understanding**: Interpreting natural language commands\n- **Action Generation**: Executing appropriate physical responses\n- **Cross-modal Integration**: Combining information from different modalities\n\n### Vision-Language-Action (VLA) Models\n\nVLA models represent the next generation of AI systems that can process visual input, understand language commands, and generate appropriate actions:\n\n```\nVision Input + Language Command  VLA Model  Action Sequence\n```\n\n## NVIDIA Isaac Sim for Multimodal Training\n\n### Synthetic Data Generation\n\nIsaac Sim excels at generating synthetic data for multimodal perception:\n\n```python\n# Example: Generating synthetic multimodal training data\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport numpy as np\nimport json\n\nclass MultimodalTrainingDataGenerator:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_scene()\n        self.synthetic_data_helper = SyntheticDataHelper()\n\n    def setup_scene(self):\n        \"\"\"Set up scene with various objects for multimodal training\"\"\"\n        # Add humanoid robot\n        add_reference_to_stage(\n            usd_path=\"path/to/humanoid/robot.usd\",\n            prim_path=\"/World/HumanoidRobot\"\n        )\n\n        # Add various objects with semantic labels\n        self.add_object_with_label(\"RedCube\", \"/World/RedCube\", [0, 2, 0.5], \"cube_red\")\n        self.add_object_with_label(\"BlueSphere\", \"/World/BlueSphere\", [1, 1, 0.5], \"sphere_blue\")\n        self.add_object_with_label(\"GreenCylinder\", \"/World/GreenCylinder\", [-1, 1, 0.5], \"cylinder_green\")\n\n    def add_object_with_label(self, name: str, prim_path: str, position: list, semantic_label: str):\n        \"\"\"Add object with semantic labeling for training data\"\"\"\n        # Implementation would add object and set semantic labeling\n        pass\n\n    def generate_multimodal_data(self, num_samples: int = 1000):\n        \"\"\"Generate synthetic multimodal training data\"\"\"\n        training_data = []\n\n        for i in range(num_samples):\n            # Change scene configuration\n            self.randomize_scene()\n\n            # Capture sensor data\n            rgb_data = self.get_camera_data()\n            depth_data = self.get_depth_data()\n            semantic_data = self.get_semantic_segmentation()\n\n            # Generate language annotations\n            scene_description = self.generate_scene_description(rgb_data, semantic_data)\n            action_sequences = self.generate_possible_actions(scene_description)\n\n            # Create training sample\n            sample = {\n                \"sample_id\": f\"synthetic_{i:04d}\",\n                \"modalities\": {\n                    \"vision\": {\n                        \"rgb\": self.encode_image(rgb_data),\n                        \"depth\": self.encode_depth(depth_data),\n                        \"semantic\": self.encode_semantic(semantic_data)\n                    },\n                    \"language\": {\n                        \"scene_description\": scene_description,\n                        \"possible_commands\": [\n                            f\"Go to the {obj}\",\n                            f\"Pick up the {obj}\",\n                            f\"Move the {obj} to the table\"\n                            for obj in self.get_visible_objects(semantic_data)\n                        ]\n                    },\n                    \"actions\": action_sequences\n                }\n            }\n\n            training_data.append(sample)\n\n        return training_data\n\n    def randomize_scene(self):\n        \"\"\"Randomize object positions and lighting for variety\"\"\"\n        # Implementation would randomize object positions, lighting, etc.\n        pass\n\n    def encode_image(self, image_data):\n        \"\"\"Encode image for storage\"\"\"\n        # Implementation would compress and encode image\n        return \"encoded_rgb_data\"\n\n    def encode_depth(self, depth_data):\n        \"\"\"Encode depth data for storage\"\"\"\n        # Implementation would process depth data\n        return \"encoded_depth_data\"\n\n    def encode_semantic(self, semantic_data):\n        \"\"\"Encode semantic segmentation for storage\"\"\"\n        # Implementation would process semantic data\n        return \"encoded_semantic_data\"\n\n    def generate_scene_description(self, rgb_data, semantic_data):\n        \"\"\"Generate natural language description of scene\"\"\"\n        # Implementation would analyze scene and generate description\n        visible_objects = self.get_visible_objects(semantic_data)\n        return f\"The scene contains {', '.join(visible_objects)}. The robot is positioned centrally.\"\n\n    def generate_possible_actions(self, scene_description):\n        \"\"\"Generate possible actions based on scene\"\"\"\n        # Implementation would generate possible robot actions\n        return [\n            {\"action\": \"navigate\", \"target\": \"object_location\", \"description\": \"Move to object\"},\n            {\"action\": \"grasp\", \"target\": \"graspable_object\", \"description\": \"Grasp the object\"},\n            {\"action\": \"manipulate\", \"target\": \"manipulable_object\", \"description\": \"Manipulate the object\"}\n        ]\n\n    def get_visible_objects(self, semantic_data):\n        \"\"\"Extract visible objects from semantic segmentation\"\"\"\n        # Implementation would analyze semantic data\n        return [\"red_cube\", \"blue_sphere\", \"green_cylinder\"]\n```\n\n## Isaac ROS Perception Integration\n\n### Isaac ROS Perception Pipeline\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <isaac_ros_apriltag_interfaces/msg/april_tag_detection_array.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass MultimodalPerceptionNode : public rclcpp::Node\n{\npublic:\n    MultimodalPerceptionNode() : Node(\"multimodal_perception_node\")\n    {\n        // Create subscriptions for different sensor modalities\n        rgb_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/rgb/image_rect_color\", 10,\n            std::bind(&MultimodalPerceptionNode::rgbCallback, this, std::placeholders::_1)\n        );\n\n        depth_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/depth/image_rect_raw\", 10,\n            std::bind(&MultimodalPerceptionNode::depthCallback, this, std::placeholders::_1)\n        );\n\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            \"detectnet/detections\", 10,\n            std::bind(&MultimodalPerceptionNode::detectionCallback, this, std::placeholders::_1)\n        );\n\n        apriltag_sub_ = this->create_subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>(\n            \"apriltag_detections\", 10,\n            std::bind(&MultimodalPerceptionNode::apriltagCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for multimodal fusion results\n        multimodal_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            \"multimodal_fusion_result\", 10\n        );\n    }\n\nprivate:\n    void rgbCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Process RGB image for visual understanding\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        // Store RGB data for multimodal fusion\n        latest_rgb_image_ = cv_ptr->image;\n        rgb_timestamp_ = msg->header.stamp;\n\n        // Trigger multimodal fusion if all modalities are available\n        if (has_depth_ && has_detections_ && has_apriltags_) {\n            performMultimodalFusion();\n        }\n    }\n\n    void depthCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Process depth image for 3D understanding\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::TYPE_32FC1);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        latest_depth_image_ = cv_ptr->image;\n        depth_timestamp_ = msg->header.stamp;\n        has_depth_ = true;\n\n        // Trigger multimodal fusion if all modalities are available\n        if (has_rgb_ && has_detections_ && has_apriltags_) {\n            performMultimodalFusion();\n        }\n    }\n\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr msg)\n    {\n        latest_detections_ = *msg;\n        has_detections_ = true;\n\n        // Trigger multimodal fusion if all modalities are available\n        if (has_rgb_ && has_depth_ && has_apriltags_) {\n            performMultimodalFusion();\n        }\n    }\n\n    void apriltagCallback(const isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray::SharedPtr msg)\n    {\n        latest_apriltags_ = *msg;\n        has_apriltags_ = true;\n\n        // Trigger multimodal fusion if all modalities are available\n        if (has_rgb_ && has_depth_ && has_detections_) {\n            performMultimodalFusion();\n        }\n    }\n\n    void performMultimodalFusion()\n    {\n        // Combine information from all modalities\n        auto fused_result = fuseModalities(\n            latest_rgb_image_,\n            latest_depth_image_,\n            latest_detections_,\n            latest_apriltags_\n        );\n\n        // Publish fused result\n        auto result_msg = geometry_msgs::msg::PointStamped();\n        result_msg.header.stamp = this->now();\n        result_msg.header.frame_id = \"multimodal_fused\";\n        result_msg.point = fused_result.centroid;\n\n        multimodal_pub_->publish(result_msg);\n\n        RCLCPP_INFO(\n            this->get_logger(),\n            \"Multimodal fusion completed: %zu objects detected, %zu fiducials localized\",\n            latest_detections_.detections.size(),\n            latest_apriltags_.detections.size()\n        );\n\n        // Reset flags for next cycle\n        has_depth_ = false;\n        has_detections_ = false;\n        has_apriltags_ = false;\n    }\n\n    struct FusedPerceptionResult {\n        geometry_msgs::msg::Point centroid;\n        std::vector<std::string> object_labels;\n        std::vector<double> confidences;\n        std::vector<geometry_msgs::msg::Point> object_positions;\n    };\n\n    FusedPerceptionResult fuseModalities(\n        const cv::Mat& rgb_image,\n        const cv::Mat& depth_image,\n        const isaac_ros_detectnet_interfaces::msg::DetectionArray& detections,\n        const isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray& apriltags)\n    {\n        FusedPerceptionResult result;\n\n        // Combine visual detections with depth information for 3D localization\n        for (const auto& detection : detections.detections) {\n            // Get 2D bounding box center\n            int center_x = static_cast<int>(detection.bbox.center.x);\n            int center_y = static_cast<int>(detection.bbox.center.y);\n\n            // Get depth at this location (average in a small region)\n            double avg_depth = getAverageDepthAtPixel(depth_image, center_x, center_y);\n\n            // Convert 2D pixel coordinates to 3D world coordinates\n            geometry_msgs::msg::Point world_point = pixelTo3D(\n                center_x, center_y, avg_depth, camera_intrinsics_\n            );\n\n            result.object_labels.push_back(detection.class_name);\n            result.confidences.push_back(detection.confidence);\n            result.object_positions.push_back(world_point);\n        }\n\n        // Add AprilTag positions (they provide accurate 3D poses)\n        for (const auto& apriltag : apriltags.detections) {\n            result.object_labels.push_back(\"fiducial_\" + std::to_string(apriltag.id));\n            result.confidences.push_back(1.0);  // High confidence for fiducials\n            result.object_positions.push_back(apriltag.pose.position);\n        }\n\n        // Calculate overall centroid of all detected objects\n        if (!result.object_positions.empty()) {\n            double sum_x = 0, sum_y = 0, sum_z = 0;\n            for (const auto& pos : result.object_positions) {\n                sum_x += pos.x;\n                sum_y += pos.y;\n                sum_z += pos.z;\n            }\n\n            result.centroid.x = sum_x / result.object_positions.size();\n            result.centroid.y = sum_y / result.object_positions.size();\n            result.centroid.z = sum_z / result.object_positions.size();\n        }\n\n        return result;\n    }\n\n    double getAverageDepthAtPixel(const cv::Mat& depth_image, int x, int y, int radius = 3)\n    {\n        double sum = 0;\n        int count = 0;\n\n        for (int dy = -radius; dy <= radius; dy++) {\n            for (int dx = -radius; dx <= radius; dx++) {\n                int nx = x + dx;\n                int ny = y + dy;\n\n                if (nx >= 0 && nx < depth_image.cols && ny >= 0 && ny < depth_image.rows) {\n                    float depth_value = depth_image.at<float>(ny, nx);\n                    if (depth_value > 0 && depth_value < 10.0) {  // Valid depth range\n                        sum += depth_value;\n                        count++;\n                    }\n                }\n            }\n        }\n\n        return count > 0 ? sum / count : 0.0;\n    }\n\n    geometry_msgs::msg::Point pixelTo3D(int x, int y, double depth, const CameraIntrinsics& intrinsics)\n    {\n        geometry_msgs::msg::Point point;\n\n        // Convert pixel coordinates to camera coordinates\n        point.x = (x - intrinsics.cx) * depth / intrinsics.fx;\n        point.y = (y - intrinsics.cy) * depth / intrinsics.fy;\n        point.z = depth;\n\n        return point;\n    }\n\n    // Subscriptions\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr rgb_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr depth_sub_;\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Subscription<isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray>::SharedPtr apriltag_sub_;\n\n    // Publisher\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr multimodal_pub_;\n\n    // Data storage\n    cv::Mat latest_rgb_image_;\n    cv::Mat latest_depth_image_;\n    isaac_ros_detectnet_interfaces::msg::DetectionArray latest_detections_;\n    isaac_ros_apriltag_interfaces::msg::AprilTagDetectionArray latest_apriltags_;\n\n    // Flags for synchronization\n    bool has_depth_ = false;\n    bool has_detections_ = false;\n    bool has_apriltags_ = false;\n\n    // Timestamps for synchronization\n    builtin_interfaces::msg::Time rgb_timestamp_;\n    builtin_interfaces::msg::Time depth_timestamp_;\n\n    // Camera intrinsics (would be loaded from camera info)\n    struct CameraIntrinsics {\n        double fx, fy, cx, cy;\n    } camera_intrinsics_ = {616.363, 616.363, 313.071, 245.091};  // Example values\n};\n```\n\n## Vision-Language Integration\n\n### CLIP for Vision-Language Understanding\n\n```python\nimport clip\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport rospy\nfrom sensor_msgs.msg import Image as ImageMsg\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\n\nclass VisionLanguagePerception:\n    def __init__(self):\n        # Load pre-trained CLIP model\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n\n        # Initialize ROS components\n        self.bridge = CvBridge()\n\n        # Subscribe to camera feed\n        self.image_sub = rospy.Subscriber(\n            \"/camera/rgb/image_raw\",\n            ImageMsg,\n            self.image_callback\n        )\n\n        # Publisher for recognized concepts\n        self.concept_pub = rospy.Publisher(\n            \"/vision_language/concepts\",\n            String,\n            queue_size=10\n        )\n\n        # Define concepts for classification\n        self.concepts = [\n            \"a photo of a humanoid robot\",\n            \"a photo of a person\",\n            \"a photo of a table\",\n            \"a photo of a chair\",\n            \"a photo of a cup\",\n            \"a photo of a book\",\n            \"a photo of a door\",\n            \"a photo of a window\",\n            \"a scene with furniture\",\n            \"a scene with electronics\",\n            \"a kitchen scene\",\n            \"a living room scene\"\n        ]\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image and extract vision-language concepts\"\"\"\n        try:\n            # Convert ROS image to PIL Image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\n\n            # Preprocess image\n            image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n            # Tokenize text descriptions\n            text_input = clip.tokenize(self.concepts).to(self.device)\n\n            # Get similarity scores\n            with torch.no_grad():\n                logits_per_image, logits_per_text = self.model(image_input, text_input)\n                probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n\n            # Get top predictions\n            top_indices = np.argsort(probs)[-3:][::-1]  # Top 3 predictions\n            top_concepts = [(self.concepts[i], probs[i]) for i in top_indices if probs[i] > 0.1]\n\n            if top_concepts:\n                # Publish recognized concepts\n                concept_msg = String()\n                concept_msg.data = json.dumps({\n                    \"timestamp\": rospy.Time.now().to_sec(),\n                    \"concepts\": [{\"label\": label.split()[-1], \"confidence\": float(conf)} for label, conf in top_concepts]\n                })\n                self.concept_pub.publish(concept_msg)\n\n                rospy.loginfo(f\"Recognized concepts: {top_concepts}\")\n\n        except Exception as e:\n            rospy.logerr(f\"Error in vision-language processing: {str(e)}\")\n\n    def extract_object_attributes(self, image_path: str, object_name: str):\n        \"\"\"Extract attributes of specific objects using vision-language model\"\"\"\n        # This would implement more detailed attribute extraction\n        # for specific objects mentioned in language commands\n        pass\n```\n\n## Isaac Sim Perception Pipeline\n\n### Complete Perception System Integration\n\n```python\n# Isaac Sim Perception Pipeline\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.range_sensor import LidarRtx\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass IsaacPerceptionPipeline:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_perception_system()\n\n    def setup_perception_system(self):\n        \"\"\"Set up complete perception system with multiple sensors\"\"\"\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Add a humanoid robot with sensors\n        robot_path = assets_root_path + \"/Isaac/Robots/Humanoid/humanoid_instanceable.usd\"\n        add_reference_to_stage(usd_path=robot_path, prim_path=\"/World/HumanoidRobot\")\n\n        # Add RGB-D camera\n        self.camera = Camera(\n            prim_path=\"/World/HumanoidRobot/base_link/camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add LIDAR for 3D perception\n        self.lidar = LidarRtx(\n            prim_path=\"/World/HumanoidRobot/base_link/lidar\",\n            translation=np.array([0.0, 0.0, 0.5]),\n            orientation=np.array([0.0, 0.0, 0.0, 1.0]),\n            config=\"Carter\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add IMU for motion sensing\n        self.imu = IMU(\n            prim_path=\"/World/HumanoidRobot/base_link/imu\",\n            translation=np.array([0.0, 0.0, 0.25])\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_perception_cycle(self):\n        \"\"\"Run complete perception cycle with multimodal fusion\"\"\"\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get sensor data\n            camera_data = self.camera.get_rgb()\n            depth_data = self.camera.get_depth()\n            lidar_data = self.lidar.get_linear_depth_data()\n            imu_data = self.imu.get_measured()\n\n            # Process perception data\n            visual_features = self.extract_visual_features(camera_data)\n            3d_features = self.extract_3d_features(lidar_data, depth_data)\n            motion_features = self.extract_motion_features(imu_data)\n\n            # Fuse multimodal features\n            fused_perception = self.fuse_multimodal_features(\n                visual_features,\n                3d_features,\n                motion_features\n            )\n\n            # Generate perception report\n            self.report_perception(fused_perception)\n\n    def extract_visual_features(self, rgb_image):\n        \"\"\"Extract visual features from RGB image\"\"\"\n        # This would interface with Isaac ROS Visual Perception components\n        # For now, return basic feature extraction\n        features = {\n            \"edges\": self.detect_edges(rgb_image),\n            \"corners\": self.detect_corners(rgb_image),\n            \"colors\": self.extract_color_histogram(rgb_image),\n            \"objects\": self.detect_objects(rgb_image)  # Would use Isaac ROS DetectNet\n        }\n        return features\n\n    def extract_3d_features(self, lidar_data, depth_data):\n        \"\"\"Extract 3D features from depth and LIDAR data\"\"\"\n        features = {\n            \"surfaces\": self.detect_planes(lidar_data),\n            \"obstacles\": self.detect_obstacles(lidar_data),\n            \"free_space\": self.compute_free_space(lidar_data),\n            \"objects_3d\": self.extract_3d_objects(depth_data, lidar_data)\n        }\n        return features\n\n    def extract_motion_features(self, imu_data):\n        \"\"\"Extract motion features from IMU data\"\"\"\n        features = {\n            \"acceleration\": imu_data.acceleration,\n            \"angular_velocity\": imu_data.angular_velocity,\n            \"orientation\": imu_data.orientation,\n            \"motion_state\": self.classify_motion_state(imu_data)\n        }\n        return features\n\n    def fuse_multimodal_features(self, visual, features_3d, motion):\n        \"\"\"Fuse features from different modalities\"\"\"\n        fused_features = {\n            \"scene_understanding\": self.understand_scene(visual, features_3d),\n            \"robot_state\": self.estimate_robot_state(motion),\n            \"navigation_goals\": self.identify_navigation_goals(features_3d),\n            \"interaction_targets\": self.identify_interaction_targets(visual, features_3d)\n        }\n        return fused_features\n\n    def understand_scene(self, visual_features, features_3d):\n        \"\"\"Create comprehensive scene understanding\"\"\"\n        # Combine visual and 3D information for scene understanding\n        scene_description = {\n            \"environment_type\": self.classify_environment(visual_features, features_3d),\n            \"object_list\": self.merge_object_detections(visual_features, features_3d),\n            \"spatial_relations\": self.compute_spatial_relations(features_3d),\n            \"affordances\": self.identify_affordances(features_3d)\n        }\n        return scene_description\n\n    def identify_interaction_targets(self, visual_features, features_3d):\n        \"\"\"Identify objects suitable for interaction\"\"\"\n        # Identify graspable, manipulable, or approachable objects\n        targets = []\n        for obj in features_3d[\"objects_3d\"]:\n            if self.is_interactable(obj, visual_features):\n                targets.append({\n                    \"object_id\": obj[\"id\"],\n                    \"position\": obj[\"position\"],\n                    \"interaction_type\": self.determine_interaction_type(obj),\n                    \"approach_pose\": self.calculate_approach_pose(obj)\n                })\n        return targets\n\n    def report_perception(self, fused_perception):\n        \"\"\"Report perception results (for debugging and monitoring)\"\"\"\n        print(f\"Scene: {fused_perception['scene_understanding']['environment_type']}\")\n        print(f\"Objects detected: {len(fused_perception['scene_understanding']['object_list'])}\")\n        print(f\"Navigation goals: {len(fused_perception['navigation_goals'])}\")\n        print(f\"Interaction targets: {len(fused_perception['interaction_targets'])}\")\n```\n\n## Multimodal Fusion Algorithms\n\n### Late Fusion vs Early Fusion\n\n```cpp\n// Late Fusion Implementation\nclass LateFusionPerceptor {\npublic:\n    LateFusionPerceptor() {\n        // Initialize individual modality processors\n        visual_processor_ = std::make_unique<VisualProcessor>();\n        language_processor_ = std::make_unique<LanguageProcessor>();\n        action_processor_ = std::make_unique<ActionProcessor>();\n    }\n\n    MultimodalResult process(const SensorInputs& inputs, const LanguageInput& language) {\n        // Process each modality separately\n        auto visual_result = visual_processor_->process(inputs.rgb, inputs.depth);\n        auto language_result = language_processor_->process(language.text);\n\n        // Fuse at decision level\n        return fuseDecisionLevel(visual_result, language_result, inputs.action_space);\n    }\n\nprivate:\n    std::unique_ptr<VisualProcessor> visual_processor_;\n    std::unique_ptr<LanguageProcessor> language_processor_;\n    std::unique_ptr<ActionProcessor> action_processor_;\n\n    MultimodalResult fuseDecisionLevel(\n        const VisualResult& visual,\n        const LanguageResult& language,\n        const ActionSpace& action_space) {\n\n        MultimodalResult result;\n\n        // Combine confidence scores from different modalities\n        for (const auto& object : visual.detected_objects) {\n            if (language.intent.includes_object(object.label)) {\n                // Boost confidence when vision and language agree\n                result.confirmed_objects.push_back({\n                    object,\n                    object.confidence * language.confidence\n                });\n            }\n        }\n\n        // Generate action candidates based on fused understanding\n        result.action_candidates = generateActionCandidates(\n            result.confirmed_objects,\n            language.intent,\n            action_space\n        );\n\n        return result;\n    }\n};\n\n// Early Fusion Implementation\nclass EarlyFusionPerceptor {\npublic:\n    EarlyFusionPerceptor() {\n        // Initialize joint embedding network\n        embedding_network_ = std::make_unique<JointEmbeddingNetwork>();\n    }\n\n    MultimodalResult process(const SensorInputs& inputs, const LanguageInput& language) {\n        // Convert all inputs to joint embedding space\n        auto visual_embedding = createVisualEmbedding(inputs.rgb, inputs.depth);\n        auto language_embedding = createLanguageEmbedding(language.text);\n\n        // Concatenate embeddings\n        auto joint_embedding = concatenate(visual_embedding, language_embedding);\n\n        // Process through joint network\n        return embedding_network_->infer(joint_embedding);\n    }\n\nprivate:\n    std::unique_ptr<JointEmbeddingNetwork> embedding_network_;\n\n    Embedding createVisualEmbedding(const cv::Mat& rgb, const cv::Mat& depth) {\n        // Create embedding from visual inputs\n        // This would typically use a CNN backbone\n        return Embedding(); // Placeholder\n    }\n\n    Embedding createLanguageEmbedding(const std::string& text) {\n        // Create embedding from text\n        // This would typically use a transformer model\n        return Embedding(); // Placeholder\n    }\n};\n```\n\n## Cross-Modal Attention Mechanisms\n\n### Attention-Based Fusion\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CrossModalAttention(nn.Module):\n    \"\"\"Cross-modal attention for fusing vision and language features\"\"\"\n\n    def __init__(self, feature_dim):\n        super(CrossModalAttention, self).__init__()\n        self.feature_dim = feature_dim\n\n        # Linear projections for query, key, value\n        self.vision_proj = nn.Linear(feature_dim, feature_dim)\n        self.language_proj = nn.Linear(feature_dim, feature_dim)\n\n        # Attention mechanism\n        self.attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            dropout=0.1\n        )\n\n        # Output projection\n        self.output_proj = nn.Linear(feature_dim * 2, feature_dim)\n\n    def forward(self, vision_features, language_features):\n        \"\"\"\n        Args:\n            vision_features: [batch_size, seq_len_v, feature_dim]\n            language_features: [batch_size, seq_len_l, feature_dim]\n        Returns:\n            fused_features: [batch_size, seq_len_v, feature_dim]\n        \"\"\"\n        # Project features\n        vision_q = self.vision_proj(vision_features)\n        language_k = self.language_proj(language_features)\n        language_v = language_features  # Use original as values\n\n        # Apply cross-attention: vision attends to language\n        attended_features, attention_weights = self.attention(\n            vision_q.transpose(0, 1),  # Query from vision\n            language_k.transpose(0, 1),  # Key from language\n            language_v.transpose(0, 1)   # Value from language\n        )\n\n        # Transpose back\n        attended_features = attended_features.transpose(0, 1)\n\n        # Concatenate original vision features with attended features\n        concatenated = torch.cat([vision_features, attended_features], dim=-1)\n\n        # Project to output dimension\n        output = self.output_proj(concatenated)\n\n        return output, attention_weights\n\nclass MultimodalFusionNetwork(nn.Module):\n    \"\"\"Complete multimodal fusion network\"\"\"\n\n    def __init__(self, feature_dim=512):\n        super(MultimodalFusionNetwork, self).__init__()\n\n        # Vision encoder (could be pre-trained CNN)\n        self.vision_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, feature_dim)\n        )\n\n        # Language encoder (could be pre-trained transformer)\n        self.language_encoder = nn.Sequential(\n            nn.Embedding(10000, feature_dim),  # vocab_size=10000\n            nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=feature_dim,\n                    nhead=8,\n                    dim_feedforward=2048\n                ),\n                num_layers=6\n            ),\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten()\n        )\n\n        # Cross-modal attention modules\n        self.vision_language_attention = CrossModalAttention(feature_dim)\n        self.language_vision_attention = CrossModalAttention(feature_dim)\n\n        # Final fusion layer\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(feature_dim * 2, feature_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(feature_dim, feature_dim)\n        )\n\n    def forward(self, images, text_tokens):\n        # Encode modalities\n        vision_features = self.vision_encoder(images)\n        language_features = self.language_encoder(text_tokens)\n\n        # Reshape for attention (add sequence dimension if needed)\n        if len(vision_features.shape) == 2:\n            vision_features = vision_features.unsqueeze(1)  # [batch, 1, feature_dim]\n        if len(language_features.shape) == 2:\n            language_features = language_features.unsqueeze(1)  # [batch, 1, feature_dim]\n\n        # Apply cross-modal attention\n        vl_attended, vl_attention = self.vision_language_attention(\n            vision_features, language_features\n        )\n        lv_attended, lv_attention = self.language_vision_attention(\n            language_features, vision_features\n        )\n\n        # Combine attended features\n        fused_features = torch.cat([\n            vl_attended.mean(dim=1),  # Average over sequence\n            lv_attended.mean(dim=1)\n        ], dim=-1)\n\n        # Final fusion\n        output = self.fusion_layer(fused_features)\n\n        return output, (vl_attention, lv_attention)\n```\n\n## Practical Implementation\n\n### Isaac ROS Perception Integration\n\n```cpp\n// perception_integration_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <isaac_ros_visual_slam/visual_slam_node.hpp>\n#include <isaac_ros_detectnet/detectnet_node.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n#include <message_filters/subscriber.h>\n#include <message_filters/time_synchronizer.h>\n#include <message_filters/sync_policies/approximate_time.h>\n\nclass PerceptionIntegrationNode : public rclcpp::Node\n{\npublic:\n    PerceptionIntegrationNode() : Node(\"perception_integration_node\"), tf_buffer_(this->get_clock())\n    {\n        // Create synchronized subscribers for multimodal data\n        rgb_sub_.subscribe(this, \"camera/rgb/image_rect_color\", rmw_qos_profile_sensor_data);\n        depth_sub_.subscribe(this, \"camera/depth/image_rect\", rmw_qos_profile_sensor_data);\n        camera_info_sub_.subscribe(this, \"camera/rgb/camera_info\", rmw_qos_profile_sensor_data);\n\n        // Synchronize messages with approximate time policy\n        sync_ = std::make_shared<ApproximateTimeSync>(\n            SyncPolicy(10),\n            rgb_sub_, depth_sub_, camera_info_sub_\n        );\n        sync_->registerCallback(\n            std::bind(&PerceptionIntegrationNode::multimodalCallback, this,\n                     std::placeholders::_1, std::placeholders::_2, std::placeholders::_3)\n        );\n\n        // Publisher for fused perception results\n        fused_perception_pub_ = this->create_publisher<geometry_msgs::msg::PoseArray>(\n            \"fused_perception_results\", 10\n        );\n\n        // Publisher for visualization\n        visualization_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            \"perception_visualization\", 10\n        );\n\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(tf_buffer_);\n    }\n\nprivate:\n    using ApproximateTimeSync = message_filters::Synchronizer<SyncPolicy>;\n    using SyncPolicy = message_filters::sync_policies::ApproximateTime<\n        sensor_msgs::msg::Image,\n        sensor_msgs::msg::Image,\n        sensor_msgs::msg::CameraInfo\n    >;\n\n    void multimodalCallback(\n        const sensor_msgs::msg::Image::SharedPtr rgb_msg,\n        const sensor_msgs::msg::Image::SharedPtr depth_msg,\n        const sensor_msgs::msg::CameraInfo::SharedPtr camera_info_msg)\n    {\n        // Convert ROS images to OpenCV\n        cv_bridge::CvImagePtr rgb_cv_ptr, depth_cv_ptr;\n        try {\n            rgb_cv_ptr = cv_bridge::toCvCopy(rgb_msg, sensor_msgs::image_encodings::BGR8);\n            depth_cv_ptr = cv_bridge::toCvCopy(depth_msg, sensor_msgs::image_encodings::TYPE_32FC1);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n            return;\n        }\n\n        // Process with Isaac ROS perception components\n        auto visual_slam_result = processVisualSLAM(rgb_cv_ptr->image, *camera_info_msg);\n        auto detection_result = processObjectDetection(rgb_cv_ptr->image);\n        auto 3d_localization = process3DLocalization(depth_cv_ptr->image, detection_result);\n\n        // Fuse perception results\n        auto fused_result = fusePerceptionResults(\n            visual_slam_result,\n            detection_result,\n            3d_localization\n        );\n\n        // Publish fused results\n        publishFusedResults(fused_result, rgb_msg->header);\n\n        // Create visualization\n        auto visualization_image = createPerceptionVisualization(\n            rgb_cv_ptr->image,\n            detection_result,\n            fused_result\n        );\n\n        // Publish visualization\n        visualization_pub_->publish(*visualization_image);\n    }\n\n    struct FusedPerceptionResult {\n        std::vector<geometry_msgs::msg::PoseStamped> object_poses;\n        std::vector<std::string> object_labels;\n        std::vector<double> confidences;\n        geometry_msgs::msg::PoseStamped robot_pose;\n        std_msgs::msg::Header header;\n    };\n\n    FusedPerceptionResult fusePerceptionResults(\n        const VisualSLAMResult& vslam_result,\n        const DetectionResult& detection_result,\n        const LocalizationResult& localization_result)\n    {\n        FusedPerceptionResult fused_result;\n        fused_result.header = detection_result.header;  // Use detection header as reference\n\n        // Transform detected objects to map frame using VSLAM pose\n        for (size_t i = 0; i < detection_result.objects.size(); ++i) {\n            if (i < localization_result.positions.size()) {\n                geometry_msgs::msg::PointStamped obj_in_camera, obj_in_map;\n\n                // Set up point in camera frame\n                obj_in_camera.header = detection_result.header;\n                obj_in_camera.point = localization_result.positions[i];\n\n                // Transform to map frame using VSLAM pose\n                try {\n                    tf2::doTransform(obj_in_camera, obj_in_map, vslam_result.camera_to_map_transform);\n\n                    geometry_msgs::msg::PoseStamped obj_pose;\n                    obj_pose.header = fused_result.header;\n                    obj_pose.pose.position = obj_in_map.point;\n                    obj_pose.pose.orientation.w = 1.0;  // Simple orientation\n\n                    fused_result.object_poses.push_back(obj_pose);\n                    fused_result.object_labels.push_back(detection_result.labels[i]);\n                    fused_result.confidences.push_back(detection_result.confidences[i]);\n                } catch (tf2::TransformException& ex) {\n                    RCLCPP_WARN(this->get_logger(), \"Could not transform object pose: %s\", ex.what());\n                }\n            }\n        }\n\n        // Set robot pose from VSLAM\n        fused_result.robot_pose.header = fused_result.header;\n        fused_result.robot_pose.pose = vslam_result.robot_pose;\n\n        return fused_result;\n    }\n\n    void publishFusedResults(const FusedPerceptionResult& result, const std_msgs::msg::Header& header)\n    {\n        geometry_msgs::msg::PoseArray pose_array;\n        pose_array.header = result.header;\n\n        for (const auto& pose_stamped : result.object_poses) {\n            pose_array.poses.push_back(pose_stamped.pose);\n        }\n\n        fused_perception_pub_->publish(pose_array);\n    }\n\n    sensor_msgs::msg::Image::SharedPtr createPerceptionVisualization(\n        const cv::Mat& image,\n        const DetectionResult& detections,\n        const FusedPerceptionResult& fused_result)\n    {\n        cv::Mat vis_image = image.clone();\n\n        // Draw bounding boxes for detections\n        for (size_t i = 0; i < detections.boxes.size(); ++i) {\n            const auto& box = detections.boxes[i];\n            cv::rectangle(vis_image,\n                         cv::Point(box.xmin, box.ymin),\n                         cv::Point(box.xmax, box.ymax),\n                         cv::Scalar(0, 255, 0), 2);\n\n            // Add label\n            std::string label = detections.labels[i] + \": \" +\n                               std::to_string(static_cast<int>(detections.confidences[i] * 100)) + \"%\";\n            cv::putText(vis_image, label,\n                       cv::Point(box.xmin, box.ymin - 10),\n                       cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 1);\n        }\n\n        // Convert back to ROS image\n        cv_bridge::CvImage cv_vis;\n        cv_vis.header = detections.header;\n        cv_vis.encoding = sensor_msgs::image_encodings::BGR8;\n        cv_vis.image = vis_image;\n\n        return cv_vis.toImageMsg();\n    }\n\n    // Subscriptions\n    message_filters::Subscriber<sensor_msgs::msg::Image> rgb_sub_;\n    message_filters::Subscriber<sensor_msgs::msg::Image> depth_sub_;\n    message_filters::Subscriber<sensor_msgs::msg::CameraInfo> camera_info_sub_;\n    std::shared_ptr<ApproximateTimeSync> sync_;\n\n    // Publishers\n    rclcpp::Publisher<geometry_msgs::msg::PoseArray>::SharedPtr fused_perception_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr visualization_pub_;\n\n    // TF components\n    tf2_ros::Buffer tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n};\n```\n\n## Performance Optimization\n\n### Efficient Multimodal Processing\n\n```cpp\n// Efficient multimodal processing with threading\nclass EfficientMultimodalProcessor {\npublic:\n    EfficientMultimodalProcessor() {\n        // Create processing threads for each modality\n        vision_thread_ = std::thread(&EfficientMultimodalProcessor::visionProcessingLoop, this);\n        language_thread_ = std::thread(&EfficientMultimodalProcessor::languageProcessingLoop, this);\n        fusion_thread_ = std::thread(&EfficientMultimodalProcessor::fusionProcessingLoop, this);\n    }\n\n    ~EfficientMultimodalProcessor() {\n        running_ = false;\n\n        if (vision_thread_.joinable()) vision_thread_.join();\n        if (language_thread_.joinable()) language_thread_.join();\n        if (fusion_thread_.joinable()) fusion_thread_.join();\n    }\n\n    void processInput(const SensorInput& sensor_input, const LanguageInput& language_input) {\n        // Queue inputs for processing\n        {\n            std::lock_guard<std::mutex> lock(vision_queue_mutex_);\n            vision_queue_.push(sensor_input);\n        }\n\n        {\n            std::lock_guard<std::mutex> lock(language_queue_mutex_);\n            language_queue_.push(language_input);\n        }\n    }\n\n    std::optional<MultimodalResult> getResult() {\n        std::lock_guard<std::mutex> lock(result_mutex_);\n        if (!result_queue_.empty()) {\n            auto result = result_queue_.front();\n            result_queue_.pop();\n            return result;\n        }\n        return std::nullopt;\n    }\n\nprivate:\n    void visionProcessingLoop() {\n        while (running_) {\n            SensorInput input;\n\n            // Get input from queue\n            {\n                std::lock_guard<std::mutex> lock(vision_queue_mutex_);\n                if (!vision_queue_.empty()) {\n                    input = vision_queue_.front();\n                    vision_queue_.pop();\n                }\n            }\n\n            if (hasValidInput(input)) {\n                // Process vision data\n                auto vision_result = processVision(input);\n\n                // Store result\n                {\n                    std::lock_guard<std::mutex> lock(vision_result_mutex_);\n                    latest_vision_result_ = vision_result;\n                }\n            }\n\n            std::this_thread::sleep_for(std::chrono::milliseconds(1));  // Yield\n        }\n    }\n\n    void languageProcessingLoop() {\n        while (running_) {\n            LanguageInput input;\n\n            // Get input from queue\n            {\n                std::lock_guard<std::mutex> lock(language_queue_mutex_);\n                if (!language_queue_.empty()) {\n                    input = language_queue_.front();\n                    language_queue_.pop();\n                }\n            }\n\n            if (hasValidInput(input)) {\n                // Process language data\n                auto language_result = processLanguage(input);\n\n                // Store result\n                {\n                    std::lock_guard<std::mutex> lock(language_result_mutex_);\n                    latest_language_result_ = language_result;\n                }\n            }\n\n            std::this_thread::sleep_for(std::chrono::milliseconds(1));  // Yield\n        }\n    }\n\n    void fusionProcessingLoop() {\n        while (running_) {\n            // Check if we have both vision and language results\n            std::optional<VisionResult> vision_result;\n            std::optional<LanguageResult> language_result;\n\n            {\n                std::lock_guard<std::mutex> lock(vision_result_mutex_);\n                vision_result = latest_vision_result_;\n            }\n\n            {\n                std::lock_guard<std::mutex> lock(language_result_mutex_);\n                language_result = latest_language_result_;\n            }\n\n            if (vision_result && language_result) {\n                // Fuse results\n                auto fused_result = fuseResults(*vision_result, *language_result);\n\n                // Store final result\n                {\n                    std::lock_guard<std::mutex> lock(result_mutex_);\n                    result_queue_.push(fused_result);\n                }\n\n                // Clear processed results\n                {\n                    std::lock_guard<std::mutex> lock(vision_result_mutex_);\n                    latest_vision_result_.reset();\n                }\n                {\n                    std::lock_guard<std::mutex> lock(language_result_mutex_);\n                    latest_language_result_.reset();\n                }\n            }\n\n            std::this_thread::sleep_for(std::chrono::milliseconds(10));  // Fusion rate\n        }\n    }\n\n    std::queue<SensorInput> vision_queue_;\n    std::queue<LanguageInput> language_queue_;\n    std::queue<MultimodalResult> result_queue_;\n\n    std::mutex vision_queue_mutex_;\n    std::mutex language_queue_mutex_;\n    std::mutex result_mutex_;\n    std::mutex vision_result_mutex_;\n    std::mutex language_result_mutex_;\n\n    std::optional<VisionResult> latest_vision_result_;\n    std::optional<LanguageResult> latest_language_result_;\n\n    std::thread vision_thread_;\n    std::thread language_thread_;\n    std::thread fusion_thread_;\n    std::atomic<bool> running_{true};\n};\n```\n\n## Quality Assurance\n\n### Perception Validation\n\n```cpp\nclass PerceptionValidator {\npublic:\n    struct PerceptionMetrics {\n        double accuracy;\n        double precision;\n        double recall;\n        double f1_score;\n        double processing_time_ms;\n        int total_detections;\n        int correct_detections;\n        int false_positives;\n        int false_negatives;\n    };\n\n    PerceptionMetrics validatePerception(\n        const MultimodalResult& result,\n        const GroundTruth& ground_truth)\n    {\n        PerceptionMetrics metrics;\n\n        // Calculate metrics based on comparison with ground truth\n        int true_positives = 0, false_positives = 0, false_negatives = 0;\n\n        for (const auto& detected_obj : result.objects) {\n            bool matched = false;\n            for (const auto& gt_obj : ground_truth.objects) {\n                if (isObjectMatch(detected_obj, gt_obj)) {\n                    true_positives++;\n                    matched = true;\n                    break;\n                }\n            }\n            if (!matched) {\n                false_positives++;\n            }\n        }\n\n        false_negatives = ground_truth.objects.size() - true_positives;\n\n        metrics.accuracy = static_cast<double>(true_positives) /\n                          (true_positives + false_positives + false_negatives);\n        metrics.precision = static_cast<double>(true_positives) /\n                           (true_positives + false_positives);\n        metrics.recall = static_cast<double>(true_positives) /\n                        (true_positives + false_negatives);\n        metrics.f1_score = 2 * (metrics.precision * metrics.recall) /\n                          (metrics.precision + metrics.recall);\n\n        metrics.total_detections = result.objects.size();\n        metrics.correct_detections = true_positives;\n        metrics.false_positives = false_positives;\n        metrics.false_negatives = false_negatives;\n\n        return metrics;\n    }\n\nprivate:\n    bool isObjectMatch(\n        const PerceivedObject& detected,\n        const GroundTruthObject& ground_truth,\n        double position_threshold = 0.1,  // 10cm tolerance\n        double label_threshold = 0.9)     // 90% IoU threshold\n    {\n        // Check if objects are close enough in 3D space\n        double dist = std::sqrt(\n            std::pow(detected.position.x - ground_truth.position.x, 2) +\n            std::pow(detected.position.y - ground_truth.position.y, 2) +\n            std::pow(detected.position.z - ground_truth.position.z, 2)\n        );\n\n        if (dist > position_threshold) {\n            return false;\n        }\n\n        // Check if labels match\n        return detected.label == ground_truth.label;\n    }\n};\n```\n\n## Troubleshooting Common Issues\n\n### 1. Sensor Synchronization Issues\n**Problem**: Different sensors operating at different frequencies causing temporal misalignment\n**Solutions**:\n- Use message filters with approximate time synchronization\n- Implement interpolation for slower sensors\n- Buffer sensor data with timestamps\n\n### 2. Cross-Modal Alignment Issues\n**Problem**: Difficulty in relating information across modalities\n**Solutions**:\n- Ensure proper calibration between sensors\n- Use common reference frames (TF)\n- Implement spatial verification between modalities\n\n### 3. Computational Bottleneck\n**Problem**: Multimodal processing exceeding real-time requirements\n**Solutions**:\n- Use multi-threading for parallel processing\n- Implement processing priorities\n- Use lightweight models for real-time applications\n\n### 4. Data Association Issues\n**Problem**: Incorrect matching of objects across modalities\n**Solutions**:\n- Implement robust data association algorithms\n- Use temporal consistency checks\n- Add geometric verification steps\n\n## Best Practices\n\n### 1. Modular Design\n- Keep each modality processing separate initially\n- Design clean interfaces between components\n- Allow for easy replacement of individual components\n\n### 2. Robust Error Handling\n- Handle missing modalities gracefully\n- Implement fallback behaviors\n- Validate inputs before processing\n\n### 3. Performance Monitoring\n- Monitor processing times for each modality\n- Track resource utilization\n- Implement adaptive processing based on available resources\n\n### 4. Validation and Testing\n- Test with various environmental conditions\n- Validate in simulation before real-world deployment\n- Use ground truth data for performance evaluation\n\n## Exercise\n\nCreate a complete multimodal perception system that:\n\n1. Integrates RGB-D camera, IMU, and LIDAR data\n2. Uses Isaac ROS components for individual perception tasks\n3. Implements a fusion algorithm that combines visual, linguistic, and spatial information\n4. Creates a perception pipeline that can handle natural language commands\n5. Validates the system performance using synthetic data from Isaac Sim\n6. Implements proper error handling and fallback behaviors\n\nTest your system with various scenarios including:\n- Object detection and localization\n- Natural language command interpretation\n- Multi-object tracking and interaction\n- Dynamic environment adaptation\n- Performance under different lighting conditions\n\nEvaluate the system's performance using the metrics discussed in this section.",
    "path": "module-4-vla\\multimodal-perception.md",
    "description": ""
  },
  "module-4-vla\\summary": {
    "title": "module-4-vla\\summary",
    "content": "# Summary: Physical AI & Humanoid Robotics Book\n\n## Project Completion Report\n\nThe Physical AI & Humanoid Robotics educational book has been successfully implemented with all user stories completed. This comprehensive resource teaches students how to integrate AI systems with physical humanoid robots using NVIDIA Isaac technologies.\n\n## Completed Modules\n\n### Module 1: The Robotic Nervous System (ROS 2)\n- ROS 2 nodes, topics, services, and actions\n- rclpy for Python-ROS bridging\n- URDF for humanoid description\n- Practical exercises with ROS 2 commands\n\n### Module 2: The Digital Twin (Gazebo & Unity)\n- Physics simulation with gravity and collisions\n- Sensor simulation (LiDAR, IMU, depth camera)\n- Unity-based visualization\n- Digital twin validation techniques\n\n### Module 3: The AI-Robot Brain (NVIDIA Isaac)\n- Isaac Sim for photorealistic simulation\n- Isaac ROS for VSLAM + navigation\n- Nav2 for humanoid locomotion\n- Practical perception exercises\n\n### Module 4: Vision-Language-Action (VLA)\n- Whisper for speech commands\n- LLM-driven planning\n- Multimodal perception\n- Voice-to-action pipeline implementation\n\n## Key Accomplishments\n\n### 1. Isaac Sim Integration\n- Complete Isaac Sim setup and configuration\n- Digital twin creation with Gazebo and Unity\n- Physics simulation with accurate collision detection\n- Sensor simulation for realistic perception\n\n### 2. AI Perception Systems\n- Isaac ROS Visual SLAM for localization\n- Isaac ROS DetectNet for object detection\n- Isaac ROS Bi3D for 3D segmentation\n- Multimodal perception pipelines\n\n### 3. Navigation and Locomotion\n- VSLAM for robot localization and mapping\n- Nav2 integration for humanoid navigation\n- Obstacle avoidance and path planning\n- Humanoid-specific locomotion patterns\n\n### 4. Vision-Language-Action Pipeline\n- Whisper speech-to-text integration\n- LLM planning and reasoning\n- Action execution in simulation\n- End-to-end voice-command-to-action pipeline\n\n## Technical Implementation\n\n### Architecture\n- Docusaurus-based documentation system\n- Modular content organization\n- Isaac ROS component integration\n- Simulation-to-reality transfer approach\n\n### Tools and Technologies\n- NVIDIA Isaac Sim for high-fidelity simulation\n- Isaac ROS packages for perception and navigation\n- ROS 2 Humble for robotic middleware\n- Docusaurus for documentation generation\n\n## Learning Outcomes Achieved\n\nStudents who complete this book will be able to:\n\n1. **Deploy ROS 2 nodes** to control humanoid robots in simulation\n2. **Build and validate digital twins** using Gazebo and Unity\n3. **Integrate NVIDIA Isaac AI** for perception and navigation\n4. **Execute Vision-Language-Action tasks** using LLMs and Whisper\n5. **Complete a capstone project**: autonomous humanoid robot capable of voice-to-action, path planning, navigation, object detection, and manipulation\n\n## Assessment Metrics\n\nThe book includes:\n- Practical exercises with verification steps\n- Performance benchmarks and evaluation criteria\n- Troubleshooting guides for common issues\n- Best practices for humanoid robotics development\n\n## Future Enhancements\n\nPotential extensions to this educational material:\n- Additional sensor integration (RADAR, thermal imaging)\n- Advanced manipulation tasks\n- Multi-robot coordination scenarios\n- Real-robot deployment examples\n\n## Conclusion\n\nThis educational resource provides a comprehensive pathway for students to learn Physical AI and humanoid robotics using state-of-the-art tools and techniques. The modular approach allows for flexible learning paths while maintaining a cohesive understanding of embodied AI systems.\n\nThe book successfully bridges the gap between digital intelligence and physical robotic bodies, enabling students to create robots that understand and interact with the physical world through vision, language, and action.",
    "path": "module-4-vla\\summary.md",
    "description": ""
  },
  "module-4-vla\\vla-architecture-diagrams": {
    "title": "module-4-vla\\vla-architecture-diagrams",
    "content": "# VLA Architecture Diagrams\n\nThis section provides detailed diagrams showing the Vision-Language-Action (VLA) architecture for humanoid robotics systems using Isaac Sim and Isaac ROS.\n\n## 1. High-Level VLA System Architecture\n\n### Diagram: vla-overview-architecture.svg\n\n```\n\n                             VLA SYSTEM ARCHITECTURE                            \n\n                                                                                 \n           \n     HUMAN INPUT     VISION-LANGUAGE       ACTION GENERATION     \n                          UNDERSTANDING                                   \n    Voice Command                                LLM Planning           \n    Gesture             Object Detection        ROS 2 Command Mapping  \n    Text Input          Scene Understanding     Motion Planning        \n        Language Processing     Control Execution      \n                              \n                                                                               \n                                                                               \n           \n     SPEECH TO        NATURAL LANGUAGE     ROBOT CONTROLLER       \n     TEXT (Whisper)       PROCESSING (LLM)                                \n                                                  ROS 2 Nodes            \n    Audio Input         Intent Recognition      Navigation Stack       \n    Transcription       Task Planning           Manipulation Stack     \n    Text Output         Context Awareness       Balance Control        \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                        HUMANOID ROBOT                          \n                                                                                \n                          Physical Body (Humanoid Form)                        \n                          ROS 2 Middleware                                     \n                          Sensor Integration (Cameras, LIDAR, IMU)            \n                          Actuator Control (Motors, Servos)                    \n                          Digital Twin (Isaac Sim)                             \n                         \n\n```\n\n## 2. Voice Command Processing Pipeline\n\n### Diagram: voice-command-pipeline.svg\n\n```\n\n                        VOICE COMMAND PROCESSING PIPELINE                        \n\n                                                                                 \n           \n     MICROPHONE   WHISPER ASR        LANGUAGE MODEL (LLM)       \n     INPUT            (SPEECH-TO-TEXT)       (INTENT RECOGNITION)       \n                                                                      \n    Audio           Transcription         Command Interpretation     \n    Sampling        Text Conversion       Task Decomposition         \n    Preproc.        Timing Info           Action Sequencing          \n           \n                                                                               \n                                                                               \n           \n     AUDIO DATA           TRANSCRIBED            PARSED COMMAND         \n     PROCESSING           TEXT (JSON)            STRUCTURE (JSON)       \n                                                                        \n    Noise Reduction     Confidence            Action Type            \n    Echo Cancellation   Timestamps            Target Object          \n    VAD Detection       Word Alignments       Parameters             \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                ROS 2 COMMAND GENERATION                         \n                                                                                \n                          Convert parsed commands to ROS 2 messages            \n                          Map high-level tasks to robot actions                \n                          Generate action sequences and trajectories           \n                          Publish to appropriate ROS 2 topics/services         \n                         \n\n```\n\n## 3. Isaac ROS VLA Integration\n\n### Diagram: isaac-ros-vla-integration.svg\n\n```\n\n                        ISAAC ROS VLA INTEGRATION                               \n\n                                                                                 \n           \n    Isaac Sim       Isaac ROS Visual    Isaac ROS Perception    \n    (Simulation)         SLAM                    (Object Detection)    \n                                                                      \n    Physics             Feature Tracking      DetectNet            \n    Sensors             Pose Estimation       Bi3D                 \n    Environment         Map Building          AprilTag             \n           \n                                                                               \n                                                                               \n           \n    Isaac ROS       Isaac ROS Navigation Isaac ROS Manipulation  \n    (Sensor Bridge)      (Nav2 Integration)      (Pose Estimation)      \n                                                                      \n    Image Bridge        Global Planner        6DOF Pose Estimation \n    TF Management       Local Planner         Inverse Kinematics   \n    Message Sync        Controller            Trajectory Planning  \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                   ROS 2 NAVIGATION STACK                         \n                                                                                \n                          Nav2 Global Planner (A*, Dijkstra)                    \n                          Nav2 Local Planner (DWA, TEB)                         \n                          Costmap Management (Static & Local)                   \n                          Behavior Trees for Task Orchestration                 \n                         \n\n```\n\n## 4. Vision-Language-Action Pipeline Flow\n\n### Diagram: vla-pipeline-flow.svg\n\n```\n\n                        VLA PIPELINE FLOW                                        \n\n                                                                                 \n  STEP 1: PERCEPTION                STEP 2: UNDERSTANDING              STEP 3: ACTION \n                             \n     CAMERA                         WHISPER & LLM                    ROBOT          \n     CAPTURE          PROCESSING           EXECUTION      \n                                                                                    \n    RGB Images                    Speech-to-Text                  ROS 2 Commands \n    Depth Data                    Intent Recognition              Navigation     \n    Point Clouds                  Task Planning                   Manipulation   \n                             \n                                                                                          \n                                                                                          \n                             \n     OBJECT                         COMMAND                          PHYSICAL       \n     DETECTION        GENERATION           INTERACTION    \n                                                                                    \n    Isaac ROS                     High-level to                   Walking        \n     DetectNet                      low-level mapping               Grasping       \n    Semantic                      Path planning                   Navigation     \n     Segmentation                  Motion sequences                Manipulation   \n                             \n                                                                                          \n                            \n                                                                                            \n                                 \n                                         VISION-LANGUAGE-ACTION LOOP                     \n                                                                                        \n                                  Continuous perception-action cycle                   \n                                  Real-time feedback and adaptation                    \n                                  Closed-loop control with sensory feedback            \n                                  Task completion verification                           \n                                 \n\n```\n\n## 5. Humanoid-Specific VLA Architecture\n\n### Diagram: humanoid-vla-architecture.svg\n\n```\n\n                     HUMANOID-SPECIFIC VLA ARCHITECTURE                          \n\n                                                                                 \n           \n     VOICE/SPEECH     HUMANOID             PHYSICAL LOCOMOTION    \n     COMMAND              COMMAND                  CONTROL                \n                          PROCESSING                                      \n    \"Go to kitchen\"                              Walking Pattern Gen    \n    \"Pick up cup\"       Task decomposition      Balance Control        \n    \"Navigate to        Gait planning           Step Planning          \n     table\"              Obstacle avoidance      Fall Prevention        \n           \n                                                                               \n                                                                               \n           \n     SPEECH           TASK PLANNER         JOINT TRAJECTORY       \n     RECOGNITION          (LLM + NAV2)             GENERATOR              \n                                                                          \n    Whisper             Path planning           Inverse kinematics     \n    Text output         Action sequencing       Joint position cmds    \n    Intent parsing      Context awareness       Balance constraints    \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                 HUMANOID ROBOT CONTROL SYSTEM                   \n                                                                                \n                          ROS 2 Control Framework                              \n                          Joint position/effort controllers                    \n                          Whole-body motion control                            \n                          Center of Mass (CoM) management                      \n                          Zero Moment Point (ZMP) control                      \n                         \n\n```\n\n## 6. Isaac Sim Integration Pipeline\n\n### Diagram: isaac-sim-vla-integration.svg\n\n```\n\n                        ISAAC SIM VLA INTEGRATION                               \n\n                                                                                 \n           \n     PHYSICAL         ISAAC SIM           SYNTHETIC DATA         \n     WORLD                SIMULATION              GENERATION             \n                                                                        \n    Real sensors        Gazebo/Unity           Training datasets      \n    Real robots         Physics engine         Ground truth labels    \n    Real humans         Sensor simulation      Annotation tools       \n           \n                                                                               \n                                                                               \n           \n     REAL SENSORS     ISAAC ROS BRIDGE    TRAINED AI MODELS      \n     (CAMERA, LIDAR)      (MESSAGE PASSING)       (DEPLOYMENT)           \n                                                                       \n    Camera feeds        Topic mapping         Isaac ROS packages     \n    LIDAR scans         TF management         TensorRT optimization  \n    IMU data            Service calls         GPU acceleration       \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                 ROS 2 PERCEPTION & NAVIGATION                   \n                                                                                \n                          Isaac ROS Visual SLAM for localization               \n                          Isaac ROS DetectNet for object detection             \n                          Isaac ROS Bi3D for 3D perception                     \n                          Isaac ROS Navigation for path planning               \n                          Isaac ROS Pose Estimation for manipulation           \n                         \n\n```\n\n## 7. VLA System Data Flow\n\n### Diagram: vla-data-flow.svg\n\n```\n\n                           VLA DATA FLOW                                         \n\n                                                                                 \n  INPUT MODALITIES              PROCESSING                    OUTPUT ACTIONS      \n                     \n     VOICE            VISION-LANGUAGE    PHYSICAL       \n     COMMAND                    FUSION                     ACTIONS        \n                                                                          \n    Microphone                Multi-modal               ROS 2 commands \n    Audio stream               attention                 Navigation     \n    Speech input              Cross-modal               Manipulation   \n               reasoning                 Locomotion     \n                                        \n                                                                                  \n                                          \n     SPEECH-TO-       LANGUAGE                                       \n     TEXT (Whisper)             UNDERSTANDING     \n                                (LLM)                       ROBOT          \n    Transcription                                          CONTROLLER     \n    Text output               Intent parsing                            \n    Confidence                Task planning             Joint control  \n              Context aware             Trajectory     \n                                          execution      \n                                                             \n                                          \n     NATURAL          ACTION                                         \n     LANGUAGE                   SEQUENCING        \n     COMMAND                                                HUMANOID       \n                               Task breakdown              ROBOT          \n    \"Go to kitchen            Motion planning             PHYSICS        \n     and bring                 Path generation                            \n     me water\"                 Safety checks              Joint dynamics \n                      Balance        \n                                                              Stability      \n                                                             \n                                          \n     COMMAND          EXECUTION                           \n     STRUCTURE                  VERIFICATION                                    \n     (JSON)                                                                     \n                               Success metrics                                 \n    Action type               Failure recovery                                \n    Parameters                Feedback loop                                   \n    Constraints               Performance eval                                \n                                           \n\n```\n\n## 8. Isaac ROS Component Integration\n\n### Diagram: isaac-ros-components-integration.svg\n\n```\n\n                        ISAAC ROS COMPONENTS INTEGRATION                        \n\n                                                                                 \n   \n                      ISAAC ROS PERCEPTION STACK                               \n                                                                                \n              \n     Isaac ROS    Isaac ROS Visual    Isaac ROS Bi3D            \n     DetectNet         SLAM                    (3D Segmentation)       \n                                                                      \n      Object          Feature tracking      Depth estimation        \n       detection       Pose estimation       3D bounding boxes       \n      Bounding        Map building          Instance segmentation   \n       boxes           Loop closure          Point cloud gen         \n              \n                                                                              \n                           \n                                                                                \n                            \n                                  ISAAC ROS NAVIGATION STACK                     \n                                                                               \n                            Isaac ROS Pose Estimation for 6DOF poses          \n                            Isaac ROS AprilTag for fiducial localization      \n                            Isaac ROS Occupancy Grid for mapping              \n                            Isaac ROS Path Planning for navigation            \n                            \n   \n                                                                                    \n                                                                                    \n   \n                           ROS 2 INTEGRATION LAYER                                \n                                                                                  \n    TF2 for coordinate transforms                                                \n    Message filters for sensor synchronization                                   \n    Parameter server for configuration                                           \n    Action clients/servers for long-running tasks                                \n   \n                                                                                    \n                                                                                    \n   \n                        NAVIGATION 2 (NAV2) STACK                                 \n                                                                                  \n    Global Planner (NavFn, A*, etc.)                                            \n    Local Planner (DWA, TEB, etc.)                                              \n    Controller (PID, MPC, etc.)                                                 \n    Costmap (Static & Local)                                                    \n    Behavior Trees for task orchestration                                        \n    Recovery behaviors for failure handling                                      \n   \n\n```\n\n## 9. Humanoid Balance and Locomotion Control\n\n### Diagram: humanoid-balance-control.svg\n\n```\n\n                      HUMANOID BALANCE & LOCOMOTION                              \n\n                                                                                 \n           \n     SENSORY          BALANCE             LOCOMOTION            \n     FEEDBACK             CONTROLLER              GENERATION            \n                                                                      \n    IMU data            ZMP (Zero Moment      Walking pattern      \n    Joint encoders       Point) control         generation           \n    Force sensors       CoM (Center of        Gait synthesis       \n    Vision input         Mass) tracking        Step planning        \n        PID controllers       Trajectory gen       \n                              \n                                                                               \n                                                             \n     STATE                  \n     ESTIMATION       COMPENSATION        JOINT COMMANDS         \n                          GENERATION              GENERATION            \n    Robot pose                                                       \n    Velocity            Balance               Inverse kinematics   \n    Acceleration         corrections           Joint trajectories   \n    Orientation         Recovery              Torque commands      \n         actions               Compliance control   \n                              \n          \n                                                                                 \n                                                                                 \n                         \n                                    HUMANOID ROBOT PHYSICS                       \n                                                                                \n                          URDF model with accurate physical properties         \n                          Joint limits and constraints                         \n                          Collision detection                                  \n                          Dynamics simulation                                  \n                          Force/torque control                                 \n                         \n\n```\n\n## 10. VLA Performance Optimization\n\n### Diagram: vla-performance-optimization.svg\n\n```\n\n                       VLA PERFORMANCE OPTIMIZATION                              \n\n                                                                                 \n           \n     HARDWARE         ACCELERATION        OPTIMIZATION          \n     OPTIMIZATION         OPTIMIZATION            STRATEGIES            \n                                                                      \n    GPU selection       TensorRT               Pipeline parallelism \n    Memory config        optimization           Asynchronous         \n    CPU allocation      CUDA kernels            processing           \n    Network setup       Quantization           Memory management    \n        Pruning                Load balancing       \n                              \n                                                                               \n                                                             \n     COMPUTE                \n     PLATFORM         ISAAC ROS           PERFORMANCE           \n     OPTIMIZATION         OPTIMIZATION            MONITORING            \n                                                                      \n    CUDA cores          Nitros transport       Real-time metrics    \n    Tensor cores        Message batching       Bottleneck analysis  \n    VRAM config         Memory pooling         Resource utilization \n    PCIe lanes          Pipeline stages        Quality metrics      \n           \n                                                                               \n                            \n                                                                                 \n                         \n                                      VLA SYSTEM TUNING                          \n                                                                                \n                          Adjustable inference parameters                      \n                          Dynamic batch sizing                                 \n                          Adaptive resolution scaling                          \n                          Real-time performance feedback                       \n                          Automatic resource allocation                        \n                         \n\n```\n\n## Implementation Notes\n\nThese diagrams should be saved as SVG files in the `static/img/` directory and referenced in the appropriate sections of the documentation. Each diagram illustrates a key aspect of the Vision-Language-Action system for humanoid robotics:\n\n1. **System Architecture**: Shows how all components connect\n2. **Voice Pipeline**: Details the speech-to-action flow\n3. **Isaac Integration**: Shows how Isaac ROS components work together\n4. **VLA Flow**: Illustrates the complete perception-action pipeline\n5. **Humanoid Specifics**: Highlights humanoid-specific control aspects\n6. **Simulation Integration**: Shows Isaac Sim's role in the pipeline\n7. **Data Flow**: Shows how information moves through the system\n8. **Component Integration**: Details how Isaac ROS packages connect\n9. **Balance Control**: Shows humanoid-specific balance systems\n10. **Performance**: Shows optimization strategies\n\nEach diagram can be customized further based on specific implementation details and used in presentations or documentation to explain the VLA system architecture.",
    "path": "module-4-vla\\vla-architecture-diagrams.md",
    "description": ""
  },
  "module-4-vla\\vslam-navigation": {
    "title": "module-4-vla\\vslam-navigation",
    "content": "# VSLAM and Navigation\n\nVisual Simultaneous Localization and Mapping (VSLAM) is a critical technology for autonomous robots, enabling them to understand their environment and navigate without prior knowledge. This section covers VSLAM concepts and navigation techniques for humanoid robots using Isaac Sim and Isaac ROS.\n\n## Introduction to VSLAM\n\nVSLAM (Visual Simultaneous Localization and Mapping) combines computer vision and sensor data to:\n- **Localize** the robot in its environment\n- **Map** the environment in real-time\n- **Navigate** safely through the mapped space\n\n### Key Components of VSLAM\n- **Feature Detection**: Identify distinctive points in images\n- **Feature Matching**: Match features between frames\n- **Pose Estimation**: Calculate robot position and orientation\n- **Map Building**: Create and update environmental map\n- **Loop Closure**: Recognize previously visited locations\n\n## Visual SLAM Approaches\n\n### 1. Feature-Based VSLAM\nRelies on detecting and tracking distinctive features in the environment.\n\n### 2. Direct VSLAM\nUses pixel intensities directly rather than features.\n\n### 3. Semi-Direct VSLAM (SVO)\nCombines feature-based tracking with direct methods.\n\n## Popular VSLAM Systems\n\n### ORB-SLAM\n- **Features**: Real-time operation, loop closure, relocalization\n- **Strengths**: Robust, well-tested, handles monocular/stereo/RGB-D\n- **Weaknesses**: Requires texture-rich environments\n\n### LSD-SLAM\n- **Features**: Dense reconstruction, direct method\n- **Strengths**: Works in low-texture environments\n- **Weaknesses**: Computationally intensive\n\n### DSO (Direct Sparse Odometry)\n- **Features**: Direct optimization, photometric calibration\n- **Strengths**: Accurate, handles exposure changes\n- **Weaknesses**: Requires good initialization\n\n## Isaac ROS VSLAM Integration\n\n### Isaac ROS Visual SLAM Package\n\nIsaac ROS provides optimized VSLAM capabilities through the Isaac ROS Visual SLAM package:\n\n```yaml\n# Example launch configuration\nvisual_slam_node:\n  ros__parameters:\n    enable_occupancy_grid: true\n    enable_diagnostics: false\n    occupancy_grid_resolution: 0.05\n    frame_id: \"oak-d_frame\"\n    base_frame: \"base_link\"\n    odom_frame: \"odom\"\n    enable_slam_visualization: true\n    enable_landmarks_view: true\n    enable_observations_view: true\n    calibration_file: \"/tmp/calibration.json\"\n    rescale_threshold: 2.0\n```\n\n### Implementation Example\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import OccupancyGrid\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacVSLAMNode(Node):\n    def __init__(self):\n        super().__init__('isaac_vslam_node')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera and camera info\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_rect_color',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/rgb/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.pose_pub = self.create_publisher(PoseStamped, '/visual_slam/pose', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, '/visual_slam/map', 10)\n\n        # Internal state\n        self.camera_info = None\n        self.latest_image = None\n\n        self.get_logger().info('Isaac VSLAM node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera images for VSLAM\"\"\"\n        if self.camera_info is None:\n            self.get_logger().warn('Waiting for camera info...')\n            return\n\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process with Isaac ROS Visual SLAM (simulated here)\n            pose_estimate, map_update = self.process_vslam(cv_image)\n\n            # Publish results\n            if pose_estimate is not None:\n                pose_msg = PoseStamped()\n                pose_msg.header = msg.header\n                pose_msg.pose = pose_estimate\n                self.pose_pub.publish(pose_msg)\n\n            if map_update is not None:\n                map_msg = OccupancyGrid()\n                map_msg.header = msg.header\n                # Populate map message with map_update data\n                self.map_pub.publish(map_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera calibration information\"\"\"\n        self.camera_info = msg\n\n    def process_vslam(self, image):\n        \"\"\"Process image through VSLAM pipeline (simulated)\"\"\"\n        # In a real implementation, this would interface with Isaac ROS Visual SLAM\n        # For simulation, we'll return dummy data\n\n        # Simulate pose estimation\n        pose = geometry_msgs.msg.Pose()\n        # This would come from actual VSLAM processing\n        pose.position.x = 0.0  # Would be actual position\n        pose.position.y = 0.0\n        pose.position.z = 0.0\n        pose.orientation.w = 1.0  # Unit quaternion\n\n        # Simulate map update\n        map_data = None  # Would be actual map data\n\n        return pose, map_data\n```\n\n## Navigation with VSLAM\n\n### Integration with Nav2\n\nWhen using VSLAM for localization in navigation, integrate with Nav2:\n\n```yaml\n# nav2_params.yaml with VSLAM localization\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: \"base_link\"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: \"map\"  # VSLAM provides the map frame\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: \"likelihood_field\"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: \"odom\"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: \"nav2_amcl::DifferentialMotionModel\"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.2\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\n# For VSLAM, we might use a different approach than AMCL\nslam_toolbox:\n  ros__parameters:\n    use_sim_time: True\n    # SLAM Toolbox parameters for online/offline SLAM\n    solver_plugin: \"slam_toolbox::OptimizationSolverLevenbergMarquardt\"\n    ceres_linear_solver: \"SPARSE_NORMAL_CHOLESKY\"\n    ceres_preconditioner: \"SCHUR_JACOBI\"\n    ceres_trust_strategy: \"LEVENBERG_MARQUARDT\"\n    ceres_dogleg_type: \"TRADITIONAL_DOGLEG\"\n    max_iterations: 100\n    map_file_name: \"map\"\n    map_start_pose: [0.0, 0.0, 0.0]\n    map_update_interval: 5.0\n    resolution: 0.05\n    max_laser_range: 20.0\n    minimum_time_interval: 0.5\n    transform_publish_period: 0.02\n    tf_buffer_duration: 30.\n    stack_size_to_use: 40000000  # 40MB\n    enable_interactive_mode: true\n    scan_buffer_size: 30\n    scan_buffer_maximum_scan_distance: 10.0\n    scan_buffer_minimum_scan_distance: 0.1\n    scan_topic: \"/scan\"\n    mode: \"localization\"  # or \"mapping\" depending on use case\n```\n\n### Isaac Sim Navigation Scene\n\nCreate a navigation scene in Isaac Sim:\n\n```python\n# Isaac Sim navigation scene setup\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass IsaacNavigationScene:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_navigation_environment()\n\n    def setup_navigation_environment(self):\n        \"\"\"Set up a navigation environment in Isaac Sim\"\"\"\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Add a navigation robot (Carter robot)\n        robot_asset_path = assets_root_path + \"/Isaac/Robots/Carter/carter_navigate.usd\"\n        add_reference_to_stage(usd_path=robot_asset_path, prim_path=\"/World/Carter\")\n\n        # Add a simple navigation environment\n        room_asset_path = assets_root_path + \"/Isaac/Environments/Simple_Room/simple_room.usd\"\n        add_reference_to_stage(usd_path=room_asset_path, prim_path=\"/World/Room\")\n\n        # Add a LIDAR sensor to the robot\n        lidar = RotatingLidarPhysX(\n            prim_path=\"/World/Carter/chassis/lidar\",\n            translation=np.array([0.0, 0.0, 0.25]),\n            config=\"Carter_2D\",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add a camera for visual SLAM\n        camera = Camera(\n            prim_path=\"/World/Carter/chassis/camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_navigation_simulation(self, goal_position):\n        \"\"\"Run navigation simulation with obstacle avoidance\"\"\"\n        self.world.reset()\n\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get robot state\n            robot_position = self.get_robot_position()\n            robot_orientation = self.get_robot_orientation()\n            lidar_data = self.get_lidar_data()\n\n            # Check if reached goal\n            if self.is_at_goal(robot_position, goal_position):\n                print(f\"Reached goal at {goal_position}!\")\n                break\n\n            # Plan and execute navigation\n            cmd_vel = self.plan_navigation_command(\n                robot_position, robot_orientation,\n                goal_position, lidar_data\n            )\n\n            # Apply command to robot (this would interface with ROS control)\n            self.execute_command(cmd_vel)\n\n    def get_robot_position(self):\n        \"\"\"Get current robot position from Isaac Sim\"\"\"\n        # In a real implementation, this would get the robot's position\n        # from the simulation\n        pass\n\n    def get_robot_orientation(self):\n        \"\"\"Get current robot orientation from Isaac Sim\"\"\"\n        pass\n\n    def get_lidar_data(self):\n        \"\"\"Get current LIDAR data from Isaac Sim\"\"\"\n        pass\n\n    def is_at_goal(self, current_pos, goal_pos, tolerance=0.2):\n        \"\"\"Check if robot is at goal position\"\"\"\n        distance = np.sqrt((current_pos[0] - goal_pos[0])**2 + (current_pos[1] - goal_pos[1])**2)\n        return distance < tolerance\n\n    def plan_navigation_command(self, current_pos, current_orient, goal_pos, lidar_data):\n        \"\"\"Plan navigation command based on goal and sensor data\"\"\"\n        # Calculate direction to goal\n        dx = goal_pos[0] - current_pos[0]\n        dy = goal_pos[1] - current_pos[1]\n        goal_distance = np.sqrt(dx*dx + dy*dy)\n\n        # Calculate goal angle\n        goal_angle = np.arctan2(dy, dx)\n\n        # Get robot's current angle\n        robot_yaw = self.orientation_to_yaw(current_orient)\n\n        # Calculate angle difference\n        angle_diff = self.normalize_angle(goal_angle - robot_yaw)\n\n        # Simple proportional controller\n        linear_vel = min(0.5, goal_distance * 0.5)  # Scale with distance\n        angular_vel = angle_diff * 1.0  # Proportional control\n\n        # Obstacle avoidance\n        min_distance = np.min(lidar_data) if len(lidar_data) > 0 else float('inf')\n\n        if min_distance < 0.5:  # Obstacle detected\n            # Slow down and turn away from obstacle\n            linear_vel *= 0.3\n            angular_vel += self.avoid_obstacle(lidar_data)\n\n        # Ensure velocities are within limits\n        linear_vel = np.clip(linear_vel, 0.0, 0.5)\n        angular_vel = np.clip(angular_vel, -0.5, 0.5)\n\n        return [linear_vel, 0.0, 0.0], [0.0, 0.0, angular_vel]  # linear, angular velocities\n\n    def orientation_to_yaw(self, orientation):\n        \"\"\"Convert quaternion orientation to yaw angle\"\"\"\n        # Simplified conversion - in practice, use proper quaternion to euler conversion\n        import math\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\n    def normalize_angle(self, angle):\n        \"\"\"Normalize angle to [-pi, pi] range\"\"\"\n        while angle > np.pi:\n            angle -= 2*np.pi\n        while angle < -np.pi:\n            angle += 2*np.pi\n        return angle\n\n    def avoid_obstacle(self, lidar_data):\n        \"\"\"Calculate avoidance angular velocity based on LIDAR data\"\"\"\n        if len(lidar_data) == 0:\n            return 0.0\n\n        # Find the direction of the closest obstacle\n        min_idx = np.argmin(lidar_data)\n        angle_resolution = 2 * np.pi / len(lidar_data)\n        obstacle_angle = min_idx * angle_resolution - np.pi  # Convert to [-pi, pi]\n\n        # Turn away from the obstacle\n        # If obstacle is on the right, turn left (negative angular velocity)\n        # If obstacle is on the left, turn right (positive angular velocity)\n        if abs(obstacle_angle) < np.pi/2:  # Obstacle is in front\n            return -np.sign(obstacle_angle) * 0.3  # Turn away from obstacle\n        else:\n            return 0.0  # Obstacle is behind, no need to turn\n\n    def execute_command(self, cmd_vel):\n        \"\"\"Execute the navigation command in Isaac Sim\"\"\"\n        # In a real implementation, this would send the command to the robot\n        # controller in Isaac Sim\n        pass\n```\n\n## Isaac ROS Navigation Components\n\n### Isaac ROS Navigation Stack\n\nIsaac ROS provides navigation components optimized for NVIDIA hardware:\n\n```cpp\n// Isaac ROS Navigation Integration Example\n#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass IsaacNavigationNode : public rclcpp::Node\n{\npublic:\n    IsaacNavigationNode() : Node(\"isaac_navigation_node\")\n    {\n        // Subscriptions\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            \"odom\", 10,\n            std::bind(&IsaacNavigationNode::odomCallback, this, std::placeholders::_1)\n        );\n\n        scan_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\n            \"scan\", 10,\n            std::bind(&IsaacNavigationNode::scanCallback, this, std::placeholders::_1)\n        );\n\n        goal_sub_ = this->create_subscription<geometry_msgs::msg::PoseStamped>(\n            \"goal\", 10,\n            std::bind(&IsaacNavigationNode::goalCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for velocity commands\n        cmd_vel_pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\"cmd_vel\", 10);\n\n        // Timer for navigation control loop\n        control_timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(100),  // 10 Hz\n            std::bind(&IsaacNavigationNode::controlLoop, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Isaac Navigation Node initialized\");\n    }\n\nprivate:\n    void odomCallback(const nav_msgs::msg::Odometry::SharedPtr msg)\n    {\n        current_pose_ = msg->pose.pose;\n        current_twist_ = msg->twist.twist;\n        has_odom_ = true;\n    }\n\n    void scanCallback(const sensor_msgs::msg::LaserScan::SharedPtr msg)\n    {\n        latest_scan_ = *msg;\n        has_scan_ = true;\n    }\n\n    void goalCallback(const geometry_msgs::msg::PoseStamped::SharedPtr msg)\n    {\n        goal_pose_ = msg->pose;\n        has_goal_ = true;\n\n        RCLCPP_INFO(\n            this->get_logger(),\n            \"Received new goal: (%.2f, %.2f)\",\n            goal_pose_.position.x, goal_pose_.position.y\n        );\n    }\n\n    void controlLoop()\n    {\n        if (!has_odom_ || !has_scan_ || !has_goal_) {\n            return;  // Wait for all required data\n        }\n\n        // Calculate control command\n        auto cmd_vel = calculateNavigationCommand();\n\n        // Publish command\n        cmd_vel_pub_->publish(cmd_vel);\n\n        // Check if goal reached\n        if (isGoalReached()) {\n            stopRobot();\n            has_goal_ = false;\n            RCLCPP_INFO(this->get_logger(), \"Goal reached!\");\n        }\n    }\n\n    geometry_msgs::msg::Twist calculateNavigationCommand()\n    {\n        geometry_msgs::msg::Twist cmd;\n\n        // Calculate direction to goal\n        double dx = goal_pose_.position.x - current_pose_.position.x;\n        double dy = goal_pose_.position.y - current_pose_.position.y;\n        double goal_distance = sqrt(dx*dx + dy*dy);\n        double goal_angle = atan2(dy, dx);\n\n        // Get robot's current orientation\n        tf2::Quaternion q(\n            current_pose_.orientation.x,\n            current_pose_.orientation.y,\n            current_pose_.orientation.z,\n            current_pose_.orientation.w\n        );\n        tf2::Matrix3x3 m(q);\n        double roll, pitch, yaw;\n        m.getRPY(roll, pitch, yaw);\n\n        // Calculate angle difference\n        double angle_diff = goal_angle - yaw;\n        // Normalize angle to [-, ]\n        while (angle_diff > M_PI) angle_diff -= 2*M_PI;\n        while (angle_diff < -M_PI) angle_diff += 2*M_PI;\n\n        // Simple proportional controller\n        double kp_linear = 0.5;\n        double kp_angular = 1.0;\n\n        cmd.linear.x = std::min(0.5, kp_linear * goal_distance);  // Limit max speed\n        cmd.angular.z = kp_angular * angle_diff;\n\n        // Obstacle avoidance\n        if (has_scan_) {\n            double min_distance = *std::min_element(latest_scan_.ranges.begin(), latest_scan_.ranges.end());\n\n            if (min_distance < 0.5) {  // Obstacle detected\n                // Reduce forward speed\n                cmd.linear.x *= 0.3;\n\n                // Calculate avoidance based on scan data\n                cmd.angular.z += calculateObstacleAvoidance();\n            }\n        }\n\n        // Apply limits\n        cmd.linear.x = std::max(0.0, std::min(0.5, cmd.linear.x));  // Positive forward only\n        cmd.angular.z = std::max(-0.5, std::min(0.5, cmd.angular.z));\n\n        return cmd;\n    }\n\n    double calculateObstacleAvoidance()\n    {\n        if (latest_scan_.ranges.empty()) return 0.0;\n\n        // Find closest obstacle in front of robot (60 degrees)\n        int start_idx = static_cast<int>((M_PI/3 - latest_scan_.angle_min) / latest_scan_.angle_increment);\n        int end_idx = static_cast<int>((M_PI/3 + latest_scan_.angle_min) / latest_scan_.angle_increment);\n\n        start_idx = std::max(0, start_idx);\n        end_idx = std::min(static_cast<int>(latest_scan_.ranges.size()), end_idx);\n\n        double min_front_distance = std::numeric_limits<double>::infinity();\n        int min_front_idx = -1;\n\n        for (int i = start_idx; i < end_idx; ++i) {\n            if (latest_scan_.ranges[i] < min_front_distance &&\n                std::isfinite(latest_scan_.ranges[i])) {\n                min_front_distance = latest_scan_.ranges[i];\n                min_front_idx = i;\n            }\n        }\n\n        if (min_front_distance < 0.5 && min_front_idx >= 0) {  // Obstacle in front\n            // Calculate angle to closest obstacle\n            double obstacle_angle = latest_scan_.angle_min + min_front_idx * latest_scan_.angle_increment;\n\n            // Turn away from obstacle\n            return -obstacle_angle * 0.5;  // Opposite direction with gain\n        }\n\n        return 0.0;  // No obstacle in front\n    }\n\n    bool isGoalReached(double distance_threshold = 0.2, double angle_threshold = 0.1)\n    {\n        double dx = goal_pose_.position.x - current_pose_.position.x;\n        double dy = goal_pose_.position.y - current_pose_.position.y;\n        double distance = sqrt(dx*dx + dy*dy);\n\n        tf2::Quaternion q(\n            current_pose_.orientation.x,\n            current_pose_.orientation.y,\n            current_pose_.orientation.z,\n            current_pose_.orientation.w\n        );\n        tf2::Matrix3x3 m(q);\n        double roll, pitch, yaw;\n        m.getRPY(roll, pitch, yaw);\n\n        double goal_yaw = atan2(dy, dx);\n        double angle_diff = std::abs(yaw - goal_yaw);\n        // Normalize angle difference\n        if (angle_diff > M_PI) angle_diff = 2*M_PI - angle_diff;\n\n        return distance < distance_threshold && angle_diff < angle_threshold;\n    }\n\n    void stopRobot()\n    {\n        geometry_msgs::msg::Twist stop_cmd;\n        stop_cmd.linear.x = 0.0;\n        stop_cmd.angular.z = 0.0;\n        cmd_vel_pub_->publish(stop_cmd);\n    }\n\n    // Subscriptions\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr scan_sub_;\n    rclcpp::Subscription<geometry_msgs::msg::PoseStamped>::SharedPtr goal_sub_;\n\n    // Publisher\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_pub_;\n\n    // Timer\n    rclcpp::TimerBase::SharedPtr control_timer_;\n\n    // Robot state\n    geometry_msgs::msg::Pose current_pose_;\n    geometry_msgs::msg::Twist current_twist_;\n    geometry_msgs::msg::Pose goal_pose_;\n    sensor_msgs::msg::LaserScan latest_scan_;\n\n    // Flags\n    bool has_odom_ = false;\n    bool has_scan_ = false;\n    bool has_goal_ = false;\n};\n```\n\n## Humanoid Navigation Considerations\n\n### Bipedal Navigation Challenges\n\nHumanoid robots present unique navigation challenges compared to wheeled robots:\n\n1. **Balance Maintenance**: Must maintain balance while moving\n2. **Step Planning**: Requires discrete foot placement planning\n3. **Dynamic Stability**: Center of mass shifts during locomotion\n4. **Terrain Adaptation**: Different gaits for different surfaces\n\n### Humanoid-Specific Navigation Strategies\n\n```python\nclass HumanoidNavigationPlanner:\n    def __init__(self):\n        self.step_height = 0.05  # 5cm step height\n        self.step_length = 0.3   # 30cm step length\n        self.step_width = 0.2    # 20cm step width\n        self.step_duration = 0.8 # 800ms per step\n\n    def plan_bipedal_path(self, start_pose, goal_pose, environment_map):\n        \"\"\"Plan a path considering humanoid bipedal constraints\"\"\"\n        # Use a path planning algorithm that considers humanoid kinematics\n        path = self.plan_path_with_constraints(start_pose, goal_pose, environment_map)\n\n        # Convert path to step sequence\n        step_sequence = self.convert_path_to_steps(path)\n\n        return step_sequence\n\n    def convert_path_to_steps(self, path):\n        \"\"\"Convert continuous path to discrete step sequence for humanoid\"\"\"\n        steps = []\n\n        for i in range(1, len(path)):\n            # Calculate step direction and distance\n            dx = path[i].x - path[i-1].x\n            dy = path[i].y - path[i-1].y\n            step_distance = math.sqrt(dx*dx + dy*dy)\n\n            # Determine which foot to step with (alternating)\n            foot = 'left' if len(steps) % 2 == 0 else 'right'\n\n            # Calculate step parameters\n            step = {\n                'foot': foot,\n                'position': (path[i].x, path[i].y, path[i].z),\n                'orientation': self.calculate_step_orientation(dx, dy),\n                'height': self.step_height,\n                'duration': self.step_duration\n            }\n\n            steps.append(step)\n\n        return steps\n\n    def execute_bipedal_navigation(self, step_sequence):\n        \"\"\"Execute navigation using bipedal locomotion\"\"\"\n        for step in step_sequence:\n            # Execute single step with balance control\n            self.execute_single_step(step)\n\n            # Wait for step completion\n            time.sleep(step['duration'])\n\n            # Verify balance and adjust if needed\n            self.verify_balance()\n\n    def execute_single_step(self, step):\n        \"\"\"Execute a single step with balance control\"\"\"\n        # This would interface with the humanoid's walking controller\n        # Implementation would include:\n        # 1. Balance preparation\n        # 2. Swing leg trajectory planning\n        # 3. Step execution with balance feedback\n        # 4. Post-step balance recovery\n        pass\n\n    def verify_balance(self):\n        \"\"\"Verify robot balance during navigation\"\"\"\n        # Check center of mass position\n        # Verify joint positions and velocities\n        # Adjust if balance is compromised\n        pass\n```\n\n## Isaac Sim Navigation Integration\n\n### Complete Navigation Example\n\n```python\n# Complete Isaac Sim navigation example\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.navigation import NavigationGraph\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass IsaacSimNavigationExample:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_complete_navigation_scene()\n\n    def setup_complete_navigation_scene(self):\n        \"\"\"Set up a complete navigation scene with Isaac Sim\"\"\"\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print(\"Could not find Isaac Sim assets. Please check your Isaac Sim installation.\")\n            return\n\n        # Add a humanoid robot\n        robot_path = assets_root_path + \"/Isaac/Robots/Humanoid/humanoid_instanceable.usd\"\n        add_reference_to_stage(usd_path=robot_path, prim_path=\"/World/HumanoidRobot\")\n\n        # Add a complex navigation environment\n        env_path = assets_root_path + \"/Isaac/Environments/Office/office.usd\"\n        add_reference_to_stage(usd_path=env_path, prim_path=\"/World/OfficeEnvironment\")\n\n        # Add sensors for navigation\n        self.camera = Camera(\n            prim_path=\"/World/HumanoidRobot/base_link/camera\",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        self.lidar = RotatingLidarPhysX(\n            prim_path=\"/World/HumanoidRobot/base_link/lidar\",\n            translation=np.array([0.0, 0.0, 0.5]),  # Higher for humanoid\n            config=\"Carter_2D\",  # Using Carter config as base\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Create navigation graph for path planning\n        self.nav_graph = NavigationGraph(\n            prim_path=\"/World/navigation_graph\",\n            scene_path=\"/World/OfficeEnvironment\"\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_complete_navigation(self):\n        \"\"\"Run complete navigation with VSLAM and path planning\"\"\"\n        self.world.reset()\n\n        # Define navigation goals\n        goals = [\n            [2.0, 2.0, 0.0],   # Goal 1\n            [5.0, -1.0, 0.0],  # Goal 2\n            [-1.0, -3.0, 0.0], # Goal 3\n            [0.0, 0.0, 0.0]    # Return to start\n        ]\n\n        for goal in goals:\n            print(f\"Navigating to goal: {goal}\")\n\n            # Get current robot position\n            robot_pos = self.get_robot_position()\n\n            # Plan path using navigation graph\n            path = self.nav_graph.plan_path(robot_pos, goal)\n\n            if path:\n                # Execute navigation with obstacle avoidance\n                self.follow_path_with_obstacle_avoidance(path)\n            else:\n                print(f\"No path found to goal: {goal}\")\n\n            # Wait a bit before next goal\n            time.sleep(2)\n\n    def follow_path_with_obstacle_avoidance(self, path):\n        \"\"\"Follow a path with dynamic obstacle avoidance\"\"\"\n        for waypoint in path:\n            # Move towards waypoint with local obstacle avoidance\n            while not self.reached_waypoint(waypoint):\n                # Get sensor data\n                lidar_data = self.lidar.get_linear_depth_data()\n\n                # Calculate navigation command\n                cmd_vel = self.calculate_obstacle_aware_navigation(waypoint, lidar_data)\n\n                # Execute command\n                self.send_velocity_command(cmd_vel)\n\n                # Step simulation\n                self.world.step(render=True)\n\n    def calculate_obstacle_aware_navigation(self, target_waypoint, lidar_data):\n        \"\"\"Calculate navigation command with obstacle avoidance\"\"\"\n        # Get current robot state\n        robot_pos = self.get_robot_position()\n        robot_orient = self.get_robot_orientation()\n\n        # Calculate direction to target\n        dx = target_waypoint[0] - robot_pos[0]\n        dy = target_waypoint[1] - robot_pos[1]\n        target_angle = np.arctan2(dy, dx)\n\n        # Get robot's current heading\n        robot_yaw = self.orientation_to_yaw(robot_orient)\n\n        # Calculate angle difference\n        angle_diff = self.normalize_angle(target_angle - robot_yaw)\n\n        # Base navigation command\n        linear_vel = min(0.3, np.sqrt(dx*dx + dy*dy) * 0.5)  # Scale with distance\n        angular_vel = angle_diff * 1.0\n\n        # Check for obstacles using LIDAR data\n        min_distance = np.min(lidar_data) if len(lidar_data) > 0 else float('inf')\n\n        if min_distance < 0.8:  # Potential obstacle\n            # Implement Dynamic Window Approach (DWA) for local planning\n            cmd_vel = self.dwa_local_planning(\n                linear_vel, angular_vel,\n                lidar_data, robot_pos, target_waypoint\n            )\n        else:\n            # Pure pursuit to target\n            cmd_vel = geometry_msgs.msg.Twist()\n            cmd_vel.linear.x = linear_vel\n            cmd_vel.angular.z = angular_vel\n\n        # Apply velocity limits\n        cmd_vel.linear.x = np.clip(cmd_vel.linear.x, 0.0, 0.5)\n        cmd_vel.angular.z = np.clip(cmd_vel.angular.z, -0.5, 0.5)\n\n        return cmd_vel\n\n    def dwa_local_planning(self, desired_linear, desired_angular, lidar_data, robot_pos, goal_pos):\n        \"\"\"Dynamic Window Approach for local path planning with obstacles\"\"\"\n        # Define velocity search space\n        v_min = 0.0\n        v_max = 0.5\n        w_min = -0.5\n        w_max = 0.5\n\n        # Define acceleration limits\n        acc_lin = 0.5  # m/s^2\n        acc_ang = 1.0  # rad/s^2\n        dt = 0.1  # time step\n\n        # Get current velocities (would come from robot state)\n        current_v = 0.1  # placeholder\n        current_w = 0.0  # placeholder\n\n        # Calculate velocity windows\n        v_window = [max(v_min, current_v - acc_lin*dt), min(v_max, current_v + acc_lin*dt)]\n        w_window = [max(w_min, current_w - acc_ang*dt), min(w_max, current_w + acc_ang*dt)]\n\n        best_score = -float('inf')\n        best_cmd = geometry_msgs.msg.Twist()\n\n        # Sample velocities in the window\n        for v_sample in np.linspace(v_window[0], v_window[1], 10):\n            for w_sample in np.linspace(w_window[0], w_window[1], 10):\n                # Simulate trajectory\n                sim_positions = self.simulate_trajectory(robot_pos, current_v, current_w, v_sample, w_sample, dt)\n\n                # Evaluate trajectory\n                goal_score = self.evaluate_goal_distance(sim_positions, goal_pos)\n                obs_score = self.evaluate_obstacle_clearance(sim_positions, lidar_data)\n                speed_score = self.evaluate_speed(v_sample)\n\n                # Weighted score\n                total_score = 0.8 * goal_score + 0.1 * obs_score + 0.1 * speed_score\n\n                if total_score > best_score:\n                    best_score = total_score\n                    best_cmd.linear.x = v_sample\n                    best_cmd.angular.z = w_sample\n\n        return best_cmd\n\n    def simulate_trajectory(self, start_pos, v_start, w_start, v_target, w_target, dt, steps=5):\n        \"\"\"Simulate robot trajectory over time\"\"\"\n        positions = [start_pos]\n        pos = [start_pos[0], start_pos[1], start_pos[2]]\n        theta = 0  # Would get from robot orientation\n\n        for i in range(steps):\n            # Simple kinematic model for differential drive\n            pos[0] += v_target * np.cos(theta) * dt\n            pos[1] += v_target * np.sin(theta) * dt\n            theta += w_target * dt\n\n            positions.append(pos.copy())\n\n        return positions\n\n    def evaluate_goal_distance(self, trajectory, goal_pos):\n        \"\"\"Evaluate how close the trajectory gets to the goal\"\"\"\n        if not trajectory:\n            return -float('inf')\n\n        final_pos = trajectory[-1]\n        distance = np.sqrt((final_pos[0] - goal_pos[0])**2 + (final_pos[1] - goal_pos[1])**2)\n        return 1.0 / (1.0 + distance)  # Higher score for closer distances\n\n    def evaluate_obstacle_clearance(self, trajectory, lidar_data):\n        \"\"\"Evaluate how well the trajectory avoids obstacles\"\"\"\n        if not trajectory or len(lidar_data) == 0:\n            return -float('inf')\n\n        min_clearance = float('inf')\n\n        for pos in trajectory:\n            # Convert position to polar coordinates relative to robot\n            # and check against LIDAR data\n            pass\n\n        return min_clearance  # Higher score for greater clearance\n\n    def evaluate_speed(self, velocity):\n        \"\"\"Evaluate desirability of a particular speed\"\"\"\n        # Prefer faster but safe speeds\n        return velocity\n\n    def send_velocity_command(self, cmd_vel):\n        \"\"\"Send velocity command to robot in Isaac Sim\"\"\"\n        # This would interface with the robot's controller\n        # In Isaac Sim, this might involve setting joint velocities\n        # or using a ROS interface if ROS bridge is enabled\n        pass\n```\n\n## Performance Optimization\n\n### VSLAM Optimization Techniques\n\n```cpp\n// Optimized VSLAM node with performance considerations\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass OptimizedVSLAMNode : public rclcpp::Node\n{\npublic:\n    OptimizedVSLAMNode() : Node(\"optimized_vslam_node\")\n    {\n        // Use intra-process communication where possible\n        rclcpp::QoS qos(10);\n        qos.best_effort();  // For sensor data\n\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            \"camera/image_rect_color\", qos,\n            std::bind(&OptimizedVSLAMNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        // Throttle processing if needed\n        processing_rate_ = this->declare_parameter(\"processing_rate\", 15.0);  // Hz\n        timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(static_cast<int>(1000.0 / processing_rate_)),\n            std::bind(&OptimizedVSLAMNode::processCallback, this)\n        );\n\n        pose_pub_ = this->create_publisher<geometry_msgs::msg::PoseStamped>(\n            \"visual_slam/pose\", 10\n        );\n\n        // Initialize feature detector with optimized parameters\n        orb_detector_ = cv::ORB::create(\n            1000,        // nfeatures\n            1.2f,        // scaleFactor\n            4,           // nlevels\n            31,          // edgeThreshold\n            0,           // firstLevel\n            2,           // WTA_K\n            cv::ORB::HARRIS_SCORE,  // scoreType\n            31,          // patchSize\n            20           // fastThreshold\n        );\n\n        RCLCPP_INFO(this->get_logger(), \"Optimized VSLAM node initialized\");\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Store image for processing at fixed rate\n        latest_image_ = msg;\n        image_available_ = true;\n    }\n\n    void processCallback()\n    {\n        if (!image_available_) return;\n\n        try {\n            // Convert ROS image to OpenCV\n            cv_bridge::CvImagePtr cv_ptr = cv_bridge::toCvCopy(latest_image_, \"bgr8\");\n\n            // Process image for VSLAM\n            auto pose_estimate = processVSLAM(cv_ptr->image);\n\n            if (pose_estimate.has_value()) {\n                // Publish pose estimate\n                auto pose_msg = geometry_msgs::msg::PoseStamped();\n                pose_msg.header = latest_image_->header;\n                pose_msg.pose = pose_estimate.value();\n                pose_pub_->publish(pose_msg);\n            }\n\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), \"cv_bridge exception: %s\", e.what());\n        }\n\n        image_available_ = false;\n    }\n\n    std::optional<geometry_msgs::msg::Pose> processVSLAM(const cv::Mat& image)\n    {\n        // Convert to grayscale for feature detection\n        cv::Mat gray_image;\n        if (image.channels() == 3) {\n            cv::cvtColor(image, gray_image, cv::COLOR_BGR2GRAY);\n        } else {\n            gray_image = image;\n        }\n\n        // Detect and compute features\n        std::vector<cv::KeyPoint> keypoints;\n        cv::Mat descriptors;\n        orb_detector_->detectAndCompute(gray_image, cv::noArray(), keypoints, descriptors);\n\n        if (keypoints.size() < 50) {  // Require minimum features\n            RCLCPP_WARN_THROTTLE(\n                this->get_logger(),\n                *this->get_clock(),\n                1000,  // 1 second throttle\n                \"Insufficient features detected: %zu\",\n                keypoints.size()\n            );\n            return std::nullopt;\n        }\n\n        // In a real implementation, this would include:\n        // - Feature matching with previous frame\n        // - Pose estimation using PnP or similar\n        // - Bundle adjustment\n        // - Loop closure detection\n        // - Map maintenance\n\n        // For this example, return a dummy pose (would be calculated from features)\n        geometry_msgs::msg::Pose pose;\n        pose.position.x += 0.01;  // Simulate forward movement\n        pose.orientation.w = 1.0;  // Unit quaternion\n\n        return pose;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_pub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n\n    sensor_msgs::msg::Image::SharedPtr latest_image_;\n    bool image_available_ = false;\n    double processing_rate_;\n\n    cv::Ptr<cv::ORB> orb_detector_;\n    std::vector<cv::KeyPoint> previous_keypoints_;\n    cv::Mat previous_descriptors_;\n};\n```\n\n## Troubleshooting Common Issues\n\n### 1. Drift in VSLAM\n**Problem**: Accumulated errors causing position drift over time\n**Solutions**:\n- Implement loop closure detection\n- Use sensor fusion with IMU/odometry\n- Regular relocalization against known features\n\n### 2. Low-Texture Environments\n**Problem**: Insufficient features for tracking\n**Solutions**:\n- Use direct methods (LSD-SLAM, DSO)\n- Add artificial markers or fiducials\n- Combine with other sensors (LIDAR, depth)\n\n### 3. Dynamic Objects\n**Problem**: Moving objects affecting map/pose estimation\n**Solutions**:\n- Implement dynamic object detection and filtering\n- Use semantic segmentation to identify static objects\n- Temporal consistency checks\n\n### 4. Computational Performance\n**Problem**: VSLAM consuming too many resources\n**Solutions**:\n- Reduce feature count in parameters\n- Lower processing frequency\n- Use GPU acceleration where available\n- Optimize feature detection algorithms\n\n## Best Practices\n\n### 1. Robust Initialization\n- Ensure good initial pose estimate\n- Verify camera calibration\n- Check lighting conditions\n\n### 2. Parameter Tuning\n- Adjust parameters based on environment\n- Monitor performance metrics\n- Use adaptive parameters when possible\n\n### 3. Sensor Fusion\n- Combine VSLAM with other sensors\n- Use IMU for motion prediction\n- Integrate with wheel odometry\n\n### 4. Performance Monitoring\n- Track processing time per frame\n- Monitor memory usage\n- Validate trajectory accuracy\n\n## Exercise\n\nCreate a complete VSLAM and navigation system that includes:\n\n1. Implement a VSLAM pipeline using Isaac ROS components\n2. Integrate the VSLAM system with Nav2 for localization\n3. Create a navigation scene in Isaac Sim with obstacles\n4. Implement obstacle avoidance in the navigation system\n5. Test the complete system in simulation\n6. Evaluate the system's performance in terms of localization accuracy and navigation success rate\n\nThe system should be able to:\n- Build a map of an unknown environment\n- Localize the robot within the map\n- Navigate to specified goals while avoiding obstacles\n- Handle dynamic environments and recover from localization failures\n\nTest your system with various scenarios including:\n- Navigation in cluttered environments\n- Recovery from localization failures\n- Performance under different lighting conditions\n- Robustness to sensor noise",
    "path": "module-4-vla\\vslam-navigation.md",
    "description": ""
  },
  "module-4-vla\\whisper-speech": {
    "title": "module-4-vla\\whisper-speech",
    "content": "# Whisper Speech Processing\n\nOpenAI's Whisper model is a state-of-the-art automatic speech recognition (ASR) system that can transcribe speech to text with remarkable accuracy. In this section, we'll explore how to integrate Whisper into your humanoid robot's communication system.\n\n## Introduction to Whisper\n\nWhisper is a general-purpose speech recognition model trained on 680,000 hours of multilingual and multitask supervised data. It demonstrates strong performance in:\n- **Automatic Speech Recognition (ASR)**: Converting speech to text\n- **Speech Translation**: Translating speech from one language to another\n- **Language Identification**: Determining the spoken language\n- **Voice Activity Detection**: Identifying when speech occurs\n\n### Whisper Model Variants\n\nThere are several Whisper model variants with different sizes and capabilities:\n\n- **tiny**: Fastest, smallest (39M parameters)\n- **base**: Small (74M parameters)\n- **small**: Medium (244M parameters)\n- **medium**: Large (769M parameters)\n- **large**: Largest, most accurate (1550M parameters)\n\nFor robotics applications, the choice depends on:\n- **Accuracy requirements**: Larger models provide better accuracy\n- **Computational resources**: Smaller models run faster with less memory\n- **Latency requirements**: Real-time applications may need faster models\n\n## Whisper Integration with Robotics\n\n### Installation and Setup\n\nFirst, install the required dependencies:\n\n```bash\npip install openai-whisper\npip install sounddevice  # For audio input\npip install pyaudio      # Alternative audio input\npip install transformers # For LLM integration\n```\n\n### Basic Whisper Usage\n\n```python\nimport whisper\nimport torch\n\n# Load the Whisper model\nmodel = whisper.load_model(\"small\")  # Choose tiny, base, small, medium, or large\n\n# Transcribe audio\nresult = model.transcribe(\"path/to/audio.wav\")\nprint(result[\"text\"])\n```\n\n### Real-time Audio Processing\n\nFor real-time speech processing in robotics, we need to capture and process audio streams:\n\n```python\nimport whisper\nimport numpy as np\nimport sounddevice as sd\nimport queue\nimport threading\nimport time\n\nclass RealTimeWhisper:\n    def __init__(self, model_size=\"small\"):\n        # Load Whisper model\n        self.model = whisper.load_model(model_size)\n\n        # Audio parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.chunk_duration = 1.0  # Process 1-second chunks\n        self.chunk_size = int(self.sample_rate * self.chunk_duration)\n\n        # Audio buffer\n        self.audio_queue = queue.Queue()\n        self.transcript_queue = queue.Queue()\n\n        # Flags\n        self.recording = False\n\n    def audio_callback(self, indata, frames, time, status):\n        \"\"\"Callback for audio input\"\"\"\n        if status:\n            print(status)\n        # Add audio data to queue\n        self.audio_queue.put(indata[:, 0].copy())\n\n    def start_recording(self):\n        \"\"\"Start recording audio\"\"\"\n        self.recording = True\n\n        # Start audio stream\n        self.stream = sd.InputStream(\n            samplerate=self.sample_rate,\n            blocksize=self.chunk_size,\n            channels=1,\n            dtype='float32',\n            callback=self.audio_callback\n        )\n        self.stream.start()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.start()\n\n    def stop_recording(self):\n        \"\"\"Stop recording audio\"\"\"\n        self.recording = False\n        self.stream.stop()\n        self.stream.close()\n\n    def process_audio(self):\n        \"\"\"Process audio chunks in a separate thread\"\"\"\n        audio_buffer = np.array([])\n\n        while self.recording:\n            try:\n                # Get audio chunk\n                chunk = self.audio_queue.get(timeout=0.1)\n\n                # Add to buffer\n                audio_buffer = np.concatenate([audio_buffer, chunk])\n\n                # Process when we have enough audio\n                if len(audio_buffer) >= self.chunk_size:\n                    # Process the audio\n                    transcript = self.transcribe_chunk(audio_buffer)\n\n                    # Add to transcript queue\n                    if transcript.strip():  # Only add non-empty transcripts\n                        self.transcript_queue.put(transcript)\n\n                    # Keep remaining audio in buffer\n                    audio_buffer = audio_buffer[self.chunk_size:]\n\n            except queue.Empty:\n                continue\n\n    def transcribe_chunk(self, audio_chunk):\n        \"\"\"Transcribe a chunk of audio\"\"\"\n        # Convert to tensor\n        audio_tensor = torch.from_numpy(audio_chunk).float()\n\n        # Transcribe\n        result = self.model.transcribe(audio_tensor.numpy())\n\n        return result[\"text\"]\n\n    def get_transcript(self):\n        \"\"\"Get next transcript from queue\"\"\"\n        try:\n            return self.transcript_queue.get_nowait()\n        except queue.Empty:\n            return None\n\n# Usage example\nwhisper_robot = RealTimeWhisper(model_size=\"small\")\nwhisper_robot.start_recording()\n\ntry:\n    while True:\n        transcript = whisper_robot.get_transcript()\n        if transcript:\n            print(f\"Robot heard: {transcript}\")\n            # Process the transcript for robot actions\n\n        time.sleep(0.1)\nexcept KeyboardInterrupt:\n    whisper_robot.stop_recording()\n```\n\n## Whisper with ROS 2 Integration\n\n### Creating a Whisper ROS 2 Node\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nfrom io import BytesIO\nimport wave\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Load Whisper model\n        self.model_size = self.declare_parameter('model_size', 'small').get_parameter_value().string_value\n        self.model = whisper.load_model(self.model_size)\n\n        # Subscribe to audio data\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10\n        )\n\n        # Publish transcriptions\n        self.transcript_pub = self.create_publisher(\n            String,\n            'speech_transcription',\n            10\n        )\n\n        self.get_logger().info(f'Whisper node initialized with {self.model_size} model')\n\n    def audio_callback(self, msg):\n        \"\"\"Process incoming audio data\"\"\"\n        try:\n            # Convert audio data to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Transcribe the audio\n            result = self.model.transcribe(audio_data)\n\n            # Publish transcription\n            transcript_msg = String()\n            transcript_msg.data = result[\"text\"]\n            self.transcript_pub.publish(transcript_msg)\n\n            self.get_logger().info(f'Transcribed: {result[\"text\"]}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperNode()\n\n    try:\n        rclpy.spin(whisper_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        whisper_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n```\n\n### Launch File for Whisper Node\n\n```xml\n<!-- launch/whisper_node.launch.py -->\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='robot_voice_interface',\n            executable='whisper_node',\n            name='whisper_node',\n            parameters=[\n                {'model_size': 'small'}  # Choose tiny, base, small, medium, or large\n            ],\n            remappings=[\n                ('/audio_input', '/microphone/audio_raw'),\n                ('/speech_transcription', '/voice_commands')\n            ]\n        )\n    ])\n```\n\n## Advanced Whisper Features\n\n### Language Detection and Multilingual Support\n\n```python\ndef detect_language_and_transcribe(self, audio_data):\n    \"\"\"Detect language and transcribe accordingly\"\"\"\n    # Detect language\n    audio_tensor = torch.from_numpy(audio_data).float()\n    mel = whisper.log_mel_spectrogram(audio_tensor)\n\n    # Detect language\n    _, probs = self.model.detect_language(mel[:1])\n    detected_lang = max(probs, key=probs.get)\n\n    # Transcribe with detected language\n    result = self.model.transcribe(audio_data, language=detected_lang)\n\n    return result[\"text\"], detected_lang\n```\n\n### Improved Real-time Processing with VAD (Voice Activity Detection)\n\n```python\nimport webrtcvad  # pip install webrtcvad\n\nclass SmartWhisperNode(Node):\n    def __init__(self):\n        super().__init__('smart_whisper_node')\n\n        # Load Whisper model\n        self.model = whisper.load_model(\"small\")\n\n        # Voice activity detection\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(1)  # Aggressiveness mode (0-3)\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Speech detection parameters\n        self.speech_buffer = []\n        self.silence_threshold = 50  # frames of silence to trigger processing\n        self.silence_count = 0\n        self.is_speaking = False\n\n        # ROS 2 setup\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.smart_audio_callback,\n            10\n        )\n\n        self.transcript_pub = self.create_publisher(String, 'speech_transcription', 10)\n\n    def smart_audio_callback(self, msg):\n        \"\"\"Process audio with voice activity detection\"\"\"\n        # Convert to 16-bit PCM for VAD\n        audio_int16 = np.frombuffer(msg.data, dtype=np.int16)\n\n        # Process in frames\n        for i in range(0, len(audio_int16), self.frame_size):\n            frame = audio_int16[i:i+self.frame_size]\n\n            # Pad frame if necessary\n            if len(frame) < self.frame_size:\n                frame = np.pad(frame, (0, self.frame_size - len(frame)), 'constant')\n\n            # Check for voice activity\n            is_speech = self.vad.is_speech(frame.tobytes(), self.sample_rate)\n\n            if is_speech:\n                # Add to speech buffer\n                self.speech_buffer.extend(frame)\n                self.silence_count = 0\n                self.is_speaking = True\n            else:\n                # Add to silence counter\n                self.silence_count += 1\n\n                if self.is_speaking and self.silence_count > self.silence_threshold:\n                    # End of speech detected, process the buffer\n                    self.process_speech_buffer()\n                    self.is_speaking = False\n\n    def process_speech_buffer(self):\n        \"\"\"Process accumulated speech buffer\"\"\"\n        if len(self.speech_buffer) > 0:\n            # Convert to float32\n            audio_float32 = np.array(self.speech_buffer, dtype=np.float32) / 32768.0\n\n            # Transcribe\n            result = self.model.transcribe(audio_float32)\n\n            # Publish if we have a meaningful result\n            if result[\"text\"].strip():\n                transcript_msg = String()\n                transcript_msg.data = result[\"text\"]\n                self.transcript_pub.publish(transcript_msg)\n\n                self.get_logger().info(f'Speech detected: {result[\"text\"]}')\n\n            # Clear buffer\n            self.speech_buffer = []\n```\n\n## Performance Optimization\n\n### Using Local Whisper Models\n\nFor better performance and privacy, use local models:\n\n```python\n# Download model to local directory\nimport os\nfrom whisper import _download, _MODELS\n\ndef download_whisper_model(model_size, download_root=None):\n    \"\"\"Download Whisper model to local directory\"\"\"\n    if download_root is None:\n        download_root = os.path.expanduser(\"~/.cache/whisper\")\n\n    model_url = _MODELS[model_size]\n    return _download(model_url, download_root, False)\n\n# Use local model\nmodel_path = download_whisper_model(\"small\")\nmodel = whisper.load_model(model_path)\n```\n\n### Quantization for Better Performance\n\n```python\n# Load quantized model for better performance\nmodel = whisper.load_model(\"small\", device=\"cuda\", in_memory=True)\n\n# Or use CPU with FP16 for better performance\nmodel = whisper.load_model(\"small\", device=\"cpu\", fp16=True)\n```\n\n## Error Handling and Robustness\n\n### Handling Different Audio Formats\n\n```python\ndef process_audio_with_format_handling(self, audio_msg):\n    \"\"\"Handle different audio formats\"\"\"\n    try:\n        # Convert different sample rates to 16kHz\n        audio_data = self.convert_audio_format(audio_msg)\n\n        # Normalize audio\n        audio_data = self.normalize_audio(audio_data)\n\n        # Transcribe\n        result = self.model.transcribe(audio_data)\n\n        return result[\"text\"]\n    except Exception as e:\n        self.get_logger().error(f'Audio processing error: {str(e)}')\n        return \"\"\n\ndef convert_audio_format(self, audio_msg):\n    \"\"\"Convert audio to required format\"\"\"\n    # Convert to numpy array\n    if audio_msg.encoding == 'PCM_16':\n        audio_np = np.frombuffer(audio_msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n    elif audio_msg.encoding == 'PCM_32':\n        audio_np = np.frombuffer(audio_msg.data, dtype=np.int32).astype(np.float32) / 2147483648.0\n    else:\n        raise ValueError(f\"Unsupported audio encoding: {audio_msg.encoding}\")\n\n    # Resample if necessary\n    if audio_msg.rate != 16000:\n        import librosa\n        audio_np = librosa.resample(audio_np, orig_sr=audio_msg.rate, target_sr=16000)\n\n    return audio_np\n\ndef normalize_audio(self, audio_data):\n    \"\"\"Normalize audio to prevent clipping\"\"\"\n    max_val = np.max(np.abs(audio_data))\n    if max_val > 1.0:\n        audio_data = audio_data / max_val\n    return audio_data\n```\n\n## Troubleshooting Common Issues\n\n### 1. Audio Quality Issues\n**Problem**: Poor transcription accuracy\n**Solutions**:\n- Use noise reduction preprocessing\n- Ensure proper microphone positioning\n- Check audio input levels\n- Use directional microphones\n\n### 2. Performance Issues\n**Problem**: Slow processing or high latency\n**Solutions**:\n- Use smaller Whisper models\n- Optimize audio chunk sizes\n- Use GPU acceleration\n- Implement audio buffering\n\n### 3. Memory Issues\n**Problem**: High memory consumption\n**Solutions**:\n- Use CPU instead of GPU for smaller models\n- Process audio in smaller chunks\n- Implement memory cleanup\n- Use quantized models\n\n## Best Practices\n\n### 1. Audio Preprocessing\n- Apply noise reduction filters\n- Normalize audio levels\n- Use appropriate sample rates\n- Implement silence detection\n\n### 2. Model Selection\n- Choose model size based on accuracy requirements\n- Consider computational constraints\n- Test with domain-specific audio\n- Use appropriate languages\n\n### 3. Integration\n- Implement proper error handling\n- Use appropriate ROS 2 QoS settings\n- Implement buffering for smooth operation\n- Monitor performance metrics\n\n## Exercise\n\nCreate a complete Whisper integration for your humanoid robot that includes:\n\n1. Real-time audio capture from the robot's microphone\n2. Whisper-based speech-to-text processing\n3. Integration with ROS 2 for message passing\n4. Voice activity detection to reduce processing overhead\n5. Performance optimization for real-time operation\n6. Error handling for various audio conditions\n\nTest your system with various commands and evaluate the accuracy and response time.",
    "path": "module-4-vla\\whisper-speech.md",
    "description": ""
  }
}