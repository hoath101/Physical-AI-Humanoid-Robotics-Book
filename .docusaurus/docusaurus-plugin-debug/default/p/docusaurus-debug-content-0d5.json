{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/","tagsPath":"/tags","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\Baloch\\GIAIC-WORK\\hackathon-book\\sidebars.js","contentPath":"C:\\Users\\Baloch\\GIAIC-WORK\\hackathon-book\\docs","docs":[{"id":"capstone-project/autonomous-humanoid","title":"Autonomous Humanoid Project","description":"","source":"@site/docs/capstone-project/autonomous-humanoid.md","sourceDirName":"capstone-project","slug":"/capstone-project/autonomous-humanoid","permalink":"/capstone-project/autonomous-humanoid","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"autonomous-humanoid","title":"Autonomous Humanoid Project"}},{"id":"capstone-project/index","title":"Capstone Overview","description":"","source":"@site/docs/capstone-project/index.md","sourceDirName":"capstone-project","slug":"/capstone-project/","permalink":"/capstone-project/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"index","title":"Capstone Overview"}},{"id":"intro","title":"Introduction","description":"<!-- ---","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"intro","title":"Introduction","sidebar_position":1,"slug":"/"},"sidebar":"tutorialSidebar","next":{"title":"Module 1 ROS-2","permalink":"/module-1-ros2/"}},{"id":"module-1-ros2/installation-setup","title":"ROS 2 Installation and Setup","description":"This section provides comprehensive instructions for installing and setting up ROS 2 (Humble Hawksbill) for use with humanoid robotics applications. Following these steps will prepare your development environment for the exercises in this book.","source":"@site/docs/module-1-ros2/installation-setup.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/installation-setup","permalink":"/module-1-ros2/installation-setup","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 1 ROS-2","permalink":"/module-1-ros2/"},"next":{"title":"Nodes, Topics, Services","permalink":"/module-1-ros2/nodes-topics-services"}},{"id":"module-1-ros2/module-1-index","title":"Module 1 ROS-2","description":"Welcome to the first module of the Physical AI & Humanoid Robotics book! In this module, you'll learn about ROS 2 (Robot Operating System 2), the foundational middleware that enables communication and coordination in robotic systems.","source":"@site/docs/module-1-ros2/index.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/","permalink":"/module-1-ros2/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"module-1-index","title":"Module 1 ROS-2","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Introduction","permalink":"/"},"next":{"title":"ROS 2 Installation and Setup","permalink":"/module-1-ros2/installation-setup"}},{"id":"module-1-ros2/nodes-topics-services","title":"Nodes, Topics, Services","description":"In this section, we'll explore the fundamental communication patterns in ROS 2: nodes, topics, and services. These concepts form the backbone of robotic applications and enable different components to work together seamlessly.","source":"@site/docs/module-1-ros2/nodes-topics-services.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/nodes-topics-services","permalink":"/module-1-ros2/nodes-topics-services","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Installation and Setup","permalink":"/module-1-ros2/installation-setup"},"next":{"title":"Practical Exercises - ROS 2 Fundamentals","permalink":"/module-1-ros2/practical-exercises"}},{"id":"module-1-ros2/practical-exercises","title":"Practical Exercises - ROS 2 Fundamentals","description":"This section provides hands-on exercises to reinforce the concepts learned in the ROS 2 module. Complete these exercises to gain practical experience with ROS 2 nodes, topics, services, and URDF.","source":"@site/docs/module-1-ros2/practical-exercises.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/practical-exercises","permalink":"/module-1-ros2/practical-exercises","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Nodes, Topics, Services","permalink":"/module-1-ros2/nodes-topics-services"},"next":{"title":"rclpy Basics","permalink":"/module-1-ros2/rclpy-basics"}},{"id":"module-1-ros2/rclpy-basics","title":"rclpy Basics","description":"rclpy is the Python client library for ROS 2. It provides the Python API for developing ROS 2 applications, allowing you to create nodes, publish/subscribe to topics, provide/call services, and interact with other ROS 2 concepts.","source":"@site/docs/module-1-ros2/rclpy-basics.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/rclpy-basics","permalink":"/module-1-ros2/rclpy-basics","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Practical Exercises - ROS 2 Fundamentals","permalink":"/module-1-ros2/practical-exercises"},"next":{"title":"URDF Description","permalink":"/module-1-ros2/urdf-description"}},{"id":"module-1-ros2/urdf-description","title":"URDF Description","description":"URDF (Unified Robot Description Format) is an XML format used to describe robot models in ROS. It defines the physical and visual properties of a robot, including its links, joints, and other components. URDF is essential for simulating robots in Gazebo and visualizing them in RViz.","source":"@site/docs/module-1-ros2/urdf-description.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/urdf-description","permalink":"/module-1-ros2/urdf-description","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"rclpy Basics","permalink":"/module-1-ros2/rclpy-basics"},"next":{"title":"Module 2 Digital Twin","permalink":"/module-2-digital-twin/"}},{"id":"module-2-digital-twin/gazebo-simulation","title":"Gazebo Simulation","description":"Gazebo is the standard simulation environment for ROS and provides a powerful physics engine for realistic robot simulation. This section covers setting up Gazebo, creating simulation worlds, and integrating with ROS 2.","source":"@site/docs/module-2-digital-twin/gazebo-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/gazebo-simulation","permalink":"/module-2-digital-twin/gazebo-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 2 Digital Twin","permalink":"/module-2-digital-twin/"},"next":{"title":"Gazebo World Setup and Configuration","permalink":"/module-2-digital-twin/gazebo-world-setup"}},{"id":"module-2-digital-twin/gazebo-world-setup","title":"Gazebo World Setup and Configuration","description":"Setting up realistic and functional simulation environments is crucial for effective digital twin development. This section covers creating and configuring Gazebo worlds for humanoid robotics applications.","source":"@site/docs/module-2-digital-twin/gazebo-world-setup.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/gazebo-world-setup","permalink":"/module-2-digital-twin/gazebo-world-setup","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Simulation","permalink":"/module-2-digital-twin/gazebo-simulation"},"next":{"title":"Physics and Collisions","permalink":"/module-2-digital-twin/physics-collisions"}},{"id":"module-2-digital-twin/module-2-index","title":"Module 2 Digital Twin","description":"Welcome to the Digital Twin module! This module focuses on creating and validating virtual representations of physical robots, enabling safe testing and iterative development without requiring physical hardware.","source":"@site/docs/module-2-digital-twin/index.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/","permalink":"/module-2-digital-twin/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"module-2-index","title":"Module 2 Digital Twin","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"URDF Description","permalink":"/module-1-ros2/urdf-description"},"next":{"title":"Gazebo Simulation","permalink":"/module-2-digital-twin/gazebo-simulation"}},{"id":"module-2-digital-twin/physics-collisions","title":"Physics and Collisions","description":"Understanding physics and collision properties is crucial for creating realistic digital twins. This section covers the principles of physics simulation and collision detection in both Gazebo and Unity environments.","source":"@site/docs/module-2-digital-twin/physics-collisions.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/physics-collisions","permalink":"/module-2-digital-twin/physics-collisions","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo World Setup and Configuration","permalink":"/module-2-digital-twin/gazebo-world-setup"},"next":{"title":"Practical Exercises - Digital Twin Simulation","permalink":"/module-2-digital-twin/practical-exercises"}},{"id":"module-2-digital-twin/practical-exercises","title":"Practical Exercises - Digital Twin Simulation","description":"This section provides hands-on exercises to reinforce the concepts learned in the Digital Twin module. Complete these exercises to gain practical experience with Gazebo and Unity simulation environments.","source":"@site/docs/module-2-digital-twin/practical-exercises.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/practical-exercises","permalink":"/module-2-digital-twin/practical-exercises","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Physics and Collisions","permalink":"/module-2-digital-twin/physics-collisions"},"next":{"title":"Sensor Simulation","permalink":"/module-2-digital-twin/sensor-simulation"}},{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Sensor simulation is a critical component of digital twin technology, enabling robots to perceive their virtual environment just as they would in the real world. This section covers simulating various types of sensors in both Gazebo and Unity environments.","source":"@site/docs/module-2-digital-twin/sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Practical Exercises - Digital Twin Simulation","permalink":"/module-2-digital-twin/practical-exercises"},"next":{"title":"Unity Scene Setup and Physics Configuration","permalink":"/module-2-digital-twin/unity-scene-setup"}},{"id":"module-2-digital-twin/unity-scene-setup","title":"Unity Scene Setup and Physics Configuration","description":"Unity provides a powerful environment for creating high-fidelity digital twins with advanced graphics and physics capabilities. This section covers setting up Unity scenes for humanoid robotics simulation with proper physics configuration.","source":"@site/docs/module-2-digital-twin/unity-scene-setup.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/unity-scene-setup","permalink":"/module-2-digital-twin/unity-scene-setup","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation","permalink":"/module-2-digital-twin/sensor-simulation"},"next":{"title":"Unity Visualization","permalink":"/module-2-digital-twin/unity-visualization"}},{"id":"module-2-digital-twin/unity-visualization","title":"Unity Visualization","description":"Unity provides a powerful game engine environment for high-fidelity robot simulation and visualization. While Gazebo is the standard for ROS simulation, Unity offers advanced graphics capabilities and a rich ecosystem for creating detailed digital twins.","source":"@site/docs/module-2-digital-twin/unity-visualization.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/unity-visualization","permalink":"/module-2-digital-twin/unity-visualization","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Unity Scene Setup and Physics Configuration","permalink":"/module-2-digital-twin/unity-scene-setup"},"next":{"title":"URDF Model Creation and Validation","permalink":"/module-2-digital-twin/urdf-validation"}},{"id":"module-2-digital-twin/urdf-validation","title":"URDF Model Creation and Validation","description":"Creating accurate URDF models is fundamental to successful digital twin implementation. This section covers best practices for creating URDF models and validating them for use in simulation environments.","source":"@site/docs/module-2-digital-twin/urdf-validation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/urdf-validation","permalink":"/module-2-digital-twin/urdf-validation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Unity Visualization","permalink":"/module-2-digital-twin/unity-visualization"},"next":{"title":"Module 3 AI Perception","permalink":"/module-3-ai-perception/"}},{"id":"module-3-ai-perception/installation-setup","title":"Isaac Sim Installation and Setup","description":"This section provides comprehensive instructions for installing and setting up NVIDIA Isaac Sim for robotics perception and navigation applications.","source":"@site/docs/module-3-ai-perception/installation-setup.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/installation-setup","permalink":"/module-3-ai-perception/installation-setup","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 AI Perception","permalink":"/module-3-ai-perception/"},"next":{"title":"Isaac Sim Fundamentals","permalink":"/module-3-ai-perception/isaac-sim"}},{"id":"module-3-ai-perception/isaac-sim","title":"Isaac Sim Fundamentals","description":"NVIDIA Isaac Sim is a high-fidelity simulation environment built on NVIDIA Omniverse, designed for developing, testing, and validating AI-based robotics applications. This section covers the fundamentals of Isaac Sim and its role in robotics development.","source":"@site/docs/module-3-ai-perception/isaac-sim.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/isaac-sim","permalink":"/module-3-ai-perception/isaac-sim","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Installation and Setup","permalink":"/module-3-ai-perception/installation-setup"},"next":{"title":"Isaac Sim Fundamentals","permalink":"/module-3-ai-perception/isaac-sim-fundamentals"}},{"id":"module-3-ai-perception/isaac-sim-fundamentals","title":"Isaac Sim Fundamentals","description":"Isaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, designed specifically for developing, testing, and validating AI-based robotics applications. This section covers the core concepts and fundamentals of Isaac Sim.","source":"@site/docs/module-3-ai-perception/isaac-sim-fundamentals.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/isaac-sim-fundamentals","permalink":"/module-3-ai-perception/isaac-sim-fundamentals","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Fundamentals","permalink":"/module-3-ai-perception/isaac-sim"},"next":{"title":"Object Detection and Localization Examples","permalink":"/module-3-ai-perception/object-detection-localization"}},{"id":"module-3-ai-perception/module-3-index","title":"Module 3 AI Perception","description":"Welcome to Module 3 of the Physical AI & Humanoid Robotics book! This module focuses on the AI components that power modern robotics: perception, navigation, and intelligent decision-making using NVIDIA Isaac technologies.","source":"@site/docs/module-3-ai-perception/index.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/","permalink":"/module-3-ai-perception/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"module-3-index","title":"Module 3 AI Perception","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"URDF Model Creation and Validation","permalink":"/module-2-digital-twin/urdf-validation"},"next":{"title":"Isaac Sim Installation and Setup","permalink":"/module-3-ai-perception/installation-setup"}},{"id":"module-3-ai-perception/nav2-locomotion","title":"Nav2 for Humanoid Locomotion","description":"Navigation 2 (Nav2) is the ROS 2 navigation stack that enables autonomous navigation for robots. For humanoid robots, Nav2 requires special configuration to handle the unique challenges of bipedal locomotion. This section covers configuring and using Nav2 for humanoid robot navigation.","source":"@site/docs/module-3-ai-perception/nav2-locomotion.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/nav2-locomotion","permalink":"/module-3-ai-perception/nav2-locomotion","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Navigation Planning and Obstacle Avoidance Examples","permalink":"/module-3-ai-perception/navigation-planning-obstacle-avoidance"},"next":{"title":"Perception and Navigation Pipeline Diagrams","permalink":"/module-3-ai-perception/perception-navigation-pipeline-diagrams"}},{"id":"module-3-ai-perception/navigation-planning-obstacle-avoidance","title":"Navigation Planning and Obstacle Avoidance Examples","description":"This section covers navigation planning and obstacle avoidance techniques for humanoid robots using AI-powered perception and navigation systems. We'll explore how to integrate perception data with navigation planning for safe and efficient robot movement.","source":"@site/docs/module-3-ai-perception/navigation-planning-obstacle-avoidance.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/navigation-planning-obstacle-avoidance","permalink":"/module-3-ai-perception/navigation-planning-obstacle-avoidance","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Object Detection and Localization Examples","permalink":"/module-3-ai-perception/object-detection-localization"},"next":{"title":"Nav2 for Humanoid Locomotion","permalink":"/module-3-ai-perception/nav2-locomotion"}},{"id":"module-3-ai-perception/object-detection-localization","title":"Object Detection and Localization Examples","description":"This section provides practical examples of object detection and localization using NVIDIA Isaac technologies, demonstrating how AI-powered perception systems work in robotics applications.","source":"@site/docs/module-3-ai-perception/object-detection-localization.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/object-detection-localization","permalink":"/module-3-ai-perception/object-detection-localization","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Fundamentals","permalink":"/module-3-ai-perception/isaac-sim-fundamentals"},"next":{"title":"Navigation Planning and Obstacle Avoidance Examples","permalink":"/module-3-ai-perception/navigation-planning-obstacle-avoidance"}},{"id":"module-3-ai-perception/perception-navigation-pipeline-diagrams","title":"Perception and Navigation Pipeline Diagrams","description":"This section describes the diagrams that illustrate the perception and navigation pipeline for humanoid robots using NVIDIA Isaac technologies. These diagrams help visualize the data flow and system architecture.","source":"@site/docs/module-3-ai-perception/perception-navigation-pipeline-diagrams.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/perception-navigation-pipeline-diagrams","permalink":"/module-3-ai-perception/perception-navigation-pipeline-diagrams","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Humanoid Locomotion","permalink":"/module-3-ai-perception/nav2-locomotion"},"next":{"title":"Practical Exercises with Isaac AI Components","permalink":"/module-3-ai-perception/practical-exercises-isaac-ai"}},{"id":"module-3-ai-perception/practical-exercises-isaac-ai","title":"Practical Exercises with Isaac AI Components","description":"This section provides hands-on exercises to reinforce the concepts learned about Isaac AI components for perception and navigation. These exercises will help you gain practical experience with NVIDIA Isaac technologies.","source":"@site/docs/module-3-ai-perception/practical-exercises-isaac-ai.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/practical-exercises-isaac-ai","permalink":"/module-3-ai-perception/practical-exercises-isaac-ai","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Perception and Navigation Pipeline Diagrams","permalink":"/module-3-ai-perception/perception-navigation-pipeline-diagrams"},"next":{"title":"VSLAM and Navigation","permalink":"/module-3-ai-perception/vslam-navigation"}},{"id":"module-3-ai-perception/vslam-navigation","title":"VSLAM and Navigation","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a critical technology for autonomous robots, enabling them to understand their environment and navigate without prior knowledge. This section covers VSLAM concepts and navigation techniques for humanoid robots using Isaac ROS.","source":"@site/docs/module-3-ai-perception/vslam-navigation.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/vslam-navigation","permalink":"/module-3-ai-perception/vslam-navigation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Practical Exercises with Isaac AI Components","permalink":"/module-3-ai-perception/practical-exercises-isaac-ai"},"next":{"title":"Module 4 VLA","permalink":"/module-4-vla/"}},{"id":"module-4-vla/humanoid-locomotion-control","title":"Humanoid Locomotion and Control","description":"Humanoid locomotion represents one of the most challenging problems in robotics, requiring sophisticated control systems to achieve stable, efficient, and human-like movement. This section covers the principles and implementation of humanoid locomotion using Isaac Sim and Isaac ROS.","source":"@site/docs/module-4-vla/humanoid-locomotion-control.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/humanoid-locomotion-control","permalink":"/module-4-vla/humanoid-locomotion-control","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Summary: Physical AI & Humanoid Robotics Book","permalink":"/module-4-vla/summary"},"next":{"title":"Isaac ROS Integration","permalink":"/module-4-vla/isaac-ros-integration"}},{"id":"module-4-vla/isaac-ros-integration","title":"Isaac ROS Integration","description":"Isaac ROS is NVIDIA's collection of hardware-accelerated perception and navigation packages that bridge the gap between NVIDIA's GPU-accelerated AI capabilities and the ROS 2 robotics framework. This section covers how to integrate Isaac ROS packages with humanoid robotics systems.","source":"@site/docs/module-4-vla/isaac-ros-integration.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/isaac-ros-integration","permalink":"/module-4-vla/isaac-ros-integration","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Locomotion and Control","permalink":"/module-4-vla/humanoid-locomotion-control"},"next":{"title":"Isaac Sim Fundamentals","permalink":"/module-4-vla/isaac-sim-fundamentals"}},{"id":"module-4-vla/isaac-sim-fundamentals","title":"Isaac Sim Fundamentals","description":"Isaac Sim is NVIDIA's high-fidelity simulation environment built on the Omniverse platform, designed specifically for developing, testing, and validating AI-based robotics applications. This section covers the core concepts and fundamentals of Isaac Sim.","source":"@site/docs/module-4-vla/isaac-sim-fundamentals.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/isaac-sim-fundamentals","permalink":"/module-4-vla/isaac-sim-fundamentals","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Integration","permalink":"/module-4-vla/isaac-ros-integration"},"next":{"title":"LLM Planning for Robotics","permalink":"/module-4-vla/llm-planning"}},{"id":"module-4-vla/llm-planning","title":"LLM Planning for Robotics","description":"Large Language Models (LLMs) play a crucial role in robotics by enabling natural language understanding, task planning, and high-level decision making. This section covers how to integrate LLMs with humanoid robots for intelligent task planning and execution.","source":"@site/docs/module-4-vla/llm-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm-planning","permalink":"/module-4-vla/llm-planning","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Fundamentals","permalink":"/module-4-vla/isaac-sim-fundamentals"},"next":{"title":"Multimodal Perception","permalink":"/module-4-vla/multimodal-perception"}},{"id":"module-4-vla/module-4-index","title":"Module 4 VLA","description":"Welcome to Module 4 of the Physical AI & Humanoid Robotics book! This module focuses on Vision-Language-Action (VLA) systems that enable humanoid robots to understand natural language commands and execute complex physical tasks in the real world.","source":"@site/docs/module-4-vla/index.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/module-4-vla/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"module-4-index","title":"Module 4 VLA","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"VSLAM and Navigation","permalink":"/module-3-ai-perception/vslam-navigation"},"next":{"title":"Summary: Physical AI & Humanoid Robotics Book","permalink":"/module-4-vla/summary"}},{"id":"module-4-vla/multimodal-perception","title":"Multimodal Perception","description":"Multimodal perception refers to the integration of multiple sensory modalities (vision, language, and action) to create a comprehensive understanding of the environment and enable intelligent robot behavior. This section covers the integration of visual, linguistic, and motor information for humanoid robotics.","source":"@site/docs/module-4-vla/multimodal-perception.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal-perception","permalink":"/module-4-vla/multimodal-perception","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning for Robotics","permalink":"/module-4-vla/llm-planning"},"next":{"title":"VLA Architecture Diagrams","permalink":"/module-4-vla/vla-architecture-diagrams"}},{"id":"module-4-vla/summary","title":"Summary: Physical AI & Humanoid Robotics Book","description":"Project Completion Report","source":"@site/docs/module-4-vla/summary.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/summary","permalink":"/module-4-vla/summary","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 VLA","permalink":"/module-4-vla/"},"next":{"title":"Humanoid Locomotion and Control","permalink":"/module-4-vla/humanoid-locomotion-control"}},{"id":"module-4-vla/vla-architecture-diagrams","title":"VLA Architecture Diagrams","description":"This section provides detailed diagrams showing the Vision-Language-Action (VLA) architecture for humanoid robotics systems using Isaac Sim and Isaac ROS.","source":"@site/docs/module-4-vla/vla-architecture-diagrams.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-architecture-diagrams","permalink":"/module-4-vla/vla-architecture-diagrams","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Multimodal Perception","permalink":"/module-4-vla/multimodal-perception"},"next":{"title":"VSLAM and Navigation","permalink":"/module-4-vla/vslam-navigation"}},{"id":"module-4-vla/vslam-navigation","title":"VSLAM and Navigation","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a critical technology for autonomous robots, enabling them to understand their environment and navigate without prior knowledge. This section covers VSLAM concepts and navigation techniques for humanoid robots using Isaac Sim and Isaac ROS.","source":"@site/docs/module-4-vla/vslam-navigation.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vslam-navigation","permalink":"/module-4-vla/vslam-navigation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VLA Architecture Diagrams","permalink":"/module-4-vla/vla-architecture-diagrams"},"next":{"title":"Whisper Speech Processing","permalink":"/module-4-vla/whisper-speech"}},{"id":"module-4-vla/whisper-speech","title":"Whisper Speech Processing","description":"OpenAI's Whisper model is a state-of-the-art automatic speech recognition (ASR) system that can transcribe speech to text with remarkable accuracy. In this section, we'll explore how to integrate Whisper into your humanoid robot's communication system.","source":"@site/docs/module-4-vla/whisper-speech.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper-speech","permalink":"/module-4-vla/whisper-speech","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VSLAM and Navigation","permalink":"/module-4-vla/vslam-navigation"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"Module 1 – ROS2","items":[{"type":"doc","id":"module-1-ros2/module-1-index"},{"type":"doc","id":"module-1-ros2/installation-setup"},{"type":"doc","id":"module-1-ros2/nodes-topics-services"},{"type":"doc","id":"module-1-ros2/practical-exercises"},{"type":"doc","id":"module-1-ros2/rclpy-basics"},{"type":"doc","id":"module-1-ros2/urdf-description"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2 – Digital Twin","items":[{"type":"doc","id":"module-2-digital-twin/module-2-index"},{"type":"doc","id":"module-2-digital-twin/gazebo-simulation"},{"type":"doc","id":"module-2-digital-twin/gazebo-world-setup"},{"type":"doc","id":"module-2-digital-twin/physics-collisions"},{"type":"doc","id":"module-2-digital-twin/practical-exercises"},{"type":"doc","id":"module-2-digital-twin/sensor-simulation"},{"type":"doc","id":"module-2-digital-twin/unity-scene-setup"},{"type":"doc","id":"module-2-digital-twin/unity-visualization"},{"type":"doc","id":"module-2-digital-twin/urdf-validation"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3 – AI Perception","items":[{"type":"doc","id":"module-3-ai-perception/module-3-index"},{"type":"doc","id":"module-3-ai-perception/installation-setup"},{"type":"doc","id":"module-3-ai-perception/isaac-sim"},{"type":"doc","id":"module-3-ai-perception/isaac-sim-fundamentals"},{"type":"doc","id":"module-3-ai-perception/object-detection-localization"},{"type":"doc","id":"module-3-ai-perception/navigation-planning-obstacle-avoidance"},{"type":"doc","id":"module-3-ai-perception/nav2-locomotion"},{"type":"doc","id":"module-3-ai-perception/perception-navigation-pipeline-diagrams"},{"type":"doc","id":"module-3-ai-perception/practical-exercises-isaac-ai"},{"type":"doc","id":"module-3-ai-perception/vslam-navigation"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4 – VLA","items":[{"type":"doc","id":"module-4-vla/module-4-index"},{"type":"doc","id":"module-4-vla/summary"},{"type":"doc","id":"module-4-vla/humanoid-locomotion-control"},{"type":"doc","id":"module-4-vla/isaac-ros-integration"},{"type":"doc","id":"module-4-vla/isaac-sim-fundamentals"},{"type":"doc","id":"module-4-vla/llm-planning"},{"type":"doc","id":"module-4-vla/multimodal-perception"},{"type":"doc","id":"module-4-vla/vla-architecture-diagrams"},{"type":"doc","id":"module-4-vla/vslam-navigation"},{"type":"doc","id":"module-4-vla/whisper-speech"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-pages":{"default":null},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}