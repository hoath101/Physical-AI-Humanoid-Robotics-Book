from typing import List, Dict, Any
import logging
import asyncio
from openai import AsyncOpenAI

from api.config.settings import settings

# Set up logging
logger = logging.getLogger(__name__)

class EmbeddingService:
    """
    Service for generating embeddings using OpenAI's API.
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.model = "text-embedding-ada-002"  # OpenAI's recommended embedding model
        self.batch_size = 20  # OpenAI's API allows up to 2048 texts per request, but we'll use a smaller batch for safety

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate a single embedding for the given text.
        """
        try:
            response = await self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Error generating embedding: {str(e)}")
            raise

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts.
        """
        if not texts:
            return []

        all_embeddings = []

        # Process in batches to respect API limits
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]

            try:
                response = await self.client.embeddings.create(
                    input=batch,
                    model=self.model
                )

                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)

                logger.info(f"Generated embeddings for batch {i//self.batch_size + 1}/{(len(texts)-1)//self.batch_size + 1}")

            except Exception as e:
                logger.error(f"Error generating embeddings for batch: {str(e)}")
                raise

        return all_embeddings

    async def get_embedding_dimensions(self) -> int:
        """
        Get the dimensionality of the embeddings generated by this service.
        """
        # Generate a test embedding to determine dimensions
        test_embedding = await self.generate_embedding("test")
        return len(test_embedding)

    async def calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """
        Calculate cosine similarity between two embeddings.
        """
        import math

        # Calculate dot product
        dot_product = sum(a * b for a, b in zip(embedding1, embedding2))

        # Calculate magnitudes
        magnitude1 = math.sqrt(sum(a * a for a in embedding1))
        magnitude2 = math.sqrt(sum(b * b for b in embedding2))

        # Calculate cosine similarity
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        similarity = dot_product / (magnitude1 * magnitude2)
        return similarity

    async def find_most_similar_texts(self, query_embedding: List[float],
                                    candidate_embeddings: List[List[float]],
                                    texts: List[str],
                                    top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Find the most similar texts to the query embedding.
        """
        similarities = []

        for i, candidate_embedding in enumerate(candidate_embeddings):
            similarity = await self.calculate_similarity(query_embedding, candidate_embedding)
            similarities.append({
                'text': texts[i],
                'similarity': similarity,
                'index': i
            })

        # Sort by similarity in descending order
        similarities.sort(key=lambda x: x['similarity'], reverse=True)

        # Return top_k results
        return similarities[:top_k]

# Create a singleton instance
embedding_service = EmbeddingService()