"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[558],{2858:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3-ai-perception/practical-exercises-isaac-ai","title":"Practical Exercises with Isaac AI Components","description":"This section provides hands-on exercises to reinforce the concepts learned about Isaac AI components for perception and navigation. These exercises will help you gain practical experience with NVIDIA Isaac technologies.","source":"@site/docs/module-3-ai-perception/practical-exercises-isaac-ai.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/practical-exercises-isaac-ai","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/practical-exercises-isaac-ai","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Perception and Navigation Pipeline Diagrams","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/perception-navigation-pipeline-diagrams"},"next":{"title":"VSLAM and Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/vslam-navigation"}}');var i=t(4848),a=t(8453);const o={},r="Practical Exercises with Isaac AI Components",c={},l=[{value:"Exercise 1: Isaac ROS DetectNet Integration",id:"exercise-1-isaac-ros-detectnet-integration",level:2},{value:"Objective",id:"objective",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Steps",id:"steps",level:3},{value:"1. Set up the environment",id:"1-set-up-the-environment",level:4},{value:"2. Create a custom DetectNet configuration",id:"2-create-a-custom-detectnet-configuration",level:4},{value:"3. Launch the DetectNet pipeline",id:"3-launch-the-detectnet-pipeline",level:4},{value:"4. Create a perception processing node",id:"4-create-a-perception-processing-node",level:4},{value:"5. Build and run the exercise",id:"5-build-and-run-the-exercise",level:4},{value:"Expected Outcome",id:"expected-outcome",level:3},{value:"Exercise 2: Isaac ROS Visual SLAM Integration",id:"exercise-2-isaac-ros-visual-slam-integration",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Steps",id:"steps-1",level:3},{value:"1. Create SLAM launch file",id:"1-create-slam-launch-file",level:4},{value:"2. Create a SLAM evaluation node",id:"2-create-a-slam-evaluation-node",level:4},{value:"3. Run the SLAM exercise",id:"3-run-the-slam-exercise",level:4},{value:"Expected Outcome",id:"expected-outcome-1",level:3},{value:"Exercise 3: Isaac ROS Bi3D Integration",id:"exercise-3-isaac-ros-bi3d-integration",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Steps",id:"steps-2",level:3},{value:"1. Create Bi3D launch configuration",id:"1-create-bi3d-launch-configuration",level:4},{value:"2. Create a 3D object extraction node",id:"2-create-a-3d-object-extraction-node",level:4},{value:"Expected Outcome",id:"expected-outcome-2",level:3},{value:"Exercise 4: Isaac Sim Perception Pipeline",id:"exercise-4-isaac-sim-perception-pipeline",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Steps",id:"steps-3",level:3},{value:"1. Create Isaac Sim perception scene",id:"1-create-isaac-sim-perception-scene",level:4},{value:"2. Connect Isaac Sim to ROS",id:"2-connect-isaac-sim-to-ros",level:4},{value:"Expected Outcome",id:"expected-outcome-3",level:3},{value:"Exercise 5: Integrated Perception and Navigation",id:"exercise-5-integrated-perception-and-navigation",level:2},{value:"Objective",id:"objective-4",level:3},{value:"Steps",id:"steps-4",level:3},{value:"1. Create integrated launch file",id:"1-create-integrated-launch-file",level:4},{value:"2. Create object avoidance controller",id:"2-create-object-avoidance-controller",level:4},{value:"3. Create a mission planner node",id:"3-create-a-mission-planner-node",level:4},{value:"Expected Outcome",id:"expected-outcome-4",level:3},{value:"Exercise 6: Performance Evaluation and Optimization",id:"exercise-6-performance-evaluation-and-optimization",level:2},{value:"Objective",id:"objective-5",level:3},{value:"Steps",id:"steps-5",level:3},{value:"1. Create performance monitoring node",id:"1-create-performance-monitoring-node",level:4},{value:"2. Create optimization report",id:"2-create-optimization-report",level:4},{value:"Expected Outcome",id:"expected-outcome-5",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. TensorRT Optimization Issues",id:"1-tensorrt-optimization-issues",level:3},{value:"2. Memory Issues",id:"2-memory-issues",level:3},{value:"3. Synchronization Issues",id:"3-synchronization-issues",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Modular Design",id:"1-modular-design",level:3},{value:"2. Performance Optimization",id:"2-performance-optimization",level:3},{value:"3. Robustness",id:"3-robustness",level:3},{value:"Exercise Completion Checklist",id:"exercise-completion-checklist",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"practical-exercises-with-isaac-ai-components",children:"Practical Exercises with Isaac AI Components"})}),"\n",(0,i.jsx)(n.p,{children:"This section provides hands-on exercises to reinforce the concepts learned about Isaac AI components for perception and navigation. These exercises will help you gain practical experience with NVIDIA Isaac technologies."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-1-isaac-ros-detectnet-integration",children:"Exercise 1: Isaac ROS DetectNet Integration"}),"\n",(0,i.jsx)(n.h3,{id:"objective",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Integrate Isaac ROS DetectNet with a humanoid robot to detect and classify objects in real-time."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Isaac Sim installed and running"}),"\n",(0,i.jsx)(n.li,{children:"Isaac ROS packages installed"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble"}),"\n",(0,i.jsx)(n.li,{children:"Compatible NVIDIA GPU with TensorRT"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-set-up-the-environment",children:"1. Set up the environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Source ROS 2 and Isaac ROS\nsource /opt/ros/humble/setup.bash\nsource ~/isaac_ros_ws/install/setup.bash\n\n# Launch Isaac Sim with a simple scene\n# (We'll use a script to automate this in the exercise)\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-create-a-custom-detectnet-configuration",children:"2. Create a custom DetectNet configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# config/detectnet_config.yaml\n---\nlog_level: info\nmodel_name: "ssd_mobilenet_v2_coco"\ninput_topic: "/camera/image_rect_color"\noutput_topic: "/detectnet/detections"\nconfidence_threshold: 0.7\nmax_objects: 10\n'})}),"\n",(0,i.jsx)(n.h4,{id:"3-launch-the-detectnet-pipeline",children:"3. Launch the DetectNet pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create launch file: launch/detectnet_humanoid.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config = os.path.join(\n        get_package_share_directory('your_robot_perception'),\n        'config',\n        'detectnet_config.yaml'\n    )\n\n    detectnet_node = Node(\n        package='isaac_ros_detectnet',\n        executable='isaac_ros_detectnet',\n        name='detectnet',\n        parameters=[config],\n        remappings=[\n            ('/image_input', '/camera/image_rect_color'),\n            ('/detectnet/detections', '/detections')\n        ]\n    )\n\n    return LaunchDescription([\n        detectnet_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"4-create-a-perception-processing-node",children:"4. Create a perception processing node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/object_tracker_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectTrackerNode : public rclcpp::Node\n{\npublic:\n    ObjectTrackerNode() : Node("object_tracker_node"), tf_buffer_(this->get_clock())\n    {\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            "detections", 10,\n            std::bind(&ObjectTrackerNode::detectionCallback, this, std::placeholders::_1)\n        );\n\n        object_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            "tracked_object_position", 10\n        );\n\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(tf_buffer_);\n    }\n\nprivate:\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr msg)\n    {\n        if (msg->detections.empty()) {\n            return;\n        }\n\n        // Process the highest confidence detection\n        auto best_detection = std::max_element(\n            msg->detections.begin(),\n            msg->detections.end(),\n            [](const auto& a, const auto& b) {\n                return a.confidence < b.confidence;\n            }\n        );\n\n        if (best_detection->confidence > 0.7) {\n            // Calculate 3D position from 2D detection\n            geometry_msgs::msg::PointStamped object_pos;\n            object_pos.header = msg->header;\n\n            // Convert 2D bounding box center to 3D position\n            // This requires depth information which we\'ll simulate\n            object_pos.point.x = (best_detection->bbox.center.x - 320.0) * 0.001; // Simplified\n            object_pos.point.y = (best_detection->bbox.center.y - 240.0) * 0.001; // Simplified\n            object_pos.point.z = 1.0; // Fixed depth for simulation\n\n            object_pub_->publish(object_pos);\n\n            RCLCPP_INFO(\n                this->get_logger(),\n                "Detected %s with confidence %.2f at (%.2f, %.2f, %.2f)",\n                best_detection->label.c_str(),\n                best_detection->confidence,\n                object_pos.point.x,\n                object_pos.point.y,\n                object_pos.point.z\n            );\n        }\n    }\n\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pub_;\n\n    tf2_ros::Buffer tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ObjectTrackerNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"5-build-and-run-the-exercise",children:"5. Build and run the exercise"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build the package\ncd ~/isaac_ros_ws\nsource install/setup.bash\ncolcon build --packages-select your_robot_perception\n\n# Run the nodes\nros2 launch your_robot_perception detectnet_humanoid.launch.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A running perception pipeline that detects objects and publishes their 3D positions."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-2-isaac-ros-visual-slam-integration",children:"Exercise 2: Isaac ROS Visual SLAM Integration"}),"\n",(0,i.jsx)(n.h3,{id:"objective-1",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Set up and run Isaac ROS Visual SLAM to create a map of the environment and localize the robot."}),"\n",(0,i.jsx)(n.h3,{id:"steps-1",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-slam-launch-file",children:"1. Create SLAM launch file"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/visual_slam.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='isaac_ros_visual_slam',\n        parameters=[{\n            'enable_occupancy_grid': True,\n            'enable_diagnostics': False,\n            'occupancy_grid_resolution': 0.05,\n            'frame_id': 'oak-d_frame',\n            'base_frame': 'base_link',\n            'odom_frame': 'odom',\n            'enable_slam_visualization': True,\n            'enable_landmarks_view': True,\n            'enable_observations_view': True,\n            'calibration_file': '/tmp/calibration.json',\n            'rescale_threshold': 2.0\n        }],\n        remappings=[\n            ('/stereo_camera/left/image', '/camera/left/image_rect_color'),\n            ('/stereo_camera/right/image', '/camera/right/image_rect_color'),\n            ('/stereo_camera/left/camera_info', '/camera/left/camera_info'),\n            ('/stereo_camera/right/camera_info', '/camera/right/camera_info'),\n            ('/visual_slam/imu', '/imu/data'),\n        ]\n    )\n\n    return LaunchDescription([\n        visual_slam_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-create-a-slam-evaluation-node",children:"2. Create a SLAM evaluation node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/slam_evaluator_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass SlamevaluatorNode : public rclcpp::Node\n{\npublic:\n    SlamevaluatorNode() : Node("slam_evaluator_node")\n    {\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            "visual_slam/odometry", 10,\n            std::bind(&SlamevaluatorNode::odometryCallback, this, std::placeholders::_1)\n        );\n\n        pose_pub_ = this->create_publisher<geometry_msgs::msg::PoseStamped>(\n            "estimated_pose", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), "SLAM Evaluator Node initialized");\n    }\n\nprivate:\n    void odometryCallback(const nav_msgs::msg::Odometry::SharedPtr msg)\n    {\n        // Extract position and orientation from odometry\n        geometry_msgs::msg::PoseStamped pose_msg;\n        pose_msg.header = msg->header;\n        pose_msg.pose = msg->pose.pose;\n\n        // Publish the estimated pose\n        pose_pub_->publish(pose_msg);\n\n        // Calculate and log trajectory metrics\n        if (has_previous_pose_) {\n            double distance = calculateDistance(previous_pose_, pose_msg.pose);\n            total_distance_ += distance;\n\n            RCLCPP_INFO(\n                this->get_logger(),\n                "SLAM Position: (%.2f, %.2f, %.2f), Total distance: %.2f",\n                pose_msg.pose.position.x,\n                pose_msg.pose.position.y,\n                pose_msg.pose.position.z,\n                total_distance_\n            );\n        }\n\n        previous_pose_ = pose_msg.pose;\n        has_previous_pose_ = true;\n    }\n\n    double calculateDistance(const geometry_msgs::msg::Pose& p1, const geometry_msgs::msg::Pose& p2)\n    {\n        double dx = p1.position.x - p2.position.x;\n        double dy = p1.position.y - p2.position.y;\n        double dz = p1.position.z - p2.position.z;\n        return sqrt(dx*dx + dy*dy + dz*dz);\n    }\n\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_pub_;\n\n    geometry_msgs::msg::Pose previous_pose_;\n    bool has_previous_pose_ = false;\n    double total_distance_ = 0.0;\n};\n'})}),"\n",(0,i.jsx)(n.h4,{id:"3-run-the-slam-exercise",children:"3. Run the SLAM exercise"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Launch SLAM\nros2 launch your_robot_perception visual_slam.launch.py\n\n# Visualize results\nros2 run rviz2 rviz2 -d /path/to/slam_config.rviz\n"})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome-1",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A real-time map of the environment with the robot's estimated position and trajectory."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-3-isaac-ros-bi3d-integration",children:"Exercise 3: Isaac ROS Bi3D Integration"}),"\n",(0,i.jsx)(n.h3,{id:"objective-2",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Use Isaac ROS Bi3D for 3D object detection and segmentation."}),"\n",(0,i.jsx)(n.h3,{id:"steps-2",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-bi3d-launch-configuration",children:"1. Create Bi3D launch configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/bi3d_segmentation.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    bi3d_node = Node(\n        package='isaac_ros_bi3d',\n        executable='isaac_ros_bi3d',\n        parameters=[{\n            'engine_file_path': '/path/to/bi3d_plan_engine.plan',\n            'input_tensor_names': ['input_tensor'],\n            'output_tensor_names': ['output_tensor'],\n            'network_input_height': 512,\n            'network_input_width': 512,\n            'num_classes': 256,\n            'mask_threshold': 0.8\n        }],\n        remappings=[\n            ('/image', '/camera/image_rect_color'),\n            ('/segmentation', '/bi3d_segmentation')\n        ]\n    )\n\n    return LaunchDescription([\n        bi3d_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-create-a-3d-object-extraction-node",children:"2. Create a 3D object extraction node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/bi3d_processor_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass Bi3DProcessorNode : public rclcpp::Node\n{\npublic:\n    Bi3DProcessorNode() : Node("bi3d_processor_node")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image_rect_color", 10,\n            std::bind(&Bi3DProcessorNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        segmentation_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "bi3d_segmentation", 10,\n            std::bind(&Bi3DProcessorNode::segmentationCallback, this, std::placeholders::_1)\n        );\n\n        object_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            "extracted_3d_objects", 10\n        );\n\n        RCLCPP_INFO(this->get_logger(), "Bi3D Processor Node initialized");\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Store image for processing with segmentation\n        latest_image_ = image_msg;\n    }\n\n    void segmentationCallback(const sensor_msgs::msg::Image::SharedPtr seg_msg)\n    {\n        if (!latest_image_) {\n            return;  // Wait for image\n        }\n\n        try {\n            // Convert segmentation image to OpenCV\n            cv_bridge::CvImagePtr seg_cv_ptr = cv_bridge::toCvCopy(seg_msg, sensor_msgs::image_encodings::TYPE_32SC1);\n\n            // Process segmentation to extract 3D objects\n            std::vector<DetectedObject> objects = extractObjects(seg_cv_ptr->image);\n\n            // Publish each detected object\n            for (const auto& obj : objects) {\n                geometry_msgs::msg::PointStamped obj_pos;\n                obj_pos.header = seg_msg->header;\n                obj_pos.point = obj.centroid;\n\n                object_pub_->publish(obj_pos);\n\n                RCLCPP_INFO(\n                    this->get_logger(),\n                    "3D Object: %s at (%.2f, %.2f, %.2f), pixels: %d",\n                    obj.label.c_str(),\n                    obj.centroid.x,\n                    obj.centroid.y,\n                    obj.centroid.z,\n                    obj.pixel_count\n                );\n            }\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n        }\n    }\n\n    struct DetectedObject {\n        std::string label;\n        geometry_msgs::msg::Point centroid;\n        int pixel_count;\n        cv::Rect bounding_box;\n    };\n\n    std::vector<DetectedObject> extractObjects(const cv::Mat& segmentation_mask)\n    {\n        std::vector<DetectedObject> objects;\n\n        // Find contours for each class in the segmentation\n        for (int class_id = 1; class_id < 256; ++class_id) {  // Skip background (0)\n            cv::Mat class_mask = (segmentation_mask == class_id);\n\n            std::vector<std::vector<cv::Point>> contours;\n            cv::findContours(class_mask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);\n\n            for (const auto& contour : contours) {\n                if (contour.size() < 50) continue;  // Filter small regions\n\n                DetectedObject obj;\n                obj.pixel_count = contour.size();\n\n                // Calculate centroid\n                cv::Moments moments = cv::moments(contour);\n                if (moments.m00 != 0) {\n                    int cx = static_cast<int>(moments.m10 / moments.m00);\n                    int cy = static_cast<int>(moments.m01 / moments.m00);\n\n                    // Estimate 3D position (simplified)\n                    obj.centroid.x = (cx - 320.0) * 0.002;  // Approximate conversion\n                    obj.centroid.y = (cy - 240.0) * 0.002;\n                    obj.centroid.z = 1.0;  // Estimated depth\n\n                    obj.bounding_box = cv::boundingRect(contour);\n\n                    // Assign label based on class ID (in real system, use a mapping)\n                    obj.label = "object_" + std::to_string(class_id);\n\n                    objects.push_back(obj);\n                }\n            }\n        }\n\n        return objects;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr segmentation_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pub_;\n\n    sensor_msgs::msg::Image::SharedPtr latest_image_;\n};\n'})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome-2",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A system that segments the scene into 3D objects and publishes their positions."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-4-isaac-sim-perception-pipeline",children:"Exercise 4: Isaac Sim Perception Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"objective-3",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Create a complete perception pipeline in Isaac Sim that integrates multiple Isaac ROS components."}),"\n",(0,i.jsx)(n.h3,{id:"steps-3",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-isaac-sim-perception-scene",children:"1. Create Isaac Sim perception scene"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# scripts/setup_perception_scene.py\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass PerceptionSceneSetup:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_scene()\n\n    def setup_scene(self):\n        # Get assets root path\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add a robot with perception sensors\n        robot_path = assets_root_path + "/Isaac/Robots/Carter/carter_navigate.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Carter")\n\n        # Add a camera sensor\n        self.camera = Camera(\n            prim_path="/World/Carter/chassis/camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add a LIDAR sensor\n        self.lidar = RotatingLidarPhysX(\n            prim_path="/World/Carter/chassis/lidar",\n            translation=np.array([0.0, 0.0, 0.25]),\n            config="Carter",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add some objects to detect\n        cube_path = assets_root_path + "/Isaac/Props/Blocks/block_instanceable.usd"\n        add_reference_to_stage(usd_path=cube_path, prim_path="/World/Cube1")\n        from pxr import Gf\n        from omni.isaac.core.utils.prims import set_targets\n        from omni.isaac.core.utils.transformations import quat_from_euler_angles\n\n        # Position the cube in front of the robot\n        cube_prim = self.world.scene.add_static_object(\n            prim_path="/World/Cube1",\n            usd_path=cube_path,\n            position=[2.0, 0.0, 0.5],\n            orientation=quat_from_euler_angles([0, 0, 0])\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_perception_pipeline(self):\n        """Run the perception pipeline"""\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get sensor data\n            camera_data = self.camera.get_rgb()\n            lidar_data = self.lidar.get_linear_depth_data()\n\n            # Process perception data (placeholder)\n            self.process_perception_data(camera_data, lidar_data)\n\n    def process_perception_data(self, camera_data, lidar_data):\n        """Process perception data"""\n        print(f"Camera data shape: {camera_data.shape if hasattr(camera_data, \'shape\') else \'N/A\'}")\n        print(f"LIDAR data points: {len(lidar_data) if hasattr(lidar_data, \'__len__\') else \'N/A\'}")\n\n# Run the scene setup\nif __name__ == "__main__":\n    scene_setup = PerceptionSceneSetup()\n    scene_setup.run_perception_pipeline()\n'})}),"\n",(0,i.jsx)(n.h4,{id:"2-connect-isaac-sim-to-ros",children:"2. Connect Isaac Sim to ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# scripts/isaac_sim_ros_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacSimRosBridge(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sim_ros_bridge\')\n\n        # ROS publishers\n        self.image_pub = self.create_publisher(Image, \'/camera/image_raw\', 10)\n        self.scan_pub = self.create_publisher(LaserScan, \'/scan\', 10)\n\n        # ROS subscriber for robot commands\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.cmd_vel_callback, 10\n        )\n\n        self.bridge = CvBridge()\n\n        # Timer to publish sensor data\n        self.timer = self.create_timer(0.1, self.publish_sensor_data)\n\n        # Store robot commands\n        self.last_cmd_vel = None\n\n    def cmd_vel_callback(self, msg):\n        """Store command velocity for Isaac Sim"""\n        self.last_cmd_vel = msg\n        # In a real implementation, this would send commands to Isaac Sim\n\n    def publish_sensor_data(self):\n        """Publish sensor data from Isaac Sim"""\n        # This would normally connect to Isaac Sim\n        # For this exercise, we\'ll simulate data\n\n        # Publish a simulated image\n        sim_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        image_msg = self.bridge.cv2_to_imgmsg(sim_image, encoding="bgr8")\n        image_msg.header.stamp = self.get_clock().now().to_msg()\n        image_msg.header.frame_id = "camera_link"\n        self.image_pub.publish(image_msg)\n\n        # Publish a simulated scan\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = "lidar_link"\n        scan_msg.angle_min = -np.pi / 2\n        scan_msg.angle_max = np.pi / 2\n        scan_msg.angle_increment = np.pi / 180  # 1 degree\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 10.0\n        scan_msg.ranges = [5.0] * 180  # Simulated ranges\n\n        self.scan_pub.publish(scan_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = IsaacSimRosBridge()\n\n    try:\n        rclpy.spin(bridge)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome-3",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A complete perception pipeline running in Isaac Sim that connects to ROS and processes data from multiple sensors."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-5-integrated-perception-and-navigation",children:"Exercise 5: Integrated Perception and Navigation"}),"\n",(0,i.jsx)(n.h3,{id:"objective-4",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Combine perception and navigation systems to create a complete autonomous robot behavior."}),"\n",(0,i.jsx)(n.h3,{id:"steps-4",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-integrated-launch-file",children:"1. Create integrated launch file"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/integrated_perception_navigation.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Include perception pipeline\n    perception_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                get_package_share_directory('your_robot_perception'),\n                'launch',\n                'detectnet_humanoid.launch.py'\n            ])\n        ])\n    )\n\n    # Include navigation stack\n    navigation_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                get_package_share_directory('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': 'true'\n        }.items()\n    )\n\n    # Object avoidance node\n    object_avoidance_node = Node(\n        package='your_robot_perception',\n        executable='object_avoidance_node',\n        name='object_avoidance',\n        parameters=[\n            {'safety_distance': 0.5},\n            {'avoidance_strength': 1.0}\n        ],\n        remappings=[\n            ('/detected_objects', '/tracked_object_position'),\n            ('/cmd_vel_safe', '/cmd_vel_filtered'),\n            ('/cmd_vel_in', '/cmd_vel')\n        ]\n    )\n\n    return LaunchDescription([\n        perception_launch,\n        navigation_launch,\n        object_avoidance_node\n    ])\n"})}),"\n",(0,i.jsx)(n.h4,{id:"2-create-object-avoidance-controller",children:"2. Create object avoidance controller"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/object_avoidance_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectAvoidanceNode : public rclcpp::Node\n{\npublic:\n    ObjectAvoidanceNode() : Node("object_avoidance_node")\n    {\n        cmd_vel_in_sub_ = this->create_subscription<geometry_msgs::msg::Twist>(\n            "cmd_vel_in", 10,\n            std::bind(&ObjectAvoidanceNode::cmdVelCallback, this, std::placeholders::_1)\n        );\n\n        detected_objects_sub_ = this->create_subscription<geometry_msgs::msg::PointStamped>(\n            "detected_objects", 10,\n            std::bind(&ObjectAvoidanceNode::objectCallback, this, std::placeholders::_1)\n        );\n\n        cmd_vel_out_pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\n            "cmd_vel_safe", 10\n        );\n\n        // Get parameters\n        safety_distance_ = this->declare_parameter("safety_distance", 0.5);\n        avoidance_strength_ = this->declare_parameter("avoidance_strength", 1.0);\n\n        RCLCPP_INFO(this->get_logger(), "Object Avoidance Node initialized");\n    }\n\nprivate:\n    void cmdVelCallback(const geometry_msgs::msg::Twist::SharedPtr cmd_msg)\n    {\n        latest_cmd_vel_ = *cmd_msg;\n        applyObjectAvoidance();\n    }\n\n    void objectCallback(const geometry_msgs::msg::PointStamped::SharedPtr obj_msg)\n    {\n        // Store object position for avoidance calculation\n        detected_objects_.push_back(*obj_msg);\n\n        // Keep only recent detections (last 1 second)\n        auto current_time = this->now();\n        detected_objects_.erase(\n            std::remove_if(detected_objects_.begin(), detected_objects_.end(),\n                [current_time, this](const geometry_msgs::msg::PointStamped& obj) {\n                    return (current_time - obj.header.stamp).seconds() > 1.0;\n                }),\n            detected_objects_.end()\n        );\n    }\n\n    void applyObjectAvoidance()\n    {\n        if (!latest_cmd_vel_) {\n            return;\n        }\n\n        geometry_msgs::msg::Twist safe_cmd = *latest_cmd_vel_;\n\n        // Check for nearby objects that require avoidance\n        for (const auto& obj : detected_objects_) {\n            // Calculate distance to object in robot\'s frame\n            double dist_to_obj = sqrt(\n                pow(obj.point.x, 2) +\n                pow(obj.point.y, 2) +\n                pow(obj.point.z, 2)\n            );\n\n            if (dist_to_obj < safety_distance_) {\n                // Calculate avoidance vector\n                double avoidance_x = -obj.point.x * avoidance_strength_ / dist_to_obj;\n                double avoidance_y = -obj.point.y * avoidance_strength_ / dist_to_obj;\n\n                // Apply avoidance to commanded velocity\n                safe_cmd.linear.x += avoidance_x;\n                safe_cmd.linear.y += avoidance_y;\n\n                // Add angular component to turn away from object\n                double angle_to_obj = atan2(obj.point.y, obj.point.x);\n                safe_cmd.angular.z -= angle_to_obj * avoidance_strength_ * 0.5;\n            }\n        }\n\n        // Apply velocity limits\n        safe_cmd.linear.x = std::clamp(safe_cmd.linear.x, -0.5, 0.5);\n        safe_cmd.linear.y = std::clamp(safe_cmd.linear.y, -0.2, 0.2);\n        safe_cmd.angular.z = std::clamp(safe_cmd.angular.z, -0.5, 0.5);\n\n        cmd_vel_out_pub_->publish(safe_cmd);\n    }\n\n    rclcpp::Subscription<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_in_sub_;\n    rclcpp::Subscription<geometry_msgs::msg::PointStamped>::SharedPtr detected_objects_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_out_pub_;\n\n    std::optional<geometry_msgs::msg::Twist> latest_cmd_vel_;\n    std::vector<geometry_msgs::msg::PointStamped> detected_objects_;\n\n    double safety_distance_;\n    double avoidance_strength_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ObjectAvoidanceNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h4,{id:"3-create-a-mission-planner-node",children:"3. Create a mission planner node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/mission_planner_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav2_msgs/action/navigate_to_pose.hpp>\n#include <rclcpp_action/rclcpp_action.hpp>\n\nclass MissionPlannerNode : public rclcpp::Node\n{\npublic:\n    MissionPlannerNode() : Node("mission_planner_node")\n    {\n        nav_client_ = rclcpp_action::create_client<nav2_msgs::action::NavigateToPose>(\n            this, "navigate_to_pose"\n        );\n\n        // Define a simple mission: visit multiple waypoints\n        waypoints_ = {\n            createPose(1.0, 1.0, 0.0),\n            createPose(2.0, 0.0, 1.57),\n            createPose(1.0, -1.0, 3.14),\n            createPose(0.0, 0.0, 0.0)  // Return to start\n        };\n\n        mission_timer_ = this->create_wall_timer(\n            std::chrono::seconds(5),\n            std::bind(&MissionPlannerNode::executeNextWaypoint, this)\n        );\n\n        current_waypoint_ = 0;\n        mission_active_ = false;\n\n        RCLCPP_INFO(this->get_logger(), "Mission Planner Node initialized");\n    }\n\nprivate:\n    geometry_msgs::msg::PoseStamped createPose(double x, double y, double theta)\n    {\n        geometry_msgs::msg::PoseStamped pose;\n        pose.header.frame_id = "map";\n        pose.pose.position.x = x;\n        pose.pose.position.y = y;\n        pose.pose.position.z = 0.0;\n\n        tf2::Quaternion q;\n        q.setRPY(0, 0, theta);\n        pose.pose.orientation = tf2::toMsg(q);\n\n        return pose;\n    }\n\n    void executeNextWaypoint()\n    {\n        if (mission_active_ || current_waypoint_ >= waypoints_.size()) {\n            return;  // Wait for current navigation to complete\n        }\n\n        if (!nav_client_->wait_for_action_server(std::chrono::seconds(5))) {\n            RCLCPP_ERROR(this->get_logger(), "Navigation action server not available");\n            return;\n        }\n\n        auto goal = nav2_msgs::action::NavigateToPose::Goal();\n        goal.pose = waypoints_[current_waypoint_];\n\n        auto send_goal_options = rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SendGoalOptions();\n        send_goal_options.result_callback =\n            [this](const rclcpp_action::ClientGoalHandle<nav2_msgs::action::NavigateToPose>::WrappedResult& result) {\n                if (result.code == rclcpp_action::ResultCode::SUCCEEDED) {\n                    RCLCPP_INFO(this->get_logger(), "Waypoint %d reached!", current_waypoint_);\n                    current_waypoint_++;\n                    mission_active_ = false;\n\n                    if (current_waypoint_ >= waypoints_.size()) {\n                        RCLCPP_INFO(this->get_logger(), "Mission completed!");\n                    }\n                } else {\n                    RCLCPP_ERROR(this->get_logger(), "Failed to reach waypoint %d", current_waypoint_);\n                    mission_active_ = false;\n                }\n            };\n\n        mission_active_ = true;\n        nav_client_->async_send_goal(goal, send_goal_options);\n    }\n\n    rclcpp_action::Client<nav2_msgs::action::NavigateToPose>::SharedPtr nav_client_;\n    rclcpp::TimerBase::SharedPtr mission_timer_;\n\n    std::vector<geometry_msgs::msg::PoseStamped> waypoints_;\n    size_t current_waypoint_;\n    bool mission_active_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<MissionPlannerNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome-4",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A complete system that performs perception, navigation, and mission planning with object avoidance capabilities."}),"\n",(0,i.jsx)(n.h2,{id:"exercise-6-performance-evaluation-and-optimization",children:"Exercise 6: Performance Evaluation and Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"objective-5",children:"Objective"}),"\n",(0,i.jsx)(n.p,{children:"Evaluate and optimize the performance of your Isaac AI perception system."}),"\n",(0,i.jsx)(n.h3,{id:"steps-5",children:"Steps"}),"\n",(0,i.jsx)(n.h4,{id:"1-create-performance-monitoring-node",children:"1. Create performance monitoring node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// src/performance_monitor_node.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <std_msgs/msg/string.hpp>\n#include <chrono>\n\nclass PerformanceMonitorNode : public rclcpp::Node\n{\npublic:\n    PerformanceMonitorNode() : Node("performance_monitor_node")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image_raw", 10,\n            std::bind(&PerformanceMonitorNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        stats_timer_ = this->create_wall_timer(\n            std::chrono::seconds(1),\n            std::bind(&PerformanceMonitorNode::printStats, this)\n        );\n\n        RCLCPP_INFO(this->get_logger(), "Performance Monitor Node initialized");\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        auto current_time = std::chrono::steady_clock::now();\n\n        // Calculate frame rate\n        if (last_frame_time_.has_value()) {\n            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(\n                current_time - last_frame_time_.value()\n            ).count();\n            frame_times_.push_back(duration);\n        }\n\n        last_frame_time_ = current_time;\n\n        // Calculate bandwidth\n        size_t msg_size = msg->data.size();\n        total_bytes_processed_ += msg_size;\n        messages_processed_++;\n    }\n\n    void printStats()\n    {\n        if (frame_times_.empty()) return;\n\n        // Calculate average frame time\n        double avg_frame_time = 0.0;\n        for (auto time : frame_times_) {\n            avg_frame_time += time;\n        }\n        avg_frame_time /= frame_times_.size();\n\n        // Calculate frame rate\n        double avg_fps = 1e6 / avg_frame_time;  // microseconds to seconds\n\n        // Calculate bandwidth\n        double bandwidth = total_bytes_processed_ / 1e6;  // MB\n        double avg_msg_size = (messages_processed_ > 0) ?\n            static_cast<double>(total_bytes_processed_) / messages_processed_ : 0;\n\n        RCLCPP_INFO(\n            this->get_logger(),\n            "Performance Stats - FPS: %.2f, Avg Frame Time: %.2f ms, "\n            "Avg Msg Size: %.2f KB, Total Processed: %.2f MB",\n            avg_fps, avg_frame_time / 1000.0, avg_msg_size / 1024.0, bandwidth\n        );\n\n        // Clear for next interval\n        frame_times_.clear();\n        total_bytes_processed_ = 0;\n        messages_processed_ = 0;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::TimerBase::SharedPtr stats_timer_;\n\n    std::optional<std::chrono::steady_clock::time_point> last_frame_time_;\n    std::vector<long> frame_times_;  // in microseconds\n    size_t total_bytes_processed_ = 0;\n    size_t messages_processed_ = 0;\n};\n'})}),"\n",(0,i.jsx)(n.h4,{id:"2-create-optimization-report",children:"2. Create optimization report"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// scripts/generate_performance_report.py\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nfrom datetime import datetime\n\nclass PerformanceAnalyzer:\n    def __init__(self):\n        self.metrics = []\n\n    def load_metrics(self, metrics_file):\n        """Load performance metrics from file"""\n        with open(metrics_file, \'r\') as f:\n            self.metrics = json.load(f)\n\n    def generate_report(self):\n        """Generate performance analysis report"""\n        if not self.metrics:\n            print("No metrics loaded")\n            return\n\n        df = pd.DataFrame(self.metrics)\n\n        # Create visualizations\n        self.create_visualizations(df)\n\n        # Generate recommendations\n        self.generate_recommendations(df)\n\n        # Export detailed report\n        self.export_report(df)\n\n    def create_visualizations(self, df):\n        """Create performance visualization plots"""\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Frame rate over time\n        axes[0, 0].plot(df[\'timestamp\'], df[\'fps\'])\n        axes[0, 0].set_title(\'Frame Rate Over Time\')\n        axes[0, 0].set_xlabel(\'Time\')\n        axes[0, 0].set_ylabel(\'FPS\')\n\n        # Processing time distribution\n        axes[0, 1].hist(df[\'avg_frame_time_ms\'], bins=30)\n        axes[0, 1].set_title(\'Frame Processing Time Distribution\')\n        axes[0, 1].set_xlabel(\'Processing Time (ms)\')\n        axes[0, 1].set_ylabel(\'Frequency\')\n\n        # Bandwidth usage\n        axes[1, 0].plot(df[\'timestamp\'], df[\'bandwidth_mb\'])\n        axes[1, 0].set_title(\'Bandwidth Usage Over Time\')\n        axes[1, 0].set_xlabel(\'Time\')\n        axes[1, 0].set_ylabel(\'Bandwidth (MB)\')\n\n        # Correlation heatmap\n        correlation_data = df[[\'fps\', \'avg_frame_time_ms\', \'bandwidth_mb\']].corr()\n        im = axes[1, 1].imshow(correlation_data, cmap=\'coolwarm\', aspect=\'auto\')\n        axes[1, 1].set_xticks(range(len(correlation_data.columns)))\n        axes[1, 1].set_yticks(range(len(correlation_data.columns)))\n        axes[1, 1].set_xticklabels(correlation_data.columns, rotation=45)\n        axes[1, 1].set_yticklabels(correlation_data.columns)\n        axes[1, 1].set_title(\'Performance Metric Correlations\')\n\n        # Add colorbar\n        plt.colorbar(im, ax=axes[1, 1])\n\n        plt.tight_layout()\n        plt.savefig(\'performance_analysis.png\')\n        plt.show()\n\n    def generate_recommendations(self, df):\n        """Generate optimization recommendations"""\n        avg_fps = df[\'fps\'].mean()\n        avg_time = df[\'avg_frame_time_ms\'].mean()\n        avg_bandwidth = df[\'bandwidth_mb\'].mean()\n\n        print("PERFORMANCE ANALYSIS REPORT")\n        print("=" * 50)\n        print(f"Average Frame Rate: {avg_fps:.2f} FPS")\n        print(f"Average Processing Time: {avg_time:.2f} ms")\n        print(f"Average Bandwidth: {avg_bandwidth:.2f} MB")\n        print()\n\n        recommendations = []\n\n        if avg_fps < 15:\n            recommendations.append("LOW FPS: Consider reducing image resolution or using lighter models")\n        if avg_time > 50:\n            recommendations.append("HIGH PROCESSING TIME: Optimize algorithms or use TensorRT")\n        if avg_bandwidth > 100:\n            recommendations.append("HIGH BANDWIDTH: Compress images or reduce frequency")\n\n        if not recommendations:\n            recommendations.append("Performance looks good! Consider profiling specific bottlenecks.")\n\n        print("OPTIMIZATION RECOMMENDATIONS:")\n        for rec in recommendations:\n            print(f"- {rec}")\n        print()\n\n    def export_report(self, df):\n        """Export detailed report"""\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        report_filename = f"performance_report_{timestamp}.txt"\n\n        with open(report_filename, \'w\') as f:\n            f.write("Isaac AI Performance Report\\n")\n            f.write(f"Generated: {datetime.now()}\\n\\n")\n            f.write(f"Summary Statistics:\\n{df.describe()}\\n\\n")\n\n        print(f"Detailed report exported to {report_filename}")\n\nif __name__ == "__main__":\n    analyzer = PerformanceAnalyzer()\n    # analyzer.load_metrics(\'performance_metrics.json\')  # Load your metrics\n    # analyzer.generate_report()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"expected-outcome-5",children:"Expected Outcome"}),"\n",(0,i.jsx)(n.p,{children:"A complete system for monitoring, analyzing, and optimizing Isaac AI component performance with actionable insights."}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"1-tensorrt-optimization-issues",children:"1. TensorRT Optimization Issues"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Models not running at expected speeds\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Verify TensorRT engine files are properly built"}),"\n",(0,i.jsx)(n.li,{children:"Check GPU memory availability"}),"\n",(0,i.jsx)(n.li,{children:"Use appropriate batch sizes"}),"\n",(0,i.jsx)(n.li,{children:"Profile with NSight Systems"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-memory-issues",children:"2. Memory Issues"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": GPU memory exhaustion\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Reduce model input resolution"}),"\n",(0,i.jsx)(n.li,{children:"Use model quantization"}),"\n",(0,i.jsx)(n.li,{children:"Implement memory pooling"}),"\n",(0,i.jsx)(n.li,{children:"Monitor memory usage during runtime"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-synchronization-issues",children:"3. Synchronization Issues"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Sensor data timing problems\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use message filters for time synchronization"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper buffer sizes"}),"\n",(0,i.jsx)(n.li,{children:"Use reliable QoS policies"}),"\n",(0,i.jsx)(n.li,{children:"Add timestamps to messages"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"1-modular-design",children:"1. Modular Design"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Separate perception, planning, and control components"}),"\n",(0,i.jsx)(n.li,{children:"Use standard ROS interfaces"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper error handling"}),"\n",(0,i.jsx)(n.li,{children:"Design for easy testing and debugging"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-performance-optimization",children:"2. Performance Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use TensorRT for inference acceleration"}),"\n",(0,i.jsx)(n.li,{children:"Implement efficient data pipelines"}),"\n",(0,i.jsx)(n.li,{children:"Use multi-threading where appropriate"}),"\n",(0,i.jsx)(n.li,{children:"Profile regularly and optimize bottlenecks"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-robustness",children:"3. Robustness"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Handle sensor failures gracefully"}),"\n",(0,i.jsx)(n.li,{children:"Implement fallback behaviors"}),"\n",(0,i.jsx)(n.li,{children:"Validate inputs and outputs"}),"\n",(0,i.jsx)(n.li,{children:"Test in diverse conditions"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercise-completion-checklist",children:"Exercise Completion Checklist"}),"\n",(0,i.jsx)(n.p,{children:"After completing these exercises, you should be able to:"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up Isaac ROS perception components"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate multiple Isaac AI modules"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Process and interpret perception data"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement perception-driven navigation"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Evaluate and optimize performance"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Troubleshoot common issues"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design robust perception systems"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Successfully completing these exercises will provide you with hands-on experience with NVIDIA Isaac AI components and prepare you for implementing perception and navigation systems on real humanoid robots."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);