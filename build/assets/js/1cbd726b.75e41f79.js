"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[78],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},9765:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3-ai-perception/object-detection-localization","title":"Object Detection and Localization Examples","description":"This section provides practical examples of object detection and localization using NVIDIA Isaac technologies, demonstrating how AI-powered perception systems work in robotics applications.","source":"@site/docs/module-3-ai-perception/object-detection-localization.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/object-detection-localization","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/object-detection-localization","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/isaac-sim-fundamentals"},"next":{"title":"Navigation Planning and Obstacle Avoidance Examples","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/navigation-planning-obstacle-avoidance"}}');var o=t(4848),s=t(8453);const a={},r="Object Detection and Localization Examples",c={},d=[{value:"Introduction to Object Detection in Robotics",id:"introduction-to-object-detection-in-robotics",level:2},{value:"Isaac ROS Perception Pipeline",id:"isaac-ros-perception-pipeline",level:2},{value:"Overview of Isaac ROS Perception Stack",id:"overview-of-isaac-ros-perception-stack",level:3}];function _(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"object-detection-and-localization-examples",children:"Object Detection and Localization Examples"})}),"\n",(0,o.jsx)(n.p,{children:"This section provides practical examples of object detection and localization using NVIDIA Isaac technologies, demonstrating how AI-powered perception systems work in robotics applications."}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-object-detection-in-robotics",children:"Introduction to Object Detection in Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Object detection in robotics involves identifying and localizing objects in the robot's environment. This capability is crucial for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Navigation and path planning"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation and grasping"}),"\n",(0,o.jsx)(n.li,{children:"Scene understanding"}),"\n",(0,o.jsx)(n.li,{children:"Human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Autonomous decision making"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"overview-of-isaac-ros-perception-stack",children:"Overview of Isaac ROS Perception Stack"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Isaac ROS Perception\n\u251c\u2500\u2500 Isaac ROS Image Pipeline\n\u2502   \u251c\u2500\u2500 Image Proc\n\u2502   \u251c\u2500\u2500 Rectification\n\u2502   \u2514\u2500\u2500 Format Conversion\n\u251c\u2500\u2500 Isaac ROS Visual SLAM\n\u2502   \u251c\u2500\u2500 Feature Detection\n\u2502   \u251c\u2500\u2500 Pose Estimation\n\u2502   \u2514\u2500\u2500 Map Building\n\u251c\u2500\u2500 Isaac ROS Object Detection\n\u2502   \u251c\u2500\u2500 Deep Learning Models\n\u2502   \u251c\u2500\u2500 TensorRT Optimization\n\u2502   \u2514\u2500\u2500 Post-processing\n\u251c\u2500\u2500 Isaac ROS Pose Estimation\n\u2502   \u251c\u2500\u2500 2D-3D Correspondence\n\u2502   \u251c\u2500\u2500 PnP Solvers\n\u2502   \u2514\u2500\u2500 Refinement\n\u2514\u2500\u2500 Isaac ROS Bi3D\n    \u251c\u2500\u2500 3D Segmentation\n    \u251c\u2500\u2500 Depth Estimation\n    \u2514\u2500\u2500 Instance Segmentation\n```\n\n## Isaac ROS Object Detection Examples\n\n### 1. Isaac ROS DetectNet\n\nDetectNet is NVIDIA\'s specialized network for object detection optimized for robotics applications.\n\n#### Basic DetectNet Node Implementation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass IsaacDetectNetNode : public rclcpp::Node\n{\npublic:\n    IsaacDetectNetNode() : Node("isaac_detectnet_node")\n    {\n        // Create subscribers\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "image_input", 10,\n            std::bind(&IsaacDetectNetNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            "camera_info", 10,\n            std::bind(&IsaacDetectNetNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        // Create publisher for detections\n        detection_pub_ = this->create_publisher<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            "detections", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Process image through DetectNet model\n        auto detections = runDetectNetInference(cv_ptr->image);\n\n        // Create detection message\n        auto detection_msg = createDetectionMessage(detections, image_msg->header);\n\n        // Publish detections\n        detection_pub_->publish(detection_msg);\n    }\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr info_msg)\n    {\n        camera_info_ = *info_msg;\n    }\n\n    std::vector<Detection> runDetectNetInference(const cv::Mat& image)\n    {\n        // This would interface with the actual DetectNet model\n        // In practice, this uses TensorRT for optimized inference\n        std::vector<Detection> detections;\n\n        // Placeholder for actual inference\n        // In real implementation, this would:\n        // 1. Preprocess image for the model\n        // 2. Run inference using TensorRT\n        // 3. Post-process results\n        // 4. Apply non-maximum suppression\n        // 5. Filter by confidence threshold\n\n        return detections;\n    }\n\n    isaac_ros_detectnet_interfaces::msg::DetectionArray createDetectionMessage(\n        const std::vector<Detection>& detections,\n        const std_msgs::msg::Header& header)\n    {\n        isaac_ros_detectnet_interfaces::msg::DetectionArray detection_array;\n        detection_array.header = header;\n\n        for (const auto& detection : detections) {\n            isaac_ros_detectnet_interfaces::msg::Detection det_msg;\n            det_msg.label = detection.label;\n            det_msg.confidence = detection.confidence;\n\n            // Bounding box coordinates\n            det_msg.bbox.center.x = detection.center_x;\n            det_msg.bbox.center.y = detection.center_y;\n            det_msg.bbox.size_x = detection.width;\n            det_msg.bbox.size_y = detection.height;\n\n            detection_array.detections.push_back(det_msg);\n        }\n\n        return detection_array;\n    }\n\n    struct Detection {\n        std::string label;\n        float confidence;\n        float center_x, center_y;\n        float width, height;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_pub_;\n    sensor_msgs::msg::CameraInfo camera_info_;\n};\n```\n\n#### DetectNet Launch Configuration\n\n```xml\n\x3c!-- detectnet.launch.xml --\x3e\n<launch>\n  \x3c!-- Image rectification --\x3e\n  <node pkg="isaac_ros_image_proc" exec="isaac_ros_image_proc" name="image_proc">\n    <param name="input_encoding" value="bgr8"/>\n    <param name="output_encoding" value="bgr8"/>\n  </node>\n\n  \x3c!-- DetectNet node --\x3e\n  <node pkg="isaac_ros_detectnet" exec="isaac_ros_detectnet" name="detectnet">\n    <param name="model_name" value="ssd_mobilenet_v2_coco"/>\n    <param name="input_topic" value="/image_rect_color"/>\n    <param name="output_topic" value="/detections"/>\n    <param name="confidence_threshold" value="0.5"/>\n    <param name="max_objects" value="10"/>\n  </node>\n\n  \x3c!-- Visualization node --\x3e\n  <node pkg="isaac_ros_visualization" exec="detection_visualizer" name="detection_visualizer">\n    <param name="image_topic" value="/image_rect_color"/>\n    <param name="detection_topic" value="/detections"/>\n    <param name="output_topic" value="/detection_image"/>\n  </node>\n</launch>\n```\n\n### 2. Isaac ROS Bi3D (3D Object Detection)\n\nBi3D provides 3D object detection and segmentation capabilities.\n\n#### Bi3D Node Implementation\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <stereo_msgs/msg/disparity_image.hpp>\n#include <isaac_ros_bi3d_interfaces/msg/bi3_d_inference_array.hpp>\n\nclass IsaacBi3DNode : public rclcpp::Node\n{\npublic:\n    IsaacBi3DNode() : Node("isaac_bi3d_node")\n    {\n        // Subscribe to stereo image pair\n        left_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "left/image_rect_color", 10,\n            std::bind(&IsaacBi3DNode::leftImageCallback, this, std::placeholders::_1)\n        );\n\n        right_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "right/image_rect_color", 10,\n            std::bind(&IsaacBi3DNode::rightImageCallback, this, std::placeholders::_1)\n        );\n\n        // Subscribe to disparity for depth\n        disparity_sub_ = this->create_subscription<stereo_msgs::msg::DisparityImage>(\n            "disparity", 10,\n            std::bind(&IsaacBi3DNode::disparityCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for 3D detections\n        bi3d_pub_ = this->create_publisher<isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray>(\n            "bi3d_detections", 10\n        );\n    }\n\nprivate:\n    void leftImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        if (has_right_image_ && has_disparity_) {\n            processStereoPair(msg, right_image_, disparity_);\n        } else {\n            left_image_ = msg;\n        }\n    }\n\n    void rightImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        has_right_image_ = true;\n        right_image_ = msg;\n\n        if (has_left_image_ && has_disparity_) {\n            processStereoPair(left_image_, msg, disparity_);\n        }\n    }\n\n    void disparityCallback(const stereo_msgs::msg::DisparityImage::SharedPtr msg)\n    {\n        has_disparity_ = true;\n        disparity_ = msg;\n\n        if (has_left_image_ && has_right_image_) {\n            processStereoPair(left_image_, right_image_, msg);\n        }\n    }\n\n    void processStereoPair(\n        const sensor_msgs::msg::Image::SharedPtr left,\n        const sensor_msgs::msg::Image::SharedPtr right,\n        const stereo_msgs::msg::DisparityImage::SharedPtr disparity)\n    {\n        // Run Bi3D inference\n        auto bi3d_results = runBi3DInference(left, right);\n\n        // Create 3D detection message\n        auto bi3d_msg = createBi3DMessage(bi3d_results, left->header);\n\n        // Publish results\n        bi3d_pub_->publish(bi3d_msg);\n\n        // Reset flags\n        has_left_image_ = false;\n        has_right_image_ = false;\n        has_disparity_ = false;\n    }\n\n    std::vector<Bi3DResult> runBi3DInference(\n        const sensor_msgs::msg::Image::SharedPtr left,\n        const sensor_msgs::msg::Image::SharedPtr right)\n    {\n        // Placeholder for actual Bi3D inference\n        // This would:\n        // 1. Process stereo images through Bi3D network\n        // 2. Generate 3D segmentation masks\n        // 3. Extract 3D bounding boxes\n        // 4. Estimate 3D poses\n\n        std::vector<Bi3DResult> results;\n        // Implementation would go here\n        return results;\n    }\n\n    isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray createBi3DMessage(\n        const std::vector<Bi3DResult>& results,\n        const std_msgs::msg::Header& header)\n    {\n        isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray bi3d_array;\n        bi3d_array.header = header;\n\n        for (const auto& result : results) {\n            isaac_ros_bi3d_interfaces::msg::Bi3DInference bi3d_msg;\n            bi3d_msg.class_id = result.class_id;\n            bi3d_msg.confidence = result.confidence;\n\n            // 3D bounding box\n            bi3d_msg.bounding_box_3d.center.position.x = result.center_x;\n            bi3d_msg.bounding_box_3d.center.position.y = result.center_y;\n            bi3d_msg.bounding_box_3d.center.position.z = result.center_z;\n\n            // Convert Euler angles to quaternion\n            tf2::Quaternion q;\n            q.setRPY(result.roll, result.pitch, result.yaw);\n            bi3d_msg.bounding_box_3d.center.orientation.x = q.x();\n            bi3d_msg.bounding_box_3d.center.orientation.y = q.y();\n            bi3d_msg.bounding_box_3d.center.orientation.z = q.z();\n            bi3d_msg.bounding_box_3d.center.orientation.w = q.w();\n\n            bi3d_msg.bounding_box_3d.size.x = result.size_x;\n            bi3d_msg.bounding_box_3d.size.y = result.size_y;\n            bi3d_msg.bounding_box_3d.size.z = result.size_z;\n\n            bi3d_array.inferences.push_back(bi3d_msg);\n        }\n\n        return bi3d_array;\n    }\n\n    struct Bi3DResult {\n        int class_id;\n        float confidence;\n        float center_x, center_y, center_z;\n        float roll, pitch, yaw;\n        float size_x, size_y, size_z;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr left_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr right_sub_;\n    rclcpp::Subscription<stereo_msgs::msg::DisparityImage>::SharedPtr disparity_sub_;\n    rclcpp::Publisher<isaac_ros_bi3d_interfaces::msg::Bi3DInferenceArray>::SharedPtr bi3d_pub_;\n\n    sensor_msgs::msg::Image::SharedPtr left_image_;\n    sensor_msgs::msg::Image::SharedPtr right_image_;\n    stereo_msgs::msg::DisparityImage::SharedPtr disparity_;\n\n    bool has_left_image_ = false;\n    bool has_right_image_ = false;\n    bool has_disparity_ = false;\n};\n```\n\n## Object Localization Examples\n\n### 1. Camera-Object 3D Localization\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ObjectLocalizationNode : public rclcpp::Node\n{\npublic:\n    ObjectLocalizationNode() : Node("object_localization_node"), tf_buffer_(this->get_clock())\n    {\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            "detections", 10,\n            std::bind(&ObjectLocalizationNode::detectionCallback, this, std::placeholders::_1)\n        );\n\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            "camera_info", 10,\n            std::bind(&ObjectLocalizationNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        object_pose_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            "object_3d_position", 10\n        );\n\n        tf_listener_ = std::make_shared<tf2_ros::TransformListener>(tf_buffer_);\n    }\n\nprivate:\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr detections)\n    {\n        if (!camera_info_received_ || !has_camera_to_robot_tf_) {\n            RCLCPP_WARN(this->get_logger(), "Camera info or TF not available yet");\n            return;\n        }\n\n        for (const auto& detection : detections->detections) {\n            if (detection.confidence < confidence_threshold_) {\n                continue;  // Skip low-confidence detections\n            }\n\n            // Convert 2D bounding box center to 3D point\n            geometry_msgs::msg::PointStamped pixel_point;\n            pixel_point.header = detections->header;\n            pixel_point.point.x = detection.bbox.center.x;\n            pixel_point.point.y = detection.bbox.center.y;\n            pixel_point.point.z = 1.0;  // Placeholder depth\n\n            // Convert pixel coordinates to 3D camera frame\n            geometry_msgs::msg::PointStamped camera_point;\n            camera_point = pixelToCameraFrame(pixel_point);\n\n            // Transform to robot base frame\n            geometry_msgs::msg::PointStamped robot_point;\n            robot_point = transformToRobotFrame(camera_point);\n\n            // Create and publish object position\n            geometry_msgs::msg::PointStamped object_position;\n            object_position.header = robot_point.header;\n            object_position.point = robot_point.point;\n\n            // Add object label as metadata (in a real system, you might publish this separately)\n            RCLCPP_INFO(this->get_logger(),\n                "Detected %s at position: (%.2f, %.2f, %.2f) with confidence %.2f",\n                detection.label.c_str(),\n                object_position.point.x,\n                object_position.point.y,\n                object_position.point.z,\n                detection.confidence\n            );\n\n            object_pose_pub_->publish(object_position);\n        }\n    }\n\n    geometry_msgs::msg::PointStamped pixelToCameraFrame(const geometry_msgs::msg::PointStamped& pixel_point)\n    {\n        geometry_msgs::msg::PointStamped camera_point;\n        camera_point.header = pixel_point.header;  // Keep same timestamp/frame initially\n\n        // Convert pixel coordinates to normalized coordinates\n        double x_norm = (pixel_point.point.x - camera_info_.k[2]) / camera_info_.k[0];  // cx, fx\n        double y_norm = (pixel_point.point.y - camera_info_.k[5]) / camera_info_.k[4];  // cy, fy\n\n        // For this example, assume depth is known from other sources\n        // In practice, you\'d get depth from stereo, LIDAR, or depth sensor\n        double depth = estimateDepth(pixel_point.point.x, pixel_point.point.y);\n\n        camera_point.point.x = x_norm * depth;\n        camera_point.point.y = y_norm * depth;\n        camera_point.point.z = depth;\n\n        return camera_point;\n    }\n\n    geometry_msgs::msg::PointStamped transformToRobotFrame(const geometry_msgs::msg::PointStamped& camera_point)\n    {\n        geometry_msgs::msg::PointStamped robot_point;\n\n        try {\n            // Transform from camera frame to robot base frame\n            tf_buffer_.transform(camera_point, robot_point, "base_link");\n        } catch (tf2::TransformException& ex) {\n            RCLCPP_ERROR(this->get_logger(), "Transform failed: %s", ex.what());\n            return camera_point;  // Return original if transform fails\n        }\n\n        return robot_point;\n    }\n\n    double estimateDepth(double u, double v)\n    {\n        // Placeholder depth estimation\n        // In a real system, this would come from:\n        // 1. Stereo vision\n        // 2. Depth sensor (RGB-D camera, LIDAR)\n        // 3. Monocular depth estimation\n        // 4. Object size-based estimation (if object size is known)\n\n        // For this example, return a fixed depth\n        // A more realistic approach would use stereo disparity or other depth sources\n        return 1.0;  // 1 meter depth as placeholder\n    }\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        camera_info_ = *msg;\n        camera_info_received_ = true;\n    }\n\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr object_pose_pub_;\n\n    tf2_ros::Buffer tf_buffer_;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n\n    sensor_msgs::msg::CameraInfo camera_info_;\n    bool camera_info_received_ = false;\n    bool has_camera_to_robot_tf_ = false;\n\n    const double confidence_threshold_ = 0.7;\n};\n```\n\n### 2. Semantic Segmentation for Object Localization\n\n```cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n\nclass SemanticSegmentationNode : public rclcpp::Node\n{\npublic:\n    SemanticSegmentationNode() : Node("semantic_segmentation_node")\n    {\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "image_input", 10,\n            std::bind(&SemanticSegmentationNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        segmentation_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            "segmentation_output", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Run semantic segmentation\n        cv::Mat segmentation_mask = runSegmentationInference(cv_ptr->image);\n\n        // Create result image with segmentation overlay\n        cv::Mat result_image = createSegmentationOverlay(cv_ptr->image, segmentation_mask);\n\n        // Publish segmentation result\n        publishSegmentationResult(result_image, image_msg->header);\n    }\n\n    cv::Mat runSegmentationInference(const cv::Mat& image)\n    {\n        // Placeholder for actual segmentation inference\n        // This would typically use a model like DeepLab, SegNet, or similar\n        // For Isaac ROS, this might use Isaac ROS Segmentation packages\n\n        cv::Mat segmentation_mask;\n\n        // In a real implementation, this would:\n        // 1. Preprocess image for the segmentation model\n        // 2. Run inference using TensorRT\n        // 3. Post-process to get class labels for each pixel\n        // 4. Return a mask where each pixel value represents the class ID\n\n        // For this example, return a dummy mask\n        segmentation_mask = cv::Mat::zeros(image.size(), CV_8UC1);\n\n        // Simulate detection of a few classes in specific regions\n        cv::rectangle(segmentation_mask, cv::Rect(100, 100, 200, 150), cv::Scalar(1), -1); // Class 1\n        cv::rectangle(segmentation_mask, cv::Rect(300, 200, 150, 100), cv::Scalar(2), -1); // Class 2\n\n        return segmentation_mask;\n    }\n\n    cv::Mat createSegmentationOverlay(const cv::Mat& original_image, const cv::Mat& segmentation_mask)\n    {\n        cv::Mat overlay = original_image.clone();\n\n        // Define colors for different classes\n        std::vector<cv::Vec3b> class_colors = {\n            cv::Vec3b(0, 0, 0),      // Class 0: background (black)\n            cv::Vec3b(255, 0, 0),    // Class 1: red\n            cv::Vec3b(0, 255, 0),    // Class 2: green\n            cv::Vec3b(0, 0, 255),    // Class 3: blue\n            cv::Vec3b(255, 255, 0),  // Class 4: cyan\n            cv::Vec3b(255, 0, 255),  // Class 5: magenta\n        };\n\n        // Create overlay with transparency\n        for (int y = 0; y < segmentation_mask.rows; y++) {\n            for (int x = 0; x < segmentation_mask.cols; x++) {\n                int class_id = segmentation_mask.at<uchar>(y, x);\n                if (class_id > 0 && class_id < class_colors.size()) {\n                    // Blend original color with class color\n                    cv::Vec3b& pixel = overlay.at<cv::Vec3b>(y, x);\n                    cv::Vec3b class_color = class_colors[class_id];\n\n                    // Simple blending (50% original, 50% class color)\n                    pixel = 0.5 * pixel + 0.5 * class_color;\n                }\n            }\n        }\n\n        return overlay;\n    }\n\n    void publishSegmentationResult(const cv::Mat& result_image, const std_msgs::msg::Header& header)\n    {\n        cv_bridge::CvImage cv_image;\n        cv_image.header = header;\n        cv_image.encoding = sensor_msgs::image_encodings::BGR8;\n        cv_image.image = result_image;\n\n        segmentation_pub_->publish(*cv_image.toImageMsg());\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr segmentation_pub_;\n};\n```\n\n## Isaac Sim Perception Integration\n\n### Isaac Sim Perception Configuration\n\n```python\n# Isaac Sim perception setup\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.range_sensor import RotatingLidarPhysX\nimport numpy as np\n\nclass IsaacSimPerception:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_perception_sensors()\n\n    def setup_perception_sensors(self):\n        # Add a robot to the scene\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            print("Could not find Isaac Sim assets")\n            return\n\n        # Add a simple robot with sensors\n        robot_path = assets_root_path + "/Isaac/Robots/Carter/carter_navigate.usd"\n        add_reference_to_stage(usd_path=robot_path, prim_path="/World/Carter")\n\n        # Add a camera sensor\n        self.camera = Camera(\n            prim_path="/World/Carter/chassis/camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Add a LIDAR sensor\n        self.lidar = RotatingLidarPhysX(\n            prim_path="/World/Carter/chassis/lidar",\n            translation=np.array([0.0, 0.0, 0.25]),\n            config="Carter",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Initialize the world\n        self.world.reset()\n\n    def get_sensor_data(self):\n        # Get camera data\n        rgb_data = self.camera.get_rgb()\n        depth_data = self.camera.get_depth()\n        seg_data = self.camera.get_semantic_segmentation()\n\n        # Get LIDAR data\n        lidar_data = self.lidar.get_linear_depth_data()\n\n        return {\n            \'rgb\': rgb_data,\n            \'depth\': depth_data,\n            \'segmentation\': seg_data,\n            \'lidar\': lidar_data\n        }\n\n    def run_perception_pipeline(self):\n        # Main perception loop\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get sensor data\n            sensor_data = self.get_sensor_data()\n\n            # Process perception data\n            objects = self.detect_objects(sensor_data)\n\n            # Localize objects in world coordinates\n            object_poses = self.localize_objects(objects, sensor_data)\n\n            # Print results\n            self.print_perception_results(object_poses)\n\n    def detect_objects(self, sensor_data):\n        # Placeholder for object detection\n        # In Isaac Sim, this would interface with Isaac ROS perception packages\n        # or use built-in synthetic data generation\n\n        # For this example, return simulated detections\n        objects = [\n            {\'class\': \'box\', \'confidence\': 0.95, \'bbox\': [100, 100, 200, 150]},\n            {\'class\': \'cylinder\', \'confidence\': 0.89, \'bbox\': [300, 200, 150, 100]}\n        ]\n\n        return objects\n\n    def localize_objects(self, objects, sensor_data):\n        # Convert 2D detections to 3D world coordinates\n        # This would use depth information and camera parameters\n\n        object_poses = []\n\n        for obj in objects:\n            # Convert 2D bbox center to 3D using depth\n            center_x = (obj[\'bbox\'][0] + obj[\'bbox\'][2]) // 2\n            center_y = (obj[\'bbox\'][1] + obj[\'bbox\'][3]) // 2\n\n            # Get depth at center point\n            depth = sensor_data[\'depth\'][center_y, center_x]\n\n            if depth < 10.0:  # Valid depth check\n                # Convert pixel coordinates to world coordinates\n                # This requires camera intrinsic parameters\n                world_pos = self.pixel_to_world(\n                    center_x, center_y, depth,\n                    self.camera.prim.GetAttribute("xformOp:transform").Get()\n                )\n\n                object_poses.append({\n                    \'class\': obj[\'class\'],\n                    \'position\': world_pos,\n                    \'confidence\': obj[\'confidence\']\n                })\n\n        return object_poses\n\n    def pixel_to_world(self, u, v, depth, camera_transform):\n        # Convert pixel coordinates to world coordinates\n        # This is a simplified version - in practice, you\'d use camera intrinsics\n\n        # Camera intrinsic parameters (these would come from camera config)\n        fx = 616.363  # Focal length x\n        fy = 616.363  # Focal length y\n        cx = 313.071  # Principal point x\n        cy = 245.091  # Principal point y\n\n        # Convert to camera coordinates\n        x_cam = (u - cx) * depth / fx\n        y_cam = (v - cy) * depth / fy\n        z_cam = depth\n\n        # Transform to world coordinates using camera pose\n        # (simplified - would need proper transformation matrix)\n        x_world = x_cam  # Simplified\n        y_world = y_cam\n        z_world = z_cam\n\n        return [x_world, y_world, z_world]\n\n    def print_perception_results(self, object_poses):\n        print("Perception Results:")\n        for obj in object_poses:\n            print(f"  {obj[\'class\']}: ({obj[\'position\'][0]:.2f}, {obj[\'position\'][1]:.2f}, {obj[\'position\'][2]:.2f}), conf: {obj[\'confidence\']:.2f}")\n```\n\n## Practical Examples\n\n### Example 1: Person Detection and Localization\n\n```cpp\n// Complete example for detecting and localizing people\nclass PersonDetectionNode : public rclcpp::Node\n{\npublic:\n    PersonDetectionNode() : Node("person_detection_node")\n    {\n        // Subscribe to camera image\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image_raw", 10,\n            std::bind(&PersonDetectionNode::imageCallback, this, std::placeholders::_1)\n        );\n\n        // Subscribe to camera info\n        camera_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            "camera/camera_info", 10,\n            std::bind(&PersonDetectionNode::cameraInfoCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for person positions\n        person_pub_ = this->create_publisher<geometry_msgs::msg::PointStamped>(\n            "person_position", 10\n        );\n\n        // Publisher for visualization\n        viz_pub_ = this->create_publisher<sensor_msgs::msg::Image>(\n            "person_detection_viz", 10\n        );\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        if (!camera_info_received_) return;\n\n        // Convert to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(image_msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Run person detection\n        std::vector<PersonDetection> persons = detectPersons(cv_ptr->image);\n\n        // Process each detected person\n        for (const auto& person : persons) {\n            if (person.confidence > 0.8) {  // Confidence threshold\n                // Localize person in 3D space\n                geometry_msgs::msg::PointStamped person_3d = localizePerson(\n                    person, image_msg->header\n                );\n\n                // Publish person position\n                person_pub_->publish(person_3d);\n\n                // Add to visualization\n                cv::rectangle(cv_ptr->image, person.bbox, cv::Scalar(0, 255, 0), 2);\n                std::string label = "Person: " + std::to_string(person.confidence);\n                cv::putText(cv_ptr->image, label,\n                           cv::Point(person.bbox.x, person.bbox.y - 10),\n                           cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 1);\n            }\n        }\n\n        // Publish visualization image\n        viz_pub_->publish(*cv_ptr->toImageMsg());\n    }\n\n    std::vector<PersonDetection> detectPersons(const cv::Mat& image)\n    {\n        std::vector<PersonDetection> detections;\n\n        // In a real implementation, this would run a DNN model\n        // such as YOLO, SSD MobileNet, or Isaac ROS DetectNet\n        // For this example, we\'ll use OpenCV\'s HOG descriptor\n\n        cv::HOGDescriptor hog;\n        hog.setSVMDetector(cv::HOGDescriptor::getDefaultPeopleDetector());\n\n        std::vector<cv::Rect> found_locations;\n        std::vector<double> found_weights;\n\n        hog.detectMultiScale(image, found_locations, found_weights, 0, cv::Size(8,8), cv::Size(32,32), 1.05, 2, false);\n\n        for (size_t i = 0; i < found_locations.size(); ++i) {\n            PersonDetection detection;\n            detection.bbox = found_locations[i];\n            detection.confidence = found_weights[i];\n            detection.center_x = detection.bbox.x + detection.bbox.width / 2.0;\n            detection.center_y = detection.bbox.y + detection.bbox.height / 2.0;\n            detections.push_back(detection);\n        }\n\n        return detections;\n    }\n\n    geometry_msgs::msg::PointStamped localizePerson(\n        const PersonDetection& person,\n        const std_msgs::msg::Header& header)\n    {\n        geometry_msgs::msg::PointStamped person_3d;\n        person_3d.header = header;\n\n        // Estimate depth using simple heuristics\n        // In practice, you\'d use stereo vision or depth sensor\n        double depth = estimatePersonDepth(person.bbox.height);\n\n        // Convert pixel to 3D coordinates\n        double x_norm = (person.center_x - camera_info_.k[2]) / camera_info_.k[0];\n        double y_norm = (person.center_y - camera_info_.k[5]) / camera_info_.k[4];\n\n        person_3d.point.x = x_norm * depth;\n        person_3d.point.y = y_norm * depth;\n        person_3d.point.z = depth;\n\n        return person_3d;\n    }\n\n    double estimatePersonDepth(int bbox_height)\n    {\n        // Simple depth estimation based on bounding box height\n        // Assumes average person height is ~1.7m\n        // height_in_pixels = (focal_length * real_height) / depth\n        // So depth = (focal_length * real_height) / height_in_pixels\n\n        double focal_length = camera_info_.k[0];  // fx\n        double real_person_height = 1.7;  // meters\n        double pixel_height = static_cast<double>(bbox_height);\n\n        return (focal_length * real_person_height) / pixel_height;\n    }\n\n    struct PersonDetection {\n        cv::Rect bbox;\n        double confidence;\n        double center_x, center_y;\n    };\n\n    void cameraInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        camera_info_ = *msg;\n        camera_info_received_ = true;\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PointStamped>::SharedPtr person_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr viz_pub_;\n\n    sensor_msgs::msg::CameraInfo camera_info_;\n    bool camera_info_received_ = false;\n};\n```\n\n### Example 2: Object Detection with Isaac ROS and Isaac Sim\n\n```python\n# Isaac Sim + Isaac ROS integration example\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom isaac_ros_detectnet_interfaces.msg import DetectionArray\nfrom geometry_msgs.msg import PointStamped\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_perception_pipeline\')\n\n        # ROS 2 interface\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/camera_info\', self.camera_info_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            DetectionArray, \'/detectnet/detections\', self.detection_callback, 10\n        )\n        self.object_pub = self.create_publisher(\n            PointStamped, \'/detected_object_position\', 10\n        )\n        self.viz_pub = self.create_publisher(\n            Image, \'/perception_visualization\', 10\n        )\n\n        # Storage\n        self.camera_info = None\n        self.latest_image = None\n\n    def image_callback(self, msg):\n        self.latest_image = msg\n\n    def camera_info_callback(self, msg):\n        self.camera_info = msg\n\n    def detection_callback(self, msg):\n        if self.latest_image is None or self.camera_info is None:\n            return\n\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(self.latest_image, "bgr8")\n\n        # Process detections\n        for detection in msg.detections:\n            if detection.confidence > 0.7:  # Confidence threshold\n                # Localize object in 3D\n                object_3d = self.localize_object_3d(\n                    detection.bbox.center.x,\n                    detection.bbox.center.y,\n                    detection\n                )\n\n                # Publish 3D position\n                self.object_pub.publish(object_3d)\n\n                # Draw bounding box on image\n                pt1 = (int(detection.bbox.center.x - detection.bbox.size_x/2),\n                       int(detection.bbox.center.y - detection.bbox.size_y/2))\n                pt2 = (int(detection.bbox.center.x + detection.bbox.size_x/2),\n                       int(detection.bbox.center.y + detection.bbox.size_y/2))\n                cv2.rectangle(cv_image, pt1, pt2, (0, 255, 0), 2)\n                cv2.putText(cv_image, f"{detection.label}: {detection.confidence:.2f}",\n                           (pt1[0], pt1[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n        # Publish visualization\n        viz_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding="bgr8")\n        viz_msg.header = self.latest_image.header\n        self.viz_pub.publish(viz_msg)\n\n    def localize_object_3d(self, x_2d, y_2d, detection):\n        # This is a simplified example\n        # In practice, you\'d use depth information from stereo or depth sensor\n        point_3d = PointStamped()\n        point_3d.header = detection.header\n\n        # Estimate depth based on object size or use depth map\n        # For this example, assume a fixed depth of 2 meters\n        estimated_depth = 2.0  # meters\n\n        # Convert 2D pixel coordinates to 3D using camera intrinsics\n        if self.camera_info:\n            fx = self.camera_info.k[0]  # Focal length x\n            fy = self.camera_info.k[4]  # Focal length y\n            cx = self.camera_info.k[2]  # Principal point x\n            cy = self.camera_info.k[5]  # Principal point y\n\n            point_3d.point.x = (x_2d - cx) * estimated_depth / fx\n            point_3d.point.y = (y_2d - cy) * estimated_depth / fy\n            point_3d.point.z = estimated_depth\n\n        return point_3d\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n```\n\n## Performance Optimization\n\n### 1. TensorRT Optimization for Deep Learning Models\n\n```cpp\n// Example of TensorRT optimization for perception\n#include <NvInfer.h>\n#include <cuda_runtime_api.h>\n\nclass OptimizedPerceptionNode : public rclcpp::Node\n{\npublic:\n    OptimizedPerceptionNode() : Node("optimized_perception_node")\n    {\n        // Initialize TensorRT engine\n        initializeTensorRTEngine();\n    }\n\nprivate:\n    void initializeTensorRTEngine()\n    {\n        // This would load a pre-built TensorRT engine\n        // for optimized inference of perception models\n        // The engine would be built offline from ONNX models\n    }\n\n    std::vector<Detection> runOptimizedInference(const cv::Mat& image)\n    {\n        // Run inference using TensorRT for maximum performance\n        // This would include:\n        // 1. Memory management for GPU\n        // 2. Batch processing\n        // 3. Asynchronous execution\n        // 4. Proper input/output binding\n\n        std::vector<Detection> detections;\n        // Implementation would go here\n        return detections;\n    }\n\n    nvinfer1::ICudaEngine* engine_;\n    nvinfer1::IExecutionContext* context_;\n    cudaStream_t stream_;\n    void* buffers_[2];  // Input and output buffers\n};\n```\n\n### 2. Multi-Threaded Perception Pipeline\n\n```cpp\n#include <thread>\n#include <queue>\n#include <mutex>\n#include <condition_variable>\n\nclass MultiThreadedPerceptionNode : public rclcpp::Node\n{\npublic:\n    MultiThreadedPerceptionNode() : Node("multithreaded_perception_node")\n    {\n        // Create threads for different perception tasks\n        detection_thread_ = std::thread(&MultiThreadedPerceptionNode::detectionLoop, this);\n        localization_thread_ = std::thread(&MultiThreadedPerceptionNode::localizationLoop, this);\n\n        // Subscribe to image\n        image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "image_input", 10,\n            std::bind(&MultiThreadedPerceptionNode::imageCallback, this, std::placeholders::_1)\n        );\n    }\n\n    ~MultiThreadedPerceptionNode()\n    {\n        running_ = false;\n        if (detection_thread_.joinable()) detection_thread_.join();\n        if (localization_thread_.joinable()) localization_thread_.join();\n    }\n\nprivate:\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        std::lock_guard<std::mutex> lock(image_queue_mutex_);\n        image_queue_.push(image_msg);\n\n        if (image_queue_.size() > max_queue_size_) {\n            image_queue_.pop();  // Drop oldest if queue is full\n        }\n\n        image_queue_cond_.notify_one();\n    }\n\n    void detectionLoop()\n    {\n        while (running_) {\n            sensor_msgs::msg::Image::SharedPtr image_msg;\n\n            {\n                std::unique_lock<std::mutex> lock(image_queue_mutex_);\n                image_queue_cond_.wait(lock, [this] { return !image_queue_.empty() || !running_; });\n\n                if (!running_) break;\n\n                image_msg = image_queue_.front();\n                image_queue_.pop();\n            }\n\n            // Run object detection\n            auto detections = runDetection(image_msg);\n\n            // Add to detection queue\n            {\n                std::lock_guard<std::mutex> lock(detection_queue_mutex_);\n                detection_queue_.push(std::make_pair(image_msg->header, detections));\n            }\n\n            detection_queue_cond_.notify_one();\n        }\n    }\n\n    void localizationLoop()\n    {\n        while (running_) {\n            std_msgs::msg::Header header;\n            std::vector<Detection> detections;\n\n            {\n                std::unique_lock<std::mutex> lock(detection_queue_mutex_);\n                detection_queue_cond_.wait(lock, [this] { return !detection_queue_.empty() || !running_; });\n\n                if (!running_) break;\n\n                auto detection_pair = detection_queue_.front();\n                header = detection_pair.first;\n                detections = detection_pair.second;\n                detection_queue_.pop();\n            }\n\n            // Run localization\n            auto object_positions = runLocalization(detections, header);\n\n            // Publish results\n            publishResults(object_positions);\n        }\n    }\n\n    std::vector<Detection> runDetection(const sensor_msgs::msg::Image::SharedPtr image_msg)\n    {\n        // Run object detection on the image\n        // Implementation would go here\n        return std::vector<Detection>();\n    }\n\n    std::vector<ObjectPosition> runLocalization(\n        const std::vector<Detection>& detections,\n        const std_msgs::msg::Header& header)\n    {\n        // Localize objects in 3D space\n        // Implementation would go here\n        return std::vector<ObjectPosition>();\n    }\n\n    void publishResults(const std::vector<ObjectPosition>& positions)\n    {\n        // Publish localization results\n        // Implementation would go here\n    }\n\n    struct Detection {\n        std::string label;\n        float confidence;\n        cv::Rect bbox;\n    };\n\n    struct ObjectPosition {\n        std::string label;\n        geometry_msgs::msg::Point position;\n        float confidence;\n    };\n\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr image_sub_;\n\n    // Image processing queue\n    std::queue<sensor_msgs::msg::Image::SharedPtr> image_queue_;\n    std::mutex image_queue_mutex_;\n    std::condition_variable image_queue_cond_;\n\n    // Detection queue\n    std::queue<std::pair<std_msgs::msg::Header, std::vector<Detection>>> detection_queue_;\n    std::mutex detection_queue_mutex_;\n    std::condition_variable detection_queue_cond_;\n\n    std::thread detection_thread_;\n    std::thread localization_thread_;\n    std::atomic<bool> running_{true};\n    const size_t max_queue_size_ = 5;\n};\n```\n\n## Best Practices\n\n### 1. Confidence Thresholding\nAlways use confidence thresholds to filter out low-quality detections:\n- Set appropriate thresholds based on your application requirements\n- Consider using adaptive thresholds based on scene complexity\n- Validate detections with geometric consistency checks\n\n### 2. Multi-Sensor Fusion\nCombine data from multiple sensors for robust perception:\n- Fuse camera, LIDAR, and radar data\n- Use Kalman filters or particle filters for tracking\n- Implement sensor validation and fault detection\n\n### 3. Performance Monitoring\nMonitor perception performance in real-time:\n- Track inference time and frame rates\n- Monitor memory and GPU usage\n- Log detection accuracy and false positive rates\n\n## Exercise\n\nCreate a complete perception pipeline that includes:\n\n1. Object detection using Isaac ROS DetectNet\n2. 3D localization using stereo vision or depth information\n3. Multi-threaded processing for real-time performance\n4. Integration with Isaac Sim for testing\n5. Visualization of detection results\n6. Performance evaluation metrics\n\nTest your pipeline with various objects in different lighting conditions and evaluate its accuracy and performance.\n'})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(_,{...e})}):_(e)}}}]);