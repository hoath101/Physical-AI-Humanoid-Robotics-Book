"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[256],{4069:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/llm-planning","title":"LLM Planning for Robotics","description":"Large Language Models (LLMs) play a crucial role in robotics by enabling natural language understanding, task planning, and high-level decision making. This section covers how to integrate LLMs with humanoid robots for intelligent task planning and execution.","source":"@site/docs/module-4-vla/llm-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm-planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/llm-planning","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/isaac-sim-fundamentals"},"next":{"title":"Multimodal Perception","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/multimodal-perception"}}');var i=t(4848),a=t(8453);const s={},r="LLM Planning for Robotics",l={},c=[{value:"Introduction to LLMs in Robotics",id:"introduction-to-llms-in-robotics",level:2},{value:"Key Capabilities for Robotics",id:"key-capabilities-for-robotics",level:3},{value:"LLM Integration Approaches",id:"llm-integration-approaches",level:2},{value:"1. OpenAI GPT Models",id:"1-openai-gpt-models",level:3},{value:"2. Open-Source LLM Integration",id:"2-open-source-llm-integration",level:3},{value:"Function Calling for Robotics APIs",id:"function-calling-for-robotics-apis",level:2},{value:"OpenAI Function Calling",id:"openai-function-calling",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"LLM Planning Node",id:"llm-planning-node",level:3},{value:"Advanced Planning Techniques",id:"advanced-planning-techniques",level:2},{value:"Hierarchical Task Networks (HTN)",id:"hierarchical-task-networks-htn",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Effective Prompts for Robot Planning",id:"effective-prompts-for-robot-planning",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Plan Validation System",id:"plan-validation-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Optimization",id:"caching-and-optimization",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. API Rate Limits",id:"1-api-rate-limits",level:3},{value:"2. Hallucination Issues",id:"2-hallucination-issues",level:3},{value:"3. Context Window Limitations",id:"3-context-window-limitations",level:3},{value:"4. Integration Latency",id:"4-integration-latency",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling",id:"1-error-handling",level:3},{value:"2. Security",id:"2-security",level:3},{value:"3. Performance",id:"3-performance",level:3},{value:"4. Testing",id:"4-testing",level:3},{value:"Exercise",id:"exercise",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"llm-planning-for-robotics",children:"LLM Planning for Robotics"})}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models (LLMs) play a crucial role in robotics by enabling natural language understanding, task planning, and high-level decision making. This section covers how to integrate LLMs with humanoid robots for intelligent task planning and execution."}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-llms-in-robotics",children:"Introduction to LLMs in Robotics"}),"\n",(0,i.jsx)(e.p,{children:"LLMs bring several capabilities to robotics:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpret human commands in natural language"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Planning"}),": Decompose high-level goals into executable robot actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reasoning"}),": Apply logical reasoning to handle novel situations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Knowledge Integration"}),": Access vast knowledge bases for decision making"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Enable natural communication with humans"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"key-capabilities-for-robotics",children:"Key Capabilities for Robotics"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Command Interpretation"}),": Convert natural language commands to robot actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Decomposition"}),": Break down complex tasks into atomic robot operations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Awareness"}),": Understand the environment and robot capabilities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error Handling"}),": Generate recovery strategies when tasks fail"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Learning"}),": Adapt behavior based on past experiences"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"llm-integration-approaches",children:"LLM Integration Approaches"}),"\n",(0,i.jsx)(e.h3,{id:"1-openai-gpt-models",children:"1. OpenAI GPT Models"}),"\n",(0,i.jsx)(e.p,{children:"The most straightforward approach uses OpenAI's GPT models:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import openai\nimport json\nfrom typing import Dict, List, Optional\n\nclass RobotLLMInterface:\n    def __init__(self, api_key: str, model: str = "gpt-4-turbo"):\n        openai.api_key = api_key\n        self.model = model\n        self.system_prompt = self._create_system_prompt()\n\n    def _create_system_prompt(self) -> str:\n        """Create system prompt for robot task planning"""\n        return """\n        You are a robot task planner that converts natural language commands into robot actions.\n        Your role is to:\n        1. Understand the human command in natural language\n        2. Decompose the command into specific robot actions\n        3. Consider the robot\'s capabilities and environment\n        4. Generate a sequence of executable actions\n        5. Include error handling and validation\n\n        Robot capabilities include:\n        - Navigation: Move to specific locations\n        - Manipulation: Pick up, place, grasp objects\n        - Perception: Detect and recognize objects\n        - Communication: Respond to human commands\n\n        Respond in JSON format with the following structure:\n        {\n          "task_breakdown": [\n            {\n              "step": 1,\n              "action": "action_type",\n              "parameters": {"param1": "value1", ...},\n              "description": "Human-readable description",\n              "validation": "How to verify success"\n            }\n          ],\n          "potential_issues": ["issue1", "issue2"],\n          "success_criteria": "How to know the task is complete"\n        }\n        """\n\n    def plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        """Plan a task based on natural language command"""\n        user_prompt = f"""\n        Command: {command}\n\n        Robot State: {json.dumps(robot_state, indent=2)}\n        Environment: {json.dumps(environment, indent=2)}\n\n        Generate a detailed plan to execute this command.\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": user_prompt}\n                ],\n                temperature=0.3,\n                max_tokens=1000\n            )\n\n            # Extract and parse JSON response\n            response_text = response.choices[0].message.content\n\n            # Find JSON in response (in case of additional text)\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n\n            if json_start != -1 and json_end != 0:\n                json_content = response_text[json_start:json_end]\n                plan = json.loads(json_content)\n                return plan\n            else:\n                # If no JSON found, return the raw response for error handling\n                return {"raw_response": response_text, "parsed": False}\n\n        except Exception as e:\n            return {"error": str(e), "success": False}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"2-open-source-llm-integration",children:"2. Open-Source LLM Integration"}),"\n",(0,i.jsx)(e.p,{children:"For privacy and cost considerations, open-source models can be used:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport json\n\nclass OpenSourceRobotLLM:\n    def __init__(self, model_name: str = "meta-llama/Llama-2-7b-chat-hf"):\n        """\n        Initialize with open-source LLM.\n        Note: You\'ll need to handle model access and hardware requirements appropriately.\n        """\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map="auto"\n        )\n\n        # Add pad token if missing\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        """Plan task using open-source LLM"""\n        prompt = f"""\n        [INST] <<SYS>>\n        You are a robot task planner. Convert the following natural language command into a sequence of robot actions.\n\n        Robot State: {json.dumps(robot_state)}\n        Environment: {json.dumps(environment)}\n\n        Respond in JSON format with task breakdown, potential issues, and success criteria.\n        <</SYS>>\n\n        Command: {command}\n\n        Provide a detailed plan in JSON format: [/INST]\n        """\n\n        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)\n\n        # Move inputs to the same device as the model\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=500,\n                temperature=0.3,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        response = self.tokenizer.decode(outputs[0][inputs[\'input_ids\'].shape[1]:], skip_special_tokens=True)\n\n        try:\n            # Extract JSON from response\n            json_start = response.find(\'{\')\n            json_end = response.rfind(\'}\') + 1\n            if json_start != -1 and json_end != 0:\n                json_content = response[json_start:json_end]\n                return json.loads(json_content)\n            else:\n                return {"raw_response": response, "parsed": False}\n        except json.JSONDecodeError:\n            return {"raw_response": response, "parsed": False, "error": "Could not parse JSON"}\n'})}),"\n",(0,i.jsx)(e.h2,{id:"function-calling-for-robotics-apis",children:"Function Calling for Robotics APIs"}),"\n",(0,i.jsx)(e.h3,{id:"openai-function-calling",children:"OpenAI Function Calling"}),"\n",(0,i.jsx)(e.p,{children:"LLMs can be enhanced with function calling to directly interact with robot APIs:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import json\nfrom typing import Dict, Any\n\nclass RobotFunctionCaller:\n    def __init__(self):\n        self.functions = {\n            "move_to_location": {\n                "name": "move_to_location",\n                "description": "Move the robot to a specific location",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "x": {"type": "number", "description": "X coordinate"},\n                        "y": {"type": "number", "description": "Y coordinate"},\n                        "z": {"type": "number", "description": "Z coordinate"},\n                        "orientation": {"type": "number", "description": "Orientation in radians"}\n                    },\n                    "required": ["x", "y"]\n                }\n            },\n            "pick_object": {\n                "name": "pick_object",\n                "description": "Pick up an object",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "object_name": {"type": "string", "description": "Name of the object to pick"},\n                        "location": {"type": "string", "description": "Where to find the object"}\n                    },\n                    "required": ["object_name"]\n                }\n            },\n            "place_object": {\n                "name": "place_object",\n                "description": "Place an object at a location",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "object_name": {"type": "string", "description": "Name of the object to place"},\n                        "location": {"type": "string", "description": "Where to place the object"}\n                    },\n                    "required": ["object_name", "location"]\n                }\n            },\n            "detect_objects": {\n                "name": "detect_objects",\n                "description": "Detect objects in the environment",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "area": {"type": "string", "description": "Area to scan for objects"}\n                    }\n                }\n            }\n        }\n\n    def call_robot_functions(self, command: str, llm_interface) -> Dict:\n        """Use LLM with function calling for robot planning"""\n        messages = [\n            {\n                "role": "system",\n                "content": "You are a robot task planner. Use available functions to plan and execute tasks."\n            },\n            {\n                "role": "user",\n                "content": command\n            }\n        ]\n\n        # First, let LLM decide which functions to call\n        response = openai.ChatCompletion.create(\n            model="gpt-4-turbo",\n            messages=messages,\n            functions=list(self.functions.values()),\n            function_call="auto",\n            temperature=0.3\n        )\n\n        response_message = response.choices[0].message\n\n        # If the model wants to call a function\n        if response_message.get("function_call"):\n            function_name = response_message["function_call"]["name"]\n            function_args = json.loads(response_message["function_call"]["arguments"])\n\n            # Execute the function\n            result = self.execute_function(function_name, function_args)\n\n            # Add the result to the conversation\n            messages.append(response_message)\n            messages.append({\n                "role": "function",\n                "name": function_name,\n                "content": json.dumps(result)\n            })\n\n            # Get the final response\n            second_response = openai.ChatCompletion.create(\n                model="gpt-4-turbo",\n                messages=messages,\n                temperature=0.3\n            )\n\n            return second_response.choices[0].message\n\n        return response_message\n\n    def execute_function(self, function_name: str, args: Dict[str, Any]) -> Dict[str, Any]:\n        """Execute robot function and return result"""\n        if function_name == "move_to_location":\n            return self._move_to_location(**args)\n        elif function_name == "pick_object":\n            return self._pick_object(**args)\n        elif function_name == "place_object":\n            return self._place_object(**args)\n        elif function_name == "detect_objects":\n            return self._detect_objects(**args)\n        else:\n            return {"error": f"Unknown function: {function_name}"}\n\n    def _move_to_location(self, x: float, y: float, z: float = 0.0, orientation: float = 0.0) -> Dict[str, Any]:\n        """Simulate moving to location"""\n        # In a real implementation, this would interface with navigation stack\n        return {\n            "success": True,\n            "message": f"Moved to location ({x}, {y}, {z}) with orientation {orientation}",\n            "actual_position": {"x": x, "y": y, "z": z}\n        }\n\n    def _pick_object(self, object_name: str, location: str = None) -> Dict[str, Any]:\n        """Simulate picking up an object"""\n        # In a real implementation, this would interface with manipulation stack\n        return {\n            "success": True,\n            "message": f"Picked up {object_name}",\n            "object_status": "held"\n        }\n\n    def _place_object(self, object_name: str, location: str) -> Dict[str, Any]:\n        """Simulate placing an object"""\n        # In a real implementation, this would interface with manipulation stack\n        return {\n            "success": True,\n            "message": f"Placed {object_name} at {location}",\n            "object_status": "placed"\n        }\n\n    def _detect_objects(self, area: str = "current_view") -> Dict[str, Any]:\n        """Simulate object detection"""\n        # In a real implementation, this would interface with perception stack\n        return {\n            "success": True,\n            "objects_detected": [\n                {"name": "cup", "confidence": 0.95, "position": {"x": 1.0, "y": 2.0, "z": 0.8}},\n                {"name": "book", "confidence": 0.89, "position": {"x": 1.2, "y": 2.1, "z": 0.85}}\n            ],\n            "area_scanned": area\n        }\n'})}),"\n",(0,i.jsx)(e.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(e.h3,{id:"llm-planning-node",children:"LLM Planning Node"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import JointState\nimport json\nimport openai\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planning_node\')\n\n        # Initialize LLM interface\n        api_key = self.declare_parameter(\'openai_api_key\', \'\').get_parameter_value().string_value\n        if not api_key:\n            self.get_logger().error("OpenAI API key not provided")\n            return\n\n        openai.api_key = api_key\n        self.llm_interface = RobotLLMInterface(api_key)\n\n        # Subscriptions\n        self.command_sub = self.create_subscription(\n            String,\n            \'voice_commands\',\n            self.command_callback,\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            \'joint_states\',\n            self.joint_state_callback,\n            10\n        )\n\n        self.robot_pose_sub = self.create_subscription(\n            Pose,\n            \'robot_pose\',\n            self.robot_pose_callback,\n            10\n        )\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'robot_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'llm_status\', 10)\n\n        # Robot state tracking\n        self.current_joint_state = None\n        self.current_pose = None\n\n        self.get_logger().info(\'LLM Planning Node initialized\')\n\n    def command_callback(self, msg: String):\n        """Process voice command and generate plan"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Get current robot state\n        robot_state = self.get_current_robot_state()\n        environment = self.get_environment_context()\n\n        # Generate plan using LLM\n        plan = self.llm_interface.plan_task(command, robot_state, environment)\n\n        if "error" not in plan:\n            # Publish the plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.plan_pub.publish(plan_msg)\n\n            self.get_logger().info(f\'Generated plan: {plan}\')\n        else:\n            self.get_logger().error(f\'Plan generation failed: {plan["error"]}\')\n            status_msg = String()\n            status_msg.data = f"Error: {plan[\'error\']}"\n            self.status_pub.publish(status_msg)\n\n    def joint_state_callback(self, msg: JointState):\n        """Update joint state"""\n        self.current_joint_state = msg\n\n    def robot_pose_callback(self, msg: Pose):\n        """Update robot pose"""\n        self.current_pose = msg\n\n    def get_current_robot_state(self) -> Dict:\n        """Get current robot state"""\n        state = {\n            "timestamp": self.get_clock().now().to_msg().stamp.sec,\n            "position": {},\n            "joints": {},\n            "battery_level": 100,  # Would come from battery topic\n            "capabilities": ["navigation", "manipulation", "perception"]\n        }\n\n        if self.current_pose:\n            state["position"] = {\n                "x": self.current_pose.position.x,\n                "y": self.current_pose.position.y,\n                "z": self.current_pose.position.z,\n                "orientation": {\n                    "x": self.current_pose.orientation.x,\n                    "y": self.current_pose.orientation.y,\n                    "z": self.current_pose.orientation.z,\n                    "w": self.current_pose.orientation.w\n                }\n            }\n\n        if self.current_joint_state:\n            state["joints"] = dict(zip(\n                self.current_joint_state.name,\n                self.current_joint_state.position\n            ))\n\n        return state\n\n    def get_environment_context(self) -> Dict:\n        """Get environment context"""\n        # In a real system, this would come from:\n        # - Static map\n        # - Dynamic object tracking\n        # - Sensor data\n        # - Human presence detection\n        return {\n            "known_locations": {\n                "kitchen": {"x": 5.0, "y": 3.0},\n                "living_room": {"x": 2.0, "y": 1.0},\n                "bedroom": {"x": 8.0, "y": 2.0}\n            },\n            "recently_detected_objects": [],\n            "obstacles": [],  # Would come from costmap\n            "navigation_status": "ready"\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    llm_planning_node = LLMPlanningNode()\n\n    try:\n        rclpy.spin(llm_planning_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        llm_planning_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-planning-techniques",children:"Advanced Planning Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"hierarchical-task-networks-htn",children:"Hierarchical Task Networks (HTN)"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class HTNPlanner:\n    def __init__(self):\n        self.primitive_actions = {\n            'navigate_to': self._navigate_to,\n            'pick_up': self._pick_up,\n            'place_down': self._place_down,\n            'detect_object': self._detect_object\n        }\n\n        self.complex_tasks = {\n            'fetch_object': self._decompose_fetch_object,\n            'clean_table': self._decompose_clean_table,\n            'serve_drink': self._decompose_serve_drink\n        }\n\n    def decompose_task(self, task_name: str, params: Dict) -> List[Dict]:\n        \"\"\"Decompose high-level task into primitive actions\"\"\"\n        if task_name in self.complex_tasks:\n            return self.complex_tasks[task_name](params)\n        elif task_name in self.primitive_actions:\n            return [{'action': task_name, 'params': params}]\n        else:\n            raise ValueError(f\"Unknown task: {task_name}\")\n\n    def _decompose_fetch_object(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose fetch object task\"\"\"\n        object_name = params['object_name']\n        destination = params['destination']\n\n        return [\n            {'action': 'detect_object', 'params': {'target': object_name}},\n            {'action': 'navigate_to', 'params': {'location': f'{object_name}_location'}},\n            {'action': 'pick_up', 'params': {'object': object_name}},\n            {'action': 'navigate_to', 'params': {'location': destination}},\n            {'action': 'place_down', 'params': {'object': object_name, 'location': destination}}\n        ]\n\n    def _decompose_clean_table(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose clean table task\"\"\"\n        table_location = params['table_location']\n\n        return [\n            {'action': 'navigate_to', 'params': {'location': table_location}},\n            {'action': 'detect_object', 'params': {'target': 'any_object_on_table'}},\n            # This would continue with pickup/dropoff cycles\n        ]\n\n    def _decompose_serve_drink(self, params: Dict) -> List[Dict]:\n        \"\"\"Decompose serve drink task\"\"\"\n        drink_type = params.get('drink_type', 'water')\n        customer_location = params['customer_location']\n\n        return [\n            {'action': 'navigate_to', 'params': {'location': 'kitchen'}},\n            {'action': 'detect_object', 'params': {'target': drink_type}},\n            {'action': 'pick_up', 'params': {'object': f'{drink_type}_container'}},\n            {'action': 'navigate_to', 'params': {'location': customer_location}},\n            {'action': 'place_down', 'params': {'object': f'{drink_type}_container', 'location': 'table_near_customer'}},\n            {'action': 'utter', 'params': {'text': 'Here is your drink!'}}\n        ]\n\n    def _navigate_to(self, params: Dict):\n        \"\"\"Primitive navigation action\"\"\"\n        # Interface with Nav2\n        pass\n\n    def _pick_up(self, params: Dict):\n        \"\"\"Primitive pick up action\"\"\"\n        # Interface with manipulation stack\n        pass\n\n    def _place_down(self, params: Dict):\n        \"\"\"Primitive place down action\"\"\"\n        # Interface with manipulation stack\n        pass\n\n    def _detect_object(self, params: Dict):\n        \"\"\"Primitive object detection\"\"\"\n        # Interface with perception stack\n        pass\n"})}),"\n",(0,i.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,i.jsx)(e.h3,{id:"effective-prompts-for-robot-planning",children:"Effective Prompts for Robot Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RobotPlanningPrompts:\n    @staticmethod\n    def create_task_planning_prompt(command: str, robot_state: Dict, environment: Dict) -> str:\n        """Create effective prompt for task planning"""\n        return f"""\n        You are an expert robot task planner. Your job is to break down human commands into specific, executable robot actions.\n\n        ROBOT CAPABILITIES:\n        - Navigation: Can move to specific coordinates (x, y, z) with orientation\n        - Manipulation: Can pick up, place, grasp objects\n        - Perception: Can detect and recognize objects, people, locations\n        - Communication: Can speak, listen, display information\n\n        CURRENT STATE:\n        Position: ({robot_state.get(\'position\', {}).get(\'x\', 0)}, {robot_state.get(\'position\', {}).get(\'y\', 0)})\n        Battery: {robot_state.get(\'battery_level\', 100)}%\n        Connected: True\n        Available actions: {\', \'.join(robot_state.get(\'capabilities\', []))}\n\n        ENVIRONMENT:\n        Known locations: {list(environment.get(\'known_locations\', {}).keys())}\n        Recent detections: {environment.get(\'recently_detected_objects\', [])}\n        Obstacles: {environment.get(\'obstacles\', [])}\n\n        COMMAND: "{command}"\n\n        INSTRUCTIONS:\n        1. Analyze the command for specific goals\n        2. Consider the robot\'s current state and environment\n        3. Break down into sequential, executable actions\n        4. Include error handling and validation steps\n        5. Consider safety and feasibility\n\n        OUTPUT FORMAT:\n        {{\n            "analysis": "Brief analysis of the command",\n            "plan": [\n                {{\n                    "step": 1,\n                    "action": "action_type",\n                    "parameters": {{"param1": "value1"}},\n                    "description": "What this step does",\n                    "expected_outcome": "How to verify success",\n                    "error_handling": "What to do if this fails"\n                }}\n            ],\n            "estimated_duration": "Time estimate in seconds",\n            "resources_needed": ["list", "of", "required", "resources"],\n            "potential_risks": ["risk1", "risk2"]\n        }}\n\n        Respond ONLY in valid JSON format:\n        """\n\n    @staticmethod\n    def create_error_recovery_prompt(error: str, attempted_action: Dict, robot_state: Dict) -> str:\n        """Create prompt for error recovery"""\n        return f"""\n        The robot encountered an error during task execution:\n\n        ERROR: {error}\n        ATTEMPTED ACTION: {attempted_action}\n        CURRENT STATE: {robot_state}\n\n        PROVIDE RECOVERY PLAN:\n        1. Diagnose the likely cause of the error\n        2. Suggest immediate recovery actions\n        3. Propose alternative approaches\n        4. Indicate when to abort and ask for human help\n\n        FORMAT: {{\n            "diagnosis": "Likely cause of error",\n            "immediate_recovery": ["action1", "action2"],\n            "alternative_approach": "Different way to achieve goal",\n            "abort_conditions": ["conditions", "to", "stop", "trying"],\n            "human_help_needed": "When to ask for assistance"\n        }}\n        """\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"plan-validation-system",children:"Plan Validation System"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class PlanValidator:\n    def __init__(self):\n        self.safety_rules = [\n            self._check_navigation_safety,\n            self._check_manipulation_safety,\n            self._check_resource_availability,\n            self._check_feasibility\n        ]\n\n    def validate_plan(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        """Validate plan for safety and feasibility"""\n        validation_result = {\n            "is_valid": True,\n            "issues": [],\n            "warnings": [],\n            "suggestions": []\n        }\n\n        for rule in self.safety_rules:\n            result = rule(plan, robot_state, environment)\n            if not result["valid"]:\n                validation_result["is_valid"] = False\n                validation_result["issues"].extend(result["issues"])\n            validation_result["warnings"].extend(result["warnings"])\n            validation_result["suggestions"].extend(result["suggestions"])\n\n        return validation_result\n\n    def _check_navigation_safety(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        """Check if navigation plan is safe"""\n        issues = []\n        warnings = []\n        suggestions = []\n\n        for step in plan.get("plan", []):\n            if step.get("action") == "navigate_to":\n                target = step.get("parameters", {})\n                x, y = target.get("x"), target.get("y")\n\n                # Check if target is in known safe areas\n                if not self._is_safe_navigation_target(x, y, environment):\n                    issues.append(f"Navigation to ({x}, {y}) may be unsafe")\n\n        return {\n            "valid": len(issues) == 0,\n            "issues": issues,\n            "warnings": warnings,\n            "suggestions": suggestions\n        }\n\n    def _is_safe_navigation_target(self, x: float, y: float, environment: Dict) -> bool:\n        """Check if navigation target is safe"""\n        # Check against known obstacles\n        obstacles = environment.get("obstacles", [])\n        for obstacle in obstacles:\n            obs_x, obs_y = obstacle.get("x", 0), obstacle.get("y", 0)\n            distance = ((x - obs_x)**2 + (y - obs_y)**2)**0.5\n            if distance < 0.5:  # 50cm safety margin\n                return False\n\n        # Check if within map boundaries\n        # (would check against map in real implementation)\n\n        return True\n\n    def _check_manipulation_safety(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        """Check if manipulation plan is safe"""\n        # Implementation would check:\n        # - Reachability\n        # - Object properties (weight, fragility)\n        # - Collision avoidance\n        pass\n\n    def _check_resource_availability(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        """Check if required resources are available"""\n        # Implementation would check:\n        # - Battery level for planned duration\n        # - Required tools/accessories\n        # - Available time before deadlines\n        pass\n\n    def _check_feasibility(self, plan: Dict, robot_state: Dict, environment: Dict) -> Dict:\n        """Check if plan is technically feasible"""\n        # Implementation would check:\n        # - Robot capabilities vs required actions\n        # - Environmental constraints\n        # - Time feasibility\n        pass\n'})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"caching-and-optimization",children:"Caching and Optimization"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import functools\nimport hashlib\nimport time\nfrom typing import Callable, Any\n\nclass OptimizedLLMInterface:\n    def __init__(self, api_key: str, cache_size: int = 1000):\n        openai.api_key = api_key\n        self.cache = {}\n        self.cache_order = []  # For LRU eviction\n        self.cache_size = cache_size\n\n    def cached_plan_task(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        """Plan task with caching to reduce API calls"""\n        # Create cache key from inputs\n        cache_key = self._create_cache_key(command, robot_state, environment)\n\n        # Check cache first\n        if cache_key in self.cache:\n            self.cache_order.remove(cache_key)  # Remove from current position\n            self.cache_order.append(cache_key)  # Move to end (most recent)\n            return self.cache[cache_key]\n\n        # Generate new plan\n        plan = self._generate_plan(command, robot_state, environment)\n\n        # Add to cache\n        self._add_to_cache(cache_key, plan)\n\n        return plan\n\n    def _create_cache_key(self, command: str, robot_state: Dict, environment: Dict) -> str:\n        """Create unique cache key"""\n        combined = f"{command}|{hash(str(sorted(robot_state.items())))}|{hash(str(sorted(environment.items())))}"\n        return hashlib.md5(combined.encode()).hexdigest()\n\n    def _add_to_cache(self, key: str, value: Dict):\n        """Add to cache with LRU eviction"""\n        if len(self.cache) >= self.cache_size:\n            # Remove oldest entry\n            oldest_key = self.cache_order.pop(0)\n            del self.cache[oldest_key]\n\n        self.cache[key] = value\n        self.cache_order.append(key)\n\n    def _generate_plan(self, command: str, robot_state: Dict, environment: Dict) -> Dict:\n        """Generate plan using LLM (this is the actual API call)"""\n        # Implementation of the actual LLM call\n        # This would be the method from RobotLLMInterface\n        pass\n'})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(e.h3,{id:"1-api-rate-limits",children:"1. API Rate Limits"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": OpenAI API rate limiting\n",(0,i.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement request queuing"}),"\n",(0,i.jsx)(e.li,{children:"Use caching for repeated commands"}),"\n",(0,i.jsx)(e.li,{children:"Monitor token usage"}),"\n",(0,i.jsx)(e.li,{children:"Consider higher-tier plans for production"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"2-hallucination-issues",children:"2. Hallucination Issues"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": LLM generates invalid or impossible actions\n",(0,i.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use function calling to constrain outputs"}),"\n",(0,i.jsx)(e.li,{children:"Implement validation layers"}),"\n",(0,i.jsx)(e.li,{children:"Use lower temperatures (0.1-0.3)"}),"\n",(0,i.jsx)(e.li,{children:"Provide clear examples in system prompts"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"3-context-window-limitations",children:"3. Context Window Limitations"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": Large robot states exceed context window\n",(0,i.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Summarize robot state before sending"}),"\n",(0,i.jsx)(e.li,{children:"Use retrieval-augmented generation (RAG)"}),"\n",(0,i.jsx)(e.li,{children:"Implement state compression"}),"\n",(0,i.jsx)(e.li,{children:"Use streaming for large inputs"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"4-integration-latency",children:"4. Integration Latency"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Problem"}),": High latency in robot response\n",(0,i.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Use smaller, faster models for simple tasks"}),"\n",(0,i.jsx)(e.li,{children:"Implement asynchronous processing"}),"\n",(0,i.jsx)(e.li,{children:"Pre-plan common tasks"}),"\n",(0,i.jsx)(e.li,{children:"Use local models for basic commands"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(e.h3,{id:"1-error-handling",children:"1. Error Handling"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Always implement fallback strategies"}),"\n",(0,i.jsx)(e.li,{children:"Log LLM interactions for debugging"}),"\n",(0,i.jsx)(e.li,{children:"Provide human-in-the-loop options"}),"\n",(0,i.jsx)(e.li,{children:"Validate outputs before execution"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"2-security",children:"2. Security"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Secure API keys properly"}),"\n",(0,i.jsx)(e.li,{children:"Validate all inputs from LLM"}),"\n",(0,i.jsx)(e.li,{children:"Implement access controls"}),"\n",(0,i.jsx)(e.li,{children:"Monitor for prompt injection"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"3-performance",children:"3. Performance"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Cache frequent requests"}),"\n",(0,i.jsx)(e.li,{children:"Use appropriate model sizes"}),"\n",(0,i.jsx)(e.li,{children:"Implement request batching"}),"\n",(0,i.jsx)(e.li,{children:"Monitor and optimize token usage"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"4-testing",children:"4. Testing"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Test with edge cases"}),"\n",(0,i.jsx)(e.li,{children:"Validate safety constraints"}),"\n",(0,i.jsx)(e.li,{children:"Test error recovery paths"}),"\n",(0,i.jsx)(e.li,{children:"Performance benchmarking"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"exercise",children:"Exercise"}),"\n",(0,i.jsx)(e.p,{children:"Create a complete LLM integration for your humanoid robot that includes:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"A robust LLM interface with error handling"}),"\n",(0,i.jsx)(e.li,{children:"Function calling integration for direct robot API access"}),"\n",(0,i.jsx)(e.li,{children:"A validation system to ensure plan safety"}),"\n",(0,i.jsx)(e.li,{children:"Performance optimization techniques"}),"\n",(0,i.jsx)(e.li,{children:"ROS 2 integration for real-time command processing"}),"\n",(0,i.jsx)(e.li,{children:"A hierarchical planning system for complex tasks"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Test your system with various natural language commands and evaluate its ability to generate safe, executable robot plans."})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);