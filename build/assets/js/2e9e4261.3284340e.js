"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[859],{4305:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-3-ai-perception/navigation-planning-obstacle-avoidance","title":"Navigation Planning and Obstacle Avoidance Examples","description":"This section covers navigation planning and obstacle avoidance techniques for humanoid robots using AI-powered perception and navigation systems. We\'ll explore how to integrate perception data with navigation planning for safe and efficient robot movement.","source":"@site/docs/module-3-ai-perception/navigation-planning-obstacle-avoidance.md","sourceDirName":"module-3-ai-perception","slug":"/module-3-ai-perception/navigation-planning-obstacle-avoidance","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/navigation-planning-obstacle-avoidance","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Object Detection and Localization Examples","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/object-detection-localization"},"next":{"title":"Nav2 for Humanoid Locomotion","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-perception/nav2-locomotion"}}');var s=t(4848),a=t(8453);const i={},r="Navigation Planning and Obstacle Avoidance Examples",l={},c=[{value:"Introduction to Navigation Planning",id:"introduction-to-navigation-planning",level:2},{value:"Key Components of Navigation Planning",id:"key-components-of-navigation-planning",level:3},{value:"Global Path Planning",id:"global-path-planning",level:2},{value:"A* Algorithm Implementation",id:"a-algorithm-implementation",level:3},{value:"Nav2 Global Planner Integration",id:"nav2-global-planner-integration",level:3},{value:"Local Path Planning and Obstacle Avoidance",id:"local-path-planning-and-obstacle-avoidance",level:2},{value:"Dynamic Window Approach (DWA) for Humanoid Robots",id:"dynamic-window-approach-dwa-for-humanoid-robots",level:3},{value:"Obstacle Detection and Avoidance Integration",id:"obstacle-detection-and-avoidance-integration",level:2},{value:"Perception-Based Obstacle Detection",id:"perception-based-obstacle-detection",level:3},{value:"Humanoid-Specific Navigation Behaviors",id:"humanoid-specific-navigation-behaviors",level:2},{value:"Step Planning for Bipedal Locomotion",id:"step-planning-for-bipedal-locomotion",level:3},{value:"Isaac Sim Navigation Integration",id:"isaac-sim-navigation-integration",level:2},{value:"Isaac Sim Navigation Controller",id:"isaac-sim-navigation-controller",level:3},{value:"Recovery Behaviors",id:"recovery-behaviors",level:2},{value:"Navigation Recovery Behaviors",id:"navigation-recovery-behaviors",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Navigation Performance Metrics",id:"navigation-performance-metrics",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Multi-Layered Safety System",id:"1-multi-layered-safety-system",level:3},{value:"2. Parameter Tuning",id:"2-parameter-tuning",level:3},{value:"3. Humanoid-Specific Considerations",id:"3-humanoid-specific-considerations",level:3},{value:"Exercise",id:"exercise",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"navigation-planning-and-obstacle-avoidance-examples",children:"Navigation Planning and Obstacle Avoidance Examples"})}),"\n",(0,s.jsx)(e.p,{children:"This section covers navigation planning and obstacle avoidance techniques for humanoid robots using AI-powered perception and navigation systems. We'll explore how to integrate perception data with navigation planning for safe and efficient robot movement."}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-navigation-planning",children:"Introduction to Navigation Planning"}),"\n",(0,s.jsx)(e.p,{children:"Navigation planning involves determining a safe and efficient path for a robot to reach its goal while avoiding obstacles. For humanoid robots, this includes additional complexities like balance maintenance, step planning, and dynamic stability."}),"\n",(0,s.jsx)(e.h3,{id:"key-components-of-navigation-planning",children:"Key Components of Navigation Planning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Global Path Planning"}),": Long-term path from start to goal"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Local Path Planning"}),": Short-term path adjustment based on obstacles"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Trajectory Generation"}),": Smooth motion trajectories"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Obstacle Avoidance"}),": Real-time obstacle detection and avoidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Recovery Behaviors"}),": Handling navigation failures"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"global-path-planning",children:"Global Path Planning"}),"\n",(0,s.jsx)(e.h3,{id:"a-algorithm-implementation",children:"A* Algorithm Implementation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:"#include <vector>\n#include <queue>\n#include <cmath>\n#include <algorithm>\n\nstruct Point {\n    int x, y;\n    double g_cost = 0;  // Cost from start\n    double h_cost = 0;  // Heuristic cost to goal\n    double f_cost = 0;  // g + h\n    Point* parent = nullptr;\n\n    bool operator>(const Point& other) const {\n        return f_cost > other.f_cost;\n    }\n};\n\nclass GlobalPlanner {\npublic:\n    GlobalPlanner(const std::vector<std::vector<int>>& grid) : grid_(grid) {}\n\n    std::vector<Point> planPath(const Point& start, const Point& goal) {\n        // Initialize open and closed sets\n        std::priority_queue<Point, std::vector<Point>, std::greater<Point>> open_set;\n        std::vector<std::vector<bool>> closed_set(grid_.size(),\n                                                 std::vector<bool>(grid_[0].size(), false));\n\n        // Add start point to open set\n        Point start_copy = start;\n        start_copy.h_cost = heuristic(start_copy, goal);\n        start_copy.f_cost = start_copy.h_cost;\n        open_set.push(start_copy);\n\n        while (!open_set.empty()) {\n            Point current = open_set.top();\n            open_set.pop();\n\n            // Check if we reached the goal\n            if (current.x == goal.x && current.y == goal.y) {\n                return reconstructPath(current);\n            }\n\n            // Mark as visited\n            closed_set[current.x][current.y] = true;\n\n            // Check neighbors\n            std::vector<Point> neighbors = getNeighbors(current);\n            for (auto& neighbor : neighbors) {\n                if (neighbor.x < 0 || neighbor.x >= grid_.size() ||\n                    neighbor.y < 0 || neighbor.y >= grid_[0].size() ||\n                    grid_[neighbor.x][neighbor.y] == 1 ||  // Obstacle\n                    closed_set[neighbor.x][neighbor.y]) {\n                    continue;\n                }\n\n                double tentative_g = current.g_cost + distance(current, neighbor);\n\n                if (tentative_g < neighbor.g_cost || neighbor.parent == nullptr) {\n                    neighbor.parent = new Point(current);\n                    neighbor.g_cost = tentative_g;\n                    neighbor.h_cost = heuristic(neighbor, goal);\n                    neighbor.f_cost = neighbor.g_cost + neighbor.h_cost;\n\n                    open_set.push(neighbor);\n                }\n            }\n        }\n\n        // No path found\n        return {};\n    }\n\nprivate:\n    std::vector<std::vector<int>> grid_;\n\n    double heuristic(const Point& a, const Point& b) {\n        // Manhattan distance heuristic\n        return std::abs(a.x - b.x) + std::abs(a.y - b.y);\n    }\n\n    double distance(const Point& a, const Point& b) {\n        // Euclidean distance\n        return std::sqrt(std::pow(a.x - b.x, 2) + std::pow(a.y - b.y, 2));\n    }\n\n    std::vector<Point> getNeighbors(const Point& point) {\n        std::vector<Point> neighbors;\n        // 8-directional movement\n        int dx[] = {-1, -1, -1, 0, 0, 1, 1, 1};\n        int dy[] = {-1, 0, 1, -1, 1, -1, 0, 1};\n\n        for (int i = 0; i < 8; i++) {\n            Point neighbor;\n            neighbor.x = point.x + dx[i];\n            neighbor.y = point.y + dy[i];\n            neighbors.push_back(neighbor);\n        }\n\n        return neighbors;\n    }\n\n    std::vector<Point> reconstructPath(Point current) {\n        std::vector<Point> path;\n        while (current.parent != nullptr) {\n            path.push_back(current);\n            current = *(current.parent);\n        }\n        path.push_back(current);  // Add start point\n        std::reverse(path.begin(), path.end());\n        return path;\n    }\n};\n"})}),"\n",(0,s.jsx)(e.h3,{id:"nav2-global-planner-integration",children:"Nav2 Global Planner Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:'#include <nav2_core/global_planner.hpp>\n#include <nav2_costmap_2d/costmap_2d_ros.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <pluginlib/class_list_macros.hpp>\n\nclass HumanoidGlobalPlanner : public nav2_core::GlobalPlanner\n{\npublic:\n    HumanoidGlobalPlanner() = default;\n    ~HumanoidGlobalPlanner() override = default;\n\n    void configure(\n        const rclcpp_lifecycle::LifecycleNode::WeakPtr & parent,\n        std::string name,\n        const std::shared_ptr<tf2_ros::Buffer> & tf,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & costmap_ros) override\n    {\n        node_ = parent.lock();\n        name_ = name;\n        tf_ = tf;\n        costmap_ros_ = costmap_ros;\n        costmap_ = costmap_ros_->getCostmap();\n\n        RCLCPP_INFO(node_->get_logger(), "Configured HumanoidGlobalPlanner");\n\n        // Initialize humanoid-specific parameters\n        step_height_threshold_ = node_->declare_parameter(name_ + ".step_height_threshold", 0.2);\n        max_slope_angle_ = node_->declare_parameter(name_ + ".max_slope_angle", 15.0);\n    }\n\n    void cleanup() override\n    {\n        RCLCPP_INFO(node_->get_logger(), "Cleaning up HumanoidGlobalPlanner");\n    }\n\n    void activate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), "Activating HumanoidGlobalPlanner");\n    }\n\n    void deactivate() override\n    {\n        RCLCPP_INFO(node_->get_logger(), "Deactivating HumanoidGlobalPlanner");\n    }\n\n    nav_msgs::msg::Path createPlan(\n        const geometry_msgs::msg::PoseStamped & start,\n        const geometry_msgs::msg::PoseStamped & goal) override\n    {\n        nav_msgs::msg::Path path;\n\n        // Check if start and goal are valid\n        if (!isStartValid(start) || !isGoalValid(goal)) {\n            RCLCPP_WARN(node_->get_logger(), "Invalid start or goal position");\n            return path;\n        }\n\n        // Convert poses to grid coordinates\n        unsigned int start_x, start_y, goal_x, goal_y;\n        if (!costmap_->worldToMap(start.pose.position.x, start.pose.position.y, start_x, start_y) ||\n            !costmap_->worldToMap(goal.pose.position.x, goal.pose.position.y, goal_x, goal_y)) {\n            RCLCPP_WARN(node_->get_logger(), "Start or goal position not in costmap");\n            return path;\n        }\n\n        // Plan path using A* with humanoid constraints\n        auto path_points = planHumanoidPath(start_x, start_y, goal_x, goal_y);\n\n        // Convert to ROS message\n        path = convertToPathMsg(path_points, start.header);\n\n        // Apply path smoothing for humanoid locomotion\n        smoothPath(path);\n\n        return path;\n    }\n\nprivate:\n    bool isStartValid(const geometry_msgs::msg::PoseStamped & start)\n    {\n        unsigned int mx, my;\n        if (!costmap_->worldToMap(start.pose.position.x, start.pose.position.y, mx, my)) {\n            return false;\n        }\n\n        // Check if start is in free space\n        return costmap_->getCost(mx, my) < nav2_costmap_2d::FREE_SPACE;\n    }\n\n    bool isGoalValid(const geometry_msgs::msg::PoseStamped & goal)\n    {\n        unsigned int mx, my;\n        if (!costmap_->worldToMap(goal.pose.position.x, goal.pose.position.y, mx, my)) {\n            return false;\n        }\n\n        // Check if goal is in free space and not too close to obstacles\n        unsigned char cost = costmap_->getCost(mx, my);\n        return cost < nav2_costmap_2d::INSCRIBED_INFLATED_OBSTACLE;\n    }\n\n    std::vector<Point> planHumanoidPath(unsigned int start_x, unsigned int start_y,\n                                       unsigned int goal_x, unsigned int goal_y)\n    {\n        // Create grid representation of costmap\n        std::vector<std::vector<int>> grid(costmap_->getSizeInCellsY(),\n                                          std::vector<int>(costmap_->getSizeInCellsX()));\n\n        // Populate grid with costmap data\n        for (unsigned int y = 0; y < costmap_->getSizeInCellsY(); ++y) {\n            for (unsigned int x = 0; x < costmap_->getSizeInCellsX(); ++x) {\n                unsigned char cost = costmap_->getCost(x, y);\n                grid[y][x] = (cost >= nav2_costmap_2d::LETHAL_OBSTACLE) ? 1 : 0;\n            }\n        }\n\n        // Create start and goal points\n        Point start_point, goal_point;\n        start_point.x = start_x;\n        start_point.y = start_y;\n        goal_point.x = goal_x;\n        goal_point.y = goal_y;\n\n        // Run A* planning\n        GlobalPlanner planner(grid);\n        return planner.planPath(start_point, goal_point);\n    }\n\n    nav_msgs::msg::Path convertToPathMsg(const std::vector<Point>& points,\n                                        const std_msgs::msg::Header& header)\n    {\n        nav_msgs::msg::Path path;\n        path.header = header;\n\n        for (const auto& point : points) {\n            geometry_msgs::msg::PoseStamped pose;\n            pose.header = header;\n\n            // Convert grid coordinates to world coordinates\n            double x, y;\n            costmap_->mapToWorld(point.x, point.y, x, y);\n            pose.pose.position.x = x;\n            pose.pose.position.y = y;\n            pose.pose.position.z = 0.0;\n\n            // Set orientation to face next point\n            if (&point != &points.back()) {  // Not the last point\n                auto next_it = std::next(&point);\n                if (next_it != &points.back() + 1) {\n                    double dx = points[&point - &points[0] + 1].x - point.x;\n                    double dy = points[&point - &points[0] + 1].y - point.y;\n\n                    double yaw = atan2(dy, dx);\n                    tf2::Quaternion q;\n                    q.setRPY(0, 0, yaw);\n                    pose.pose.orientation.x = q.x();\n                    pose.pose.orientation.y = q.y();\n                    pose.pose.orientation.z = q.z();\n                    pose.pose.orientation.w = q.w();\n                }\n            }\n\n            path.poses.push_back(pose);\n        }\n\n        return path;\n    }\n\n    void smoothPath(nav_msgs::msg::Path& path)\n    {\n        // Apply path smoothing for smoother humanoid locomotion\n        // This could implement techniques like:\n        // - Dubins curves for curvature-constrained paths\n        // - B-spline smoothing\n        // - Gradient descent-based smoothing\n\n        // Simple smoothing by averaging adjacent points\n        if (path.poses.size() < 3) return;\n\n        for (size_t i = 1; i < path.poses.size() - 1; ++i) {\n            auto& curr = path.poses[i].pose.position;\n            auto prev = path.poses[i-1].pose.position;\n            auto next = path.poses[i+1].pose.position;\n\n            // Weighted average: 25% prev, 50% current, 25% next\n            curr.x = 0.25 * prev.x + 0.5 * curr.x + 0.25 * next.x;\n            curr.y = 0.25 * prev.y + 0.5 * curr.y + 0.25 * next.y;\n        }\n    }\n\n    rclcpp_lifecycle::LifecycleNode::SharedPtr node_;\n    std::string name_;\n    std::shared_ptr<tf2_ros::Buffer> tf_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> costmap_ros_;\n    nav2_costmap_2d::Costmap2D* costmap_;\n\n    // Humanoid-specific parameters\n    double step_height_threshold_;\n    double max_slope_angle_;\n};\n\nPLUGINLIB_EXPORT_CLASS(HumanoidGlobalPlanner, nav2_core::GlobalPlanner)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"local-path-planning-and-obstacle-avoidance",children:"Local Path Planning and Obstacle Avoidance"}),"\n",(0,s.jsx)(e.h3,{id:"dynamic-window-approach-dwa-for-humanoid-robots",children:"Dynamic Window Approach (DWA) for Humanoid Robots"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:"#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass HumanoidLocalPlanner {\npublic:\n    HumanoidLocalPlanner() {\n        // Initialize humanoid-specific parameters\n        max_vel_x_ = 0.5;      // Max forward velocity (m/s)\n        min_vel_x_ = 0.05;     // Min forward velocity (m/s)\n        max_vel_th_ = 0.5;     // Max angular velocity (rad/s)\n        min_vel_th_ = -0.5;    // Min angular velocity (rad/s)\n\n        max_acc_x_ = 0.5;      // Max acceleration (m/s\xb2)\n        max_acc_th_ = 1.0;     // Max angular acceleration (rad/s\xb2)\n\n        vtheta_samp_ = 20;     // Number of angular velocity samples\n        vx_samp_ = 10;         // Number of forward velocity samples\n\n        sim_time_ = 2.0;       // Simulation time horizon (seconds)\n        sim_granularity_ = 0.05; // Simulation granularity (meters)\n\n        // Humanoid-specific parameters\n        step_frequency_ = 1.25;  // Steps per second\n        balance_margin_ = 0.1;   // Balance safety margin\n    }\n\n    geometry_msgs::msg::Twist calculateVelocityCommands(\n        const geometry_msgs::msg::PoseStamped& robot_pose,\n        const geometry_msgs::msg::PoseStamped& goal_pose,\n        const geometry_msgs::msg::Twist& current_vel,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        // Get possible velocities\n        auto vel_samples = getVelocitySamples(current_vel);\n\n        double best_score = -std::numeric_limits<double>::infinity();\n        geometry_msgs::msg::Twist best_vel;\n\n        for (const auto& vel : vel_samples) {\n            // Simulate trajectory for this velocity\n            auto trajectory = simulateTrajectory(robot_pose.pose, vel, current_vel);\n\n            // Evaluate trajectory\n            double heading_score = calculateHeadingScore(trajectory, goal_pose);\n            double dist_score = calculateDistScore(trajectory);\n            double obs_score = calculateObstacleScore(trajectory, scan_data);\n\n            // Weighted combination of scores\n            double score = 0.3 * heading_score + 0.2 * dist_score + 0.5 * obs_score;\n\n            if (score > best_score) {\n                best_score = score;\n                best_vel = vel;\n            }\n        }\n\n        return best_vel;\n    }\n\nprivate:\n    struct Trajectory {\n        std::vector<geometry_msgs::msg::Pose> poses;\n        geometry_msgs::msg::Twist final_vel;\n    };\n\n    std::vector<geometry_msgs::msg::Twist> getVelocitySamples(\n        const geometry_msgs::msg::Twist& current_vel)\n    {\n        std::vector<geometry_msgs::msg::Twist> samples;\n\n        // Calculate velocity windows based on current velocity and accelerations\n        double dt = 0.1;  // Time step for sampling\n        double max_delta_vx = max_acc_x_ * dt;\n        double max_delta_vth = max_acc_th_ * dt;\n\n        double min_vx = std::max(min_vel_x_, current_vel.linear.x - max_delta_vx);\n        double max_vx = std::min(max_vel_x_, current_vel.linear.x + max_delta_vx);\n        double min_vth = std::max(min_vel_th_, current_vel.angular.z - max_delta_vth);\n        double max_vth = std::min(max_vel_th_, current_vel.angular.z + max_delta_vth);\n\n        // Sample velocities\n        double dvx = (max_vx - min_vx) / vx_samp_;\n        double dvth = (max_vth - min_vth) / vtheta_samp_;\n\n        for (int i = 0; i <= vx_samp_; ++i) {\n            for (int j = 0; j <= vtheta_samp_; ++j) {\n                geometry_msgs::msg::Twist vel;\n                vel.linear.x = min_vx + i * dvx;\n                vel.angular.z = min_vth + j * dvth;\n\n                // Humanoid-specific constraints\n                if (isValidHumanoidVelocity(vel)) {\n                    samples.push_back(vel);\n                }\n            }\n        }\n\n        return samples;\n    }\n\n    bool isValidHumanoidVelocity(const geometry_msgs::msg::Twist& vel)\n    {\n        // Check humanoid-specific constraints\n        // For example, ensure velocity is within safe walking parameters\n        double speed = sqrt(vel.linear.x * vel.linear.x + vel.linear.y * vel.linear.y);\n\n        // Simple check: speed should be within walking range\n        if (speed > max_vel_x_ * 1.5) return false;  // Too fast for stable walking\n\n        return true;\n    }\n\n    Trajectory simulateTrajectory(\n        const geometry_msgs::msg::Pose& start_pose,\n        const geometry_msgs::msg::Twist& target_vel,\n        const geometry_msgs::msg::Twist& current_vel)\n    {\n        Trajectory traj;\n        geometry_msgs::msg::Pose current_pose = start_pose;\n        geometry_msgs::msg::Twist current_vel_local = current_vel;\n\n        double dt = sim_granularity_ / std::max(std::abs(target_vel.linear.x), 0.1);\n        int steps = static_cast<int>(sim_time_ / dt);\n\n        for (int i = 0; i < steps; ++i) {\n            // Update pose based on current velocity\n            double yaw = tf2::getYaw(current_pose.orientation);\n\n            // Update position\n            current_pose.position.x += current_vel_local.linear.x * cos(yaw) * dt;\n            current_pose.position.y += current_vel_local.linear.x * sin(yaw) * dt;\n            current_pose.position.z += current_vel_local.linear.z * dt;  // For 3D movement\n\n            // Update orientation\n            yaw += current_vel_local.angular.z * dt;\n            tf2::Quaternion quat;\n            quat.setRPY(0, 0, yaw);\n            current_pose.orientation = tf2::toMsg(quat);\n\n            // Update velocity towards target (with acceleration constraints)\n            double ax = std::min(max_acc_x_,\n                                std::abs(target_vel.linear.x - current_vel_local.linear.x) / dt);\n            double ath = std::min(max_acc_th_,\n                                 std::abs(target_vel.angular.z - current_vel_local.angular.z) / dt);\n\n            if (target_vel.linear.x > current_vel_local.linear.x) {\n                current_vel_local.linear.x = std::min(target_vel.linear.x,\n                                                    current_vel_local.linear.x + ax * dt);\n            } else {\n                current_vel_local.linear.x = std::max(target_vel.linear.x,\n                                                    current_vel_local.linear.x - ax * dt);\n            }\n\n            if (target_vel.angular.z > current_vel_local.angular.z) {\n                current_vel_local.angular.z = std::min(target_vel.angular.z,\n                                                     current_vel_local.angular.z + ath * dt);\n            } else {\n                current_vel_local.angular.z = std::max(target_vel.angular.z,\n                                                     current_vel_local.angular.z - ath * dt);\n            }\n\n            traj.poses.push_back(current_pose);\n        }\n\n        traj.final_vel = current_vel_local;\n        return traj;\n    }\n\n    double calculateHeadingScore(\n        const Trajectory& traj,\n        const geometry_msgs::msg::PoseStamped& goal_pose)\n    {\n        if (traj.poses.empty()) return 0.0;\n\n        const auto& final_pose = traj.poses.back();\n\n        // Calculate angle to goal\n        double goal_x = goal_pose.pose.position.x;\n        double goal_y = goal_pose.pose.position.y;\n        double robot_x = final_pose.position.x;\n        double robot_y = final_pose.position.y;\n\n        double angle_to_goal = atan2(goal_y - robot_y, goal_x - robot_x);\n        double robot_yaw = tf2::getYaw(final_pose.orientation);\n\n        // Normalize angles\n        double angle_diff = angle_to_goal - robot_yaw;\n        angle_diff = std::atan2(std::sin(angle_diff), std::cos(angle_diff));\n\n        // Score based on how well the robot is oriented toward the goal\n        return 1.0 - std::abs(angle_diff) / M_PI;  // Higher score for smaller angle difference\n    }\n\n    double calculateDistScore(const Trajectory& traj)\n    {\n        // Score based on how far the trajectory takes the robot\n        if (traj.poses.size() < 2) return 0.0;\n\n        const auto& start = traj.poses.front();\n        const auto& end = traj.poses.back();\n\n        double dist = sqrt(pow(end.position.x - start.position.x, 2) +\n                          pow(end.position.y - start.position.y, 2));\n\n        // Normalize by simulation time\n        return dist / sim_time_;\n    }\n\n    double calculateObstacleScore(\n        const Trajectory& traj,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        double score = 0.0;\n        double min_dist_to_obstacle = std::numeric_limits<double>::infinity();\n\n        for (const auto& pose : traj.poses) {\n            // Check distance to nearest obstacle at this pose\n            double dist = getMinDistanceToObstacle(pose, scan_data);\n            min_dist_to_obstacle = std::min(min_dist_to_obstacle, dist);\n\n            if (dist < 0.2) {  // Very close to obstacle\n                return -std::numeric_limits<double>::infinity();  // Invalid trajectory\n            }\n        }\n\n        // Score based on minimum distance to obstacles\n        // Prefer trajectories that stay farther from obstacles\n        return min_dist_to_obstacle;\n    }\n\n    double getMinDistanceToObstacle(\n        const geometry_msgs::msg::Pose& pose,\n        const sensor_msgs::msg::LaserScan& scan_data)\n    {\n        // Convert laser scan points to robot coordinates and check for obstacles\n        double min_dist = std::numeric_limits<double>::infinity();\n        double robot_yaw = tf2::getYaw(pose.orientation);\n\n        for (size_t i = 0; i < scan_data.ranges.size(); ++i) {\n            if (scan_data.ranges[i] < scan_data.range_min ||\n                scan_data.ranges[i] > scan_data.range_max) {\n                continue;  // Invalid range\n            }\n\n            // Convert polar coordinates to Cartesian in laser frame\n            double angle = scan_data.angle_min + i * scan_data.angle_increment;\n            double x_laser = scan_data.ranges[i] * cos(angle);\n            double y_laser = scan_data.ranges[i] * sin(angle);\n\n            // Transform to robot base frame\n            double cos_yaw = cos(robot_yaw);\n            double sin_yaw = sin(robot_yaw);\n\n            double x_robot = x_laser * cos_yaw - y_laser * sin_yaw + pose.position.x;\n            double y_robot = x_laser * sin_yaw + y_laser * cos_yaw + pose.position.y;\n\n            // Calculate distance from robot center to this point\n            double dist = sqrt(pow(x_robot - pose.position.x, 2) +\n                              pow(y_robot - pose.position.y, 2));\n\n            min_dist = std::min(min_dist, dist);\n        }\n\n        return min_dist;\n    }\n\n    // Parameters\n    double max_vel_x_, min_vel_x_, max_vel_th_, min_vel_th_;\n    double max_acc_x_, max_acc_th_;\n    int vtheta_samp_, vx_samp_;\n    double sim_time_, sim_granularity_;\n    double step_frequency_, balance_margin_;\n};\n"})}),"\n",(0,s.jsx)(e.h2,{id:"obstacle-detection-and-avoidance-integration",children:"Obstacle Detection and Avoidance Integration"}),"\n",(0,s.jsx)(e.h3,{id:"perception-based-obstacle-detection",children:"Perception-Based Obstacle Detection"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/point_stamped.hpp>\n#include <isaac_ros_detectnet_interfaces/msg/detection_array.hpp>\n#include <pcl/point_cloud.h>\n#include <pcl/point_types.h>\n#include <pcl_conversions/pcl_conversions.h>\n\nclass PerceptionObstacleDetector : public rclcpp::Node\n{\npublic:\n    PerceptionObstacleDetector() : Node("perception_obstacle_detector")\n    {\n        // Subscribe to various sensor inputs\n        laser_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\n            "scan", 10,\n            std::bind(&PerceptionObstacleDetector::laserCallback, this, std::placeholders::_1)\n        );\n\n        detection_sub_ = this->create_subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>(\n            "detections", 10,\n            std::bind(&PerceptionObstacleDetector::detectionCallback, this, std::placeholders::_1)\n        );\n\n        pointcloud_sub_ = this->create_subscription<sensor_msgs::msg::PointCloud2>(\n            "points", 10,\n            std::bind(&PerceptionObstacleDetector::pointcloudCallback, this, std::placeholders::_1)\n        );\n\n        // Publisher for combined obstacle information\n        obstacle_pub_ = this->create_publisher<geometry_msgs::msg::PolygonStamped>(\n            "obstacle_polygon", 10\n        );\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::msg::LaserScan::SharedPtr scan_msg)\n    {\n        // Process LIDAR data to detect obstacles\n        std::vector<geometry_msgs::msg::Point32> laser_obstacles = processLaserScan(*scan_msg);\n\n        // Update obstacle map\n        updateObstacleMap(laser_obstacles, "laser");\n    }\n\n    void detectionCallback(const isaac_ros_detectnet_interfaces::msg::DetectionArray::SharedPtr detection_msg)\n    {\n        // Process 2D detections and convert to 3D obstacle positions\n        std::vector<geometry_msgs::msg::Point32> vision_obstacles = processDetections(*detection_msg);\n\n        // Update obstacle map\n        updateObstacleMap(vision_obstacles, "vision");\n    }\n\n    void pointcloudCallback(const sensor_msgs::msg::PointCloud2::SharedPtr cloud_msg)\n    {\n        // Process point cloud to detect obstacles\n        pcl::PointCloud<pcl::PointXYZ>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZ>);\n        pcl::fromROSMsg(*cloud_msg, *cloud);\n\n        std::vector<geometry_msgs::msg::Point32> pointcloud_obstacles = processPointCloud(cloud);\n\n        // Update obstacle map\n        updateObstacleMap(pointcloud_obstacles, "pointcloud");\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processLaserScan(const sensor_msgs::msg::LaserScan& scan)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        for (size_t i = 0; i < scan.ranges.size(); ++i) {\n            if (scan.ranges[i] < scan.range_min || scan.ranges[i] > scan.range_max) {\n                continue;  // Invalid range\n            }\n\n            if (scan.ranges[i] < obstacle_distance_threshold_) {\n                // Convert polar to Cartesian coordinates\n                double angle = scan.angle_min + i * scan.angle_increment;\n                geometry_msgs::msg::Point32 point;\n                point.x = scan.ranges[i] * cos(angle);\n                point.y = scan.ranges[i] * sin(angle);\n                point.z = 0.0;  // Assume ground level\n\n                obstacles.push_back(point);\n            }\n        }\n\n        return obstacles;\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processDetections(\n        const isaac_ros_detectnet_interfaces::msg::DetectionArray& detections)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        for (const auto& detection : detections.detections) {\n            if (detection.confidence > detection_confidence_threshold_) {\n                // Convert 2D bounding box to 3D position estimate\n                // This requires depth information from stereo or depth sensor\n                geometry_msgs::msg::Point32 obstacle_pos;\n\n                // For now, assume a fixed depth based on object type and size\n                double estimated_depth = estimateDepthFromSize(detection.bbox, detection.label);\n\n                // Convert image coordinates to robot coordinates\n                // This requires camera calibration parameters\n                obstacle_pos.x = (detection.bbox.center.x - camera_cx_) * estimated_depth / camera_fx_;\n                obstacle_pos.y = (detection.bbox.center.y - camera_cy_) * estimated_depth / camera_fy_;\n                obstacle_pos.z = estimated_depth;\n\n                obstacles.push_back(obstacle_pos);\n            }\n        }\n\n        return obstacles;\n    }\n\n    std::vector<geometry_msgs::msg::Point32> processPointCloud(const pcl::PointCloud<pcl::PointXYZ>::Ptr cloud)\n    {\n        std::vector<geometry_msgs::msg::Point32> obstacles;\n\n        // Use PCL to segment obstacles from ground plane\n        pcl::ModelCoefficients::Ptr coefficients(new pcl::ModelCoefficients);\n        pcl::PointIndices::Ptr inliers(new pcl::PointIndices);\n\n        // Create the segmentation object\n        pcl::SACSegmentation<pcl::PointXYZ> seg;\n        seg.setOptimizeCoefficients(true);\n        seg.setModelType(pcl::SACMODEL_PLANE);\n        seg.setMethodType(pcl::SAC_RANSAC);\n        seg.setMaxIterations(100);\n        seg.setDistanceThreshold(0.05);  // 5cm tolerance for ground plane\n\n        seg.setInputCloud(cloud);\n        seg.segment(*inliers, *coefficients);\n\n        // Extract obstacles (points not belonging to ground plane)\n        pcl::ExtractIndices<pcl::PointXYZ> extract;\n        extract.setInputCloud(cloud);\n        extract.setIndices(inliers);\n        extract.setNegative(true);  // Extract points NOT on the plane\n        extract.filter(obstacles);\n\n        // Convert PCL points to ROS points\n        std::vector<geometry_msgs::msg::Point32> obstacle_points;\n        for (const auto& point : obstacles.points) {\n            geometry_msgs::msg::Point32 ros_point;\n            ros_point.x = point.x;\n            ros_point.y = point.y;\n            ros_point.z = point.z;\n            obstacle_points.push_back(ros_point);\n        }\n\n        return obstacle_points;\n    }\n\n    void updateObstacleMap(\n        const std::vector<geometry_msgs::msg::Point32>& new_obstacles,\n        const std::string& sensor_type)\n    {\n        // Fuse obstacles from different sensors\n        for (const auto& obstacle : new_obstacles) {\n            // Add to combined obstacle map with sensor type information\n            auto it = std::find_if(combined_obstacles_.begin(), combined_obstacles_.end(),\n                [&obstacle](const FusedObstacle& existing) {\n                    double dist = sqrt(pow(existing.point.x - obstacle.x, 2) +\n                                      pow(existing.point.y - obstacle.y, 2));\n                    return dist < fusion_distance_threshold_;\n                });\n\n            if (it != combined_obstacles_.end()) {\n                // Update existing obstacle with new information\n                it->update(obstacle, sensor_type);\n            } else {\n                // Add new obstacle\n                FusedObstacle new_fused_obstacle(obstacle, sensor_type);\n                combined_obstacles_.push_back(new_fused_obstacle);\n            }\n        }\n\n        // Publish combined obstacle information\n        publishCombinedObstacles();\n    }\n\n    void publishCombinedObstacles()\n    {\n        geometry_msgs::msg::PolygonStamped obstacle_polygon;\n        obstacle_polygon.header.frame_id = "map";\n        obstacle_polygon.header.stamp = this->now();\n\n        for (const auto& obstacle : combined_obstacles_) {\n            geometry_msgs::msg::Point32 point;\n            point.x = obstacle.point.x;\n            point.y = obstacle.point.y;\n            point.z = obstacle.point.z;\n            obstacle_polygon.polygon.points.push_back(point);\n        }\n\n        obstacle_pub_->publish(obstacle_polygon);\n    }\n\n    double estimateDepthFromSize(\n        const isaac_ros_detectnet_interfaces::msg::BoundingBox& bbox,\n        const std::string& label)\n    {\n        // Estimate depth based on expected object size\n        // This is a simplified approach - in practice, you\'d use stereo or depth sensor\n        double expected_width = getExpectedWidth(label);\n        double pixel_width = bbox.size_x;\n\n        // Using thin lens equation: depth = (focal_length * real_width) / pixel_width\n        return (camera_fx_ * expected_width) / pixel_width;\n    }\n\n    double getExpectedWidth(const std::string& label)\n    {\n        // Return expected width for common object types (in meters)\n        if (label == "person") return 0.5;      // Average person width\n        if (label == "car") return 1.8;         // Average car width\n        if (label == "chair") return 0.6;       // Average chair width\n        if (label == "table") return 1.0;       // Average table width\n        return 0.5;  // Default assumption\n    }\n\n    struct FusedObstacle {\n        geometry_msgs::msg::Point32 point;\n        std::map<std::string, int> sensor_votes;  // Count of detections from each sensor\n        double confidence;                        // Overall confidence\n\n        FusedObstacle(const geometry_msgs::msg::Point32& p, const std::string& sensor_type) :\n            point(p), confidence(0.5) {\n            sensor_votes[sensor_type] = 1;\n        }\n\n        void update(const geometry_msgs::msg::Point32& new_point, const std::string& sensor_type) {\n            // Update position with weighted average\n            point.x = (point.x + new_point.x) / 2.0;\n            point.y = (point.y + new_point.y) / 2.0;\n            point.z = (point.z + new_point.z) / 2.0;\n\n            // Update sensor votes\n            sensor_votes[sensor_type]++;\n\n            // Update confidence based on number of confirming sensors\n            confidence = std::min(1.0, static_cast<double>(sensor_votes.size()) / 3.0);\n        }\n    };\n\n    // Subscriptions\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_sub_;\n    rclcpp::Subscription<isaac_ros_detectnet_interfaces::msg::DetectionArray>::SharedPtr detection_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::PointCloud2>::SharedPtr pointcloud_sub_;\n\n    // Publisher\n    rclcpp::Publisher<geometry_msgs::msg::PolygonStamped>::SharedPtr obstacle_pub_;\n\n    // Obstacle data\n    std::vector<FusedObstacle> combined_obstacles_;\n\n    // Parameters\n    const double obstacle_distance_threshold_ = 2.0;  // Max distance to consider obstacle\n    const double detection_confidence_threshold_ = 0.7;  // Min confidence for detections\n    const double fusion_distance_threshold_ = 0.3;    // Distance to fuse detections\n    const double camera_fx_ = 616.363;  // Camera focal length x\n    const double camera_fy_ = 616.363;  // Camera focal length y\n    const double camera_cx_ = 313.071;  // Camera principal point x\n    const double camera_cy_ = 245.091;  // Camera principal point y\n};\n'})}),"\n",(0,s.jsx)(e.h2,{id:"humanoid-specific-navigation-behaviors",children:"Humanoid-Specific Navigation Behaviors"}),"\n",(0,s.jsx)(e.h3,{id:"step-planning-for-bipedal-locomotion",children:"Step Planning for Bipedal Locomotion"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:"#include <rclcpp/rclcpp.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/path.h>\n#include <visualization_msgs/msg/marker_array.h>\n\nclass HumanoidStepPlanner {\npublic:\n    HumanoidStepPlanner() {\n        // Initialize humanoid-specific parameters\n        step_length_ = 0.3;      // Average step length in meters\n        step_width_ = 0.2;       // Side-to-side step distance\n        step_height_ = 0.05;     // Step clearance height\n        max_step_rotation_ = 0.2; // Max rotation per step (rad)\n        step_duration_ = 0.8;    // Time for one step (sec)\n    }\n\n    struct Step {\n        geometry_msgs::msg::Point left_foot;\n        geometry_msgs::msg::Point right_foot;\n        double time;\n        bool is_support_step;  // Which foot is supporting weight\n    };\n\n    std::vector<Step> planSteps(const nav_msgs::msg::Path& path,\n                               const geometry_msgs::msg::Pose& start_pose)\n    {\n        std::vector<Step> steps;\n\n        if (path.poses.empty()) return steps;\n\n        // Start with current stance\n        Step initial_step;\n        initial_step.left_foot = calculateInitialFootPosition(start_pose, true);\n        initial_step.right_foot = calculateInitialFootPosition(start_pose, false);\n        initial_step.time = 0.0;\n        initial_step.is_support_step = true;  // Right foot starts as support\n        steps.push_back(initial_step);\n\n        // Plan steps along the path\n        size_t current_path_idx = 0;\n        geometry_msgs::msg::Pose current_pose = start_pose;\n        double current_time = 0.0;\n\n        while (current_path_idx < path.poses.size()) {\n            // Calculate next step based on path direction\n            auto next_waypoint = getNextWaypoint(path, current_path_idx, current_pose);\n            auto next_step = calculateNextStep(current_pose, next_waypoint, steps.back());\n\n            if (isValidStep(next_step, path)) {\n                next_step.time = current_time + step_duration_;\n                steps.push_back(next_step);\n\n                // Update current pose based on step\n                updatePoseFromStep(current_pose, next_step);\n                current_time += step_duration_;\n\n                // Move to next significant waypoint\n                current_path_idx = findNextSignificantWaypoint(path, current_path_idx);\n            } else {\n                // Handle invalid step (obstacle, unstable, etc.)\n                auto recovery_step = planRecoveryStep(steps.back());\n                if (isValidStep(recovery_step, path)) {\n                    recovery_step.time = current_time + step_duration_;\n                    steps.push_back(recovery_step);\n                    updatePoseFromStep(current_pose, recovery_step);\n                    current_time += step_duration_;\n                } else {\n                    // Cannot proceed, return current plan\n                    break;\n                }\n            }\n        }\n\n        return steps;\n    }\n\nprivate:\n    geometry_msgs::msg::Point calculateInitialFootPosition(\n        const geometry_msgs::msg::Pose& robot_pose, bool is_left_foot)\n    {\n        geometry_msgs::msg::Point foot_pos;\n        double yaw = tf2::getYaw(robot_pose.orientation);\n\n        // Place feet shoulder-width apart initially\n        double offset_x = 0.0;\n        double offset_y = is_left_foot ? step_width_/2.0 : -step_width_/2.0;\n\n        // Transform offset to robot frame\n        foot_pos.x = robot_pose.position.x + offset_x * cos(yaw) - offset_y * sin(yaw);\n        foot_pos.y = robot_pose.position.y + offset_x * sin(yaw) + offset_y * cos(yaw);\n        foot_pos.z = robot_pose.position.z;  // Ground level\n\n        return foot_pos;\n    }\n\n    geometry_msgs::msg::PoseStamped getNextWaypoint(\n        const nav_msgs::msg::Path& path, size_t current_idx,\n        const geometry_msgs::msg::Pose& current_pose)\n    {\n        // Find the next waypoint that's ahead of the robot\n        for (size_t i = current_idx; i < path.poses.size(); ++i) {\n            double dist_sq = pow(path.poses[i].pose.position.x - current_pose.position.x, 2) +\n                            pow(path.poses[i].pose.position.y - current_pose.position.y, 2);\n\n            if (dist_sq > pow(step_length_ * 0.8, 2)) {  // Look ahead 80% of step length\n                return path.poses[i];\n            }\n        }\n\n        // If no significant waypoint found, return the last one\n        if (!path.poses.empty()) {\n            return path.poses.back();\n        }\n\n        // Return current pose if no path\n        geometry_msgs::msg::PoseStamped dummy;\n        dummy.pose = current_pose;\n        return dummy;\n    }\n\n    Step calculateNextStep(const geometry_msgs::msg::Pose& current_pose,\n                          const geometry_msgs::msg::PoseStamped& target_waypoint,\n                          const Step& previous_step)\n    {\n        Step next_step;\n\n        // Calculate direction to target\n        double dx = target_waypoint.pose.position.x - current_pose.position.x;\n        double dy = target_waypoint.pose.position.y - current_pose.position.y;\n        double target_yaw = atan2(dy, dx);\n        double current_yaw = tf2::getYaw(current_pose.orientation);\n\n        // Determine which foot to move (opposite of support foot)\n        bool move_left_foot = previous_step.is_support_step;  // If right was support, move left\n\n        // Calculate new foot position\n        double step_yaw = current_yaw + (move_left_foot ? max_step_rotation_ : -max_step_rotation_);\n\n        geometry_msgs::msg::Point new_foot_pos;\n        if (move_left_foot) {\n            // Move left foot toward target\n            new_foot_pos.x = current_pose.position.x + step_length_ * cos(step_yaw);\n            new_foot_pos.y = current_pose.position.y + step_length_ * sin(step_yaw);\n            new_foot_pos.z = current_pose.position.z;\n\n            // Keep right foot in place\n            next_step.right_foot = previous_step.right_foot;\n            next_step.left_foot = new_foot_pos;\n        } else {\n            // Move right foot toward target\n            new_foot_pos.x = current_pose.position.x + step_length_ * cos(step_yaw);\n            new_foot_pos.y = current_pose.position.y + step_length_ * sin(step_yaw);\n            new_foot_pos.z = current_pose.position.z;\n\n            // Keep left foot in place\n            next_step.left_foot = previous_step.left_foot;\n            next_step.right_foot = new_foot_pos;\n        }\n\n        // Update support foot (alternates with each step)\n        next_step.is_support_step = !previous_step.is_support_step;\n\n        return next_step;\n    }\n\n    bool isValidStep(const Step& step, const nav_msgs::msg::Path& path)\n    {\n        // Check if step is stable (center of mass within support polygon)\n        geometry_msgs::msg::Point com = calculateCOMPosition(step);\n\n        if (!isWithinSupportPolygon(com, step)) {\n            return false;\n        }\n\n        // Check for obstacles at step location\n        if (isStepLocationBlocked(step)) {\n            return false;\n        }\n\n        // Check if step deviates too much from planned path\n        if (isStepOffPath(step, path)) {\n            return false;\n        }\n\n        return true;\n    }\n\n    bool isWithinSupportPolygon(const geometry_msgs::msg::Point& com, const Step& step)\n    {\n        // For bipedal locomotion, support polygon is the convex hull of both feet\n        // This is a simplified check - in practice, you'd calculate the actual convex hull\n\n        // Check if COM is roughly between the feet\n        double min_x = std::min(step.left_foot.x, step.right_foot.x);\n        double max_x = std::max(step.left_foot.x, step.right_foot.x);\n        double min_y = std::min(step.left_foot.y, step.right_foot.y);\n        double max_y = std::max(step.left_foot.y, step.right_foot.y);\n\n        // Add a safety margin\n        double margin = balance_margin_;\n\n        return (com.x >= min_x - margin && com.x <= max_x + margin &&\n                com.y >= min_y - margin && com.y <= max_y + margin);\n    }\n\n    bool isStepLocationBlocked(const Step& step)\n    {\n        // Check if the step location has obstacles\n        // This would interface with the costmap or obstacle detection system\n        // For now, return false as a placeholder\n        return false;\n    }\n\n    bool isStepOffPath(const Step& step, const nav_msgs::msg::Path& path)\n    {\n        // Check if the step deviates too much from the global path\n        // This would require path tracking algorithms\n        // For now, return false as a placeholder\n        return false;\n    }\n\n    geometry_msgs::msg::Point calculateCOMPosition(const Step& step)\n    {\n        // Simplified COM calculation - in reality, this would consider\n        // the full robot kinematics and mass distribution\n        geometry_msgs::msg::Point com;\n        com.x = (step.left_foot.x + step.right_foot.x) / 2.0;\n        com.y = (step.left_foot.y + step.right_foot.y) / 2.0;\n        com.z = com_height_;  // Approximate COM height\n\n        return com;\n    }\n\n    Step planRecoveryStep(const Step& previous_step)\n    {\n        // Plan a recovery step when normal stepping is not possible\n        // This might involve: stepping in place, taking a smaller step, etc.\n\n        Step recovery_step = previous_step;\n\n        // For now, just return the previous step as a placeholder\n        // In practice, this would implement various recovery behaviors\n        return recovery_step;\n    }\n\n    size_t findNextSignificantWaypoint(const nav_msgs::msg::Path& path, size_t current_idx)\n    {\n        // Find the next waypoint that represents a significant change in direction\n        // This prevents excessive step planning for dense paths\n        size_t next_idx = current_idx + 1;\n\n        // Simple approach: skip waypoints that are very close together\n        while (next_idx < path.poses.size()) {\n            double dist_sq = pow(path.poses[next_idx].pose.position.x -\n                                path.poses[current_idx].pose.position.x, 2) +\n                            pow(path.poses[next_idx].pose.position.y -\n                               path.poses[current_idx].pose.position.y, 2);\n\n            if (dist_sq > pow(step_length_ * 0.5, 2)) {  // Minimum distance between processed waypoints\n                return next_idx;\n            }\n            next_idx++;\n        }\n\n        return path.poses.size();  // Return end if no significant waypoint found\n    }\n\n    void updatePoseFromStep(geometry_msgs::msg::Pose& pose, const Step& step)\n    {\n        // Update robot pose based on completed step\n        // This would consider the kinematics of the step\n        pose.position.x = (step.left_foot.x + step.right_foot.x) / 2.0;\n        pose.position.y = (step.left_foot.y + step.right_foot.y) / 2.0;\n\n        // Update orientation based on foot positions\n        double dx = step.right_foot.x - step.left_foot.x;\n        double dy = step.right_foot.y - step.left_foot.y;\n        double yaw = atan2(dy, dx) + M_PI/2;  // Rotate 90 degrees for forward direction\n\n        tf2::Quaternion q;\n        q.setRPY(0, 0, yaw);\n        pose.orientation = tf2::toMsg(q);\n    }\n\n    // Humanoid-specific parameters\n    double step_length_;\n    double step_width_;\n    double step_height_;\n    double max_step_rotation_;\n    double step_duration_;\n    double com_height_ = 0.8;  // Approximate height of center of mass\n    double balance_margin_ = 0.1;  // Safety margin for balance\n};\n"})}),"\n",(0,s.jsx)(e.h2,{id:"isaac-sim-navigation-integration",children:"Isaac Sim Navigation Integration"}),"\n",(0,s.jsx)(e.h3,{id:"isaac-sim-navigation-controller",children:"Isaac Sim Navigation Controller"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Isaac Sim navigation integration\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.range_sensor import LidarRtx\nimport numpy as np\nimport carb\n\nclass IsaacSimNavigationController:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_navigation_environment()\n\n    def setup_navigation_environment(self):\n        # Add robot to the scene\n        assets_root_path = get_assets_root_path()\n        if assets_root_path is None:\n            carb.log_error("Could not find Isaac Sim assets. Please check your Isaac Sim installation.")\n            return\n\n        # Add a humanoid robot (example with Carter robot)\n        robot_asset_path = assets_root_path + "/Isaac/Robots/Carter/carter_navigate.usd"\n        add_reference_to_stage(usd_path=robot_asset_path, prim_path="/World/Carter")\n\n        # Add a LIDAR sensor to the robot\n        self.lidar = LidarRtx(\n            prim_path="/World/Carter/chassis/lidar",\n            translation=np.array([0.0, 0.0, 0.3]),\n            orientation=np.array([0.0, 0.0, 0.0, 1.0]),\n            config="Carter",\n            rotation_frequency=10,\n            samples_per_scan=1080\n        )\n\n        # Add a simple environment\n        room_asset_path = assets_root_path + "/Isaac/Environments/Simple_Room/simple_room.usd"\n        add_reference_to_stage(usd_path=room_asset_path, prim_path="/World/Room")\n\n        # Initialize the world\n        self.world.reset()\n\n    def run_navigation_simulation(self, goal_position):\n        """Run navigation simulation with obstacle avoidance"""\n        self.world.reset()\n\n        while not self.world.is_stopped():\n            self.world.step(render=True)\n\n            # Get robot state\n            robot_position = self.get_robot_position()\n            robot_orientation = self.get_robot_orientation()\n            lidar_data = self.lidar.get_linear_depth_data()\n\n            # Check if reached goal\n            if self.is_at_goal(robot_position, goal_position):\n                print(f"Reached goal at {goal_position}!")\n                break\n\n            # Plan and execute navigation\n            cmd_vel = self.plan_navigation_command(\n                robot_position, robot_orientation,\n                goal_position, lidar_data\n            )\n\n            # Apply command to robot\n            self.execute_command(cmd_vel)\n\n    def get_robot_position(self):\n        """Get current robot position from Isaac Sim"""\n        # In a real implementation, this would get the robot\'s position\n        # from the simulation\n        pass\n\n    def get_robot_orientation(self):\n        """Get current robot orientation from Isaac Sim"""\n        pass\n\n    def is_at_goal(self, current_pos, goal_pos, tolerance=0.2):\n        """Check if robot is at goal position"""\n        distance = np.linalg.norm(np.array(current_pos[:2]) - np.array(goal_pos[:2]))\n        return distance < tolerance\n\n    def plan_navigation_command(self, current_pos, current_orient, goal_pos, lidar_data):\n        """Plan navigation command based on goal and sensor data"""\n        # Calculate direction to goal\n        dx = goal_pos[0] - current_pos[0]\n        dy = goal_pos[1] - current_pos[1]\n        goal_distance = np.sqrt(dx*dx + dy*dy)\n\n        # Calculate goal angle\n        goal_angle = np.arctan2(dy, dx)\n\n        # Get robot\'s current angle\n        robot_yaw = self.orientation_to_yaw(current_orient)\n\n        # Calculate angle difference\n        angle_diff = self.normalize_angle(goal_angle - robot_yaw)\n\n        # Simple proportional controller\n        linear_vel = min(0.5, goal_distance * 0.5)  # Scale with distance\n        angular_vel = angle_diff * 1.0  # Proportional control\n\n        # Obstacle avoidance\n        min_distance = np.min(lidar_data) if len(lidar_data) > 0 else float(\'inf\')\n\n        if min_distance < 0.5:  # Obstacle detected\n            # Slow down and turn away from obstacle\n            linear_vel *= 0.3\n            angular_vel += self.avoid_obstacle(lidar_data)\n\n        # Ensure velocities are within limits\n        linear_vel = np.clip(linear_vel, 0.0, 0.5)\n        angular_vel = np.clip(angular_vel, -0.5, 0.5)\n\n        return [linear_vel, 0.0, 0.0], [0.0, 0.0, angular_vel]  # linear, angular velocities\n\n    def orientation_to_yaw(self, orientation):\n        """Convert quaternion orientation to yaw angle"""\n        # Simplified conversion - in practice, use proper quaternion to euler conversion\n        return np.arctan2(2*(orientation[3]*orientation[2] + orientation[0]*orientation[1]),\n                         1 - 2*(orientation[1]**2 + orientation[2]**2))\n\n    def normalize_angle(self, angle):\n        """Normalize angle to [-pi, pi] range"""\n        while angle > np.pi:\n            angle -= 2*np.pi\n        while angle < -np.pi:\n            angle += 2*np.pi\n        return angle\n\n    def avoid_obstacle(self, lidar_data):\n        """Calculate avoidance angular velocity based on LIDAR data"""\n        if len(lidar_data) == 0:\n            return 0.0\n\n        # Find the direction of the closest obstacle\n        min_idx = np.argmin(lidar_data)\n        angle_resolution = 2 * np.pi / len(lidar_data)\n        obstacle_angle = min_idx * angle_resolution - np.pi  # Convert to [-pi, pi]\n\n        # Turn away from the obstacle\n        # If obstacle is on the right, turn left (negative angular velocity)\n        # If obstacle is on the left, turn right (positive angular velocity)\n        if abs(obstacle_angle) < np.pi/2:  # Obstacle is in front\n            return -np.sign(obstacle_angle) * 0.3  # Turn away from obstacle\n        else:\n            return 0.0  # Obstacle is behind, no need to turn\n\n    def execute_command(self, cmd_vel):\n        """Execute the navigation command in Isaac Sim"""\n        # In a real implementation, this would send the command to the robot\n        # controller in Isaac Sim\n        linear_vel, angular_vel = cmd_vel\n        # Apply these velocities to the robot\'s differential drive controller\n        pass\n'})}),"\n",(0,s.jsx)(e.h2,{id:"recovery-behaviors",children:"Recovery Behaviors"}),"\n",(0,s.jsx)(e.h3,{id:"navigation-recovery-behaviors",children:"Navigation Recovery Behaviors"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <nav2_core/recovery.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <pluginlib/class_list_macros.hpp>\n\nclass HumanoidSpinRecovery : public nav2_core::Recovery\n{\npublic:\n    HumanoidSpinRecovery() = default;\n    ~HumanoidSpinRecovery() override = default;\n\n    void configure(\n        const rclcpp_lifecycle::LifecycleNode::WeakPtr & parent,\n        const std::string & name,\n        const std::shared_ptr<tf2_ros::Buffer> & tf,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & global_costmap,\n        const std::shared_ptr<nav2_costmap_2d::Costmap2DROS> & local_costmap) override\n    {\n        node_ = parent.lock();\n        name_ = name;\n        tf_ = tf;\n        global_costmap_ = global_costmap;\n        local_costmap_ = local_costmap;\n\n        // Declare parameters specific to humanoid recovery\n        spin_angular_vel_ = node_->declare_parameter(name_ + ".spin_angular_vel", 0.5);\n        min_spin_duration_ = node_->declare_parameter(name_ + ".min_spin_duration", 1.0);\n        max_spin_duration_ = node_->declare_parameter(name_ + ".max_spin_duration", 10.0);\n\n        vel_pub_ = node_->create_publisher<geometry_msgs::msg::Twist>("cmd_vel", 1);\n    }\n\n    void cleanup() override\n    {\n        vel_pub_->on_deactivate();\n    }\n\n    void activate() override\n    {\n        vel_pub_->on_activate();\n    }\n\n    void deactivate() override\n    {\n        vel_pub_->on_deactivate();\n    }\n\n    nav2_core::RecoveryResult run(\n        const std::shared_ptr<const nav2_msgs::action::Recovery::Goal> command) override\n    {\n        RCLCPP_INFO(node_->get_logger(), "Starting humanoid spin recovery behavior");\n\n        // For humanoid robots, spinning in place may not be feasible\n        // Instead, implement a gentle turning motion with steps\n        return executeHumanoidSpin();\n    }\n\nprivate:\n    nav2_core::RecoveryResult executeHumanoidSpin()\n    {\n        nav2_core::RecoveryResult result;\n        result.outcome = nav2_core::RecoveryResult::SUCCESS;\n\n        auto start_time = node_->now();\n        auto current_time = start_time;\n\n        while (rclcpp::ok()) {\n            current_time = node_->now();\n\n            // Check if we\'ve spun enough\n            if ((current_time - start_time).seconds() > min_spin_duration_) {\n                // Check if we\'ve cleared the obstacle\n                if (isObstacleClear()) {\n                    RCLCPP_INFO(node_->get_logger(), "Obstacle cleared, stopping spin recovery");\n                    break;\n                }\n\n                // Check if we\'ve spun too long\n                if ((current_time - start_time).seconds() > max_spin_duration_) {\n                    RCLCPP_WARN(node_->get_logger(), "Spin recovery timed out");\n                    result.outcome = nav2_core::RecoveryResult::FAILURE;\n                    break;\n                }\n            }\n\n            // Generate spin command for humanoid\n            auto spin_cmd = generateHumanoidSpinCommand();\n            vel_pub_->publish(spin_cmd);\n\n            // Sleep briefly to allow other processes\n            std::this_thread::sleep_for(std::chrono::milliseconds(100));\n        }\n\n        // Stop the robot\n        geometry_msgs::msg::Twist stop_cmd;\n        vel_pub_->publish(stop_cmd);\n\n        return result;\n    }\n\n    geometry_msgs::msg::Twist generateHumanoidSpinCommand()\n    {\n        geometry_msgs::msg::Twist cmd;\n\n        // For humanoid, instead of pure rotation, we might want to step-turn\n        // This is a simplified approach - real implementation would plan actual steps\n        cmd.angular.z = spin_angular_vel_;\n\n        // Small forward motion to maintain momentum\n        cmd.linear.x = 0.05;\n\n        return cmd;\n    }\n\n    bool isObstacleClear()\n    {\n        // Check if obstacles are clear in the local costmap\n        auto costmap = local_costmap_->getCostmap();\n        unsigned int mx, my;\n\n        // Check multiple directions around the robot\n        double robot_x = costmap->getOriginX() + costmap->getSizeInMetersX() / 2.0;\n        double robot_y = costmap->getOriginY() + costmap->getSizeInMetersY() / 2.0;\n\n        for (double angle = 0; angle < 2*M_PI; angle += M_PI/4) {\n            double check_x = robot_x + 0.5 * cos(angle);  // Check 0.5m out\n            double check_y = robot_y + 0.5 * sin(angle);\n\n            if (costmap->worldToMap(check_x, check_y, mx, my)) {\n                unsigned char cost = costmap->getCost(mx, my);\n                if (cost >= nav2_costmap_2d::INSCRIBED_INFLATED_OBSTACLE) {\n                    return false;  // Found an obstacle\n                }\n            }\n        }\n\n        return true;  // No obstacles detected\n    }\n\n    rclcpp_lifecycle::LifecycleNode::SharedPtr node_;\n    std::string name_;\n    std::shared_ptr<tf2_ros::Buffer> tf_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> global_costmap_;\n    std::shared_ptr<nav2_costmap_2d::Costmap2DROS> local_costmap_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr vel_pub_;\n\n    // Parameters\n    double spin_angular_vel_;\n    double min_spin_duration_;\n    double max_spin_duration_;\n};\n\nPLUGINLIB_EXPORT_CLASS(HumanoidSpinRecovery, nav2_core::Recovery)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,s.jsx)(e.h3,{id:"navigation-performance-metrics",children:"Navigation Performance Metrics"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <geometry_msgs/msg/twist.h>\n\nclass NavigationPerformanceEvaluator\n{\npublic:\n    struct NavigationMetrics\n    {\n        double success_rate = 0.0;\n        double average_time_to_goal = 0.0;\n        double path_efficiency = 0.0;  // actual_path_length / optimal_path_length\n        double average_velocity = 0.0;\n        int collision_count = 0;\n        int oscillation_count = 0;\n        int recovery_count = 0;\n        double energy_efficiency = 0.0;\n        double obstacle_avoidance_quality = 0.0;\n    };\n\n    NavigationPerformanceEvaluator() {}\n\n    void startTrial(const geometry_msgs::msg::Pose& start, const geometry_msgs::msg::Pose& goal)\n    {\n        trial_start_time_ = std::chrono::high_resolution_clock::now();\n        start_pose_ = start;\n        goal_pose_ = goal;\n        path_length_ = 0.0;\n        collision_count_ = 0;\n        oscillation_count_ = 0;\n        recovery_count_ = 0;\n        previous_pose_ = start;\n    }\n\n    void update(const geometry_msgs::msg::Pose& current_pose,\n               const geometry_msgs::msg::Twist& cmd_vel,\n               bool in_collision = false,\n               bool in_recovery = false)\n    {\n        // Update path length\n        double delta = std::sqrt(std::pow(current_pose.position.x - previous_pose_.position.x, 2) +\n                                std::pow(current_pose.position.y - previous_pose_.position.y, 2));\n        path_length_ += delta;\n        previous_pose_ = current_pose;\n\n        // Count collisions\n        if (in_collision) {\n            collision_count_++;\n        }\n\n        // Count oscillations (rapid direction changes)\n        if (std::abs(cmd_vel.angular.z) > oscillation_threshold_) {\n            oscillation_count_++;\n        }\n\n        // Count recovery behaviors\n        if (in_recovery) {\n            recovery_count_++;\n        }\n    }\n\n    NavigationMetrics completeTrial(bool success)\n    {\n        auto end_time = std::chrono::high_resolution_clock::now();\n        double trial_time = std::chrono::duration<double>(end_time - trial_start_time_).count();\n\n        NavigationMetrics metrics;\n        metrics.success_rate = success ? 1.0 : 0.0;\n        metrics.average_time_to_goal = success ? trial_time : 0.0;\n\n        // Calculate optimal path length (straight line)\n        double optimal_length = std::sqrt(\n            std::pow(goal_pose_.position.x - start_pose_.position.x, 2) +\n            std::pow(goal_pose_.position.y - start_pose_.position.y, 2)\n        );\n\n        metrics.path_efficiency = (optimal_length > 0) ? path_length_ / optimal_length : 1.0;\n        metrics.average_velocity = (trial_time > 0) ? path_length_ / trial_time : 0.0;\n        metrics.collision_count = collision_count_;\n        metrics.oscillation_count = oscillation_count_;\n        metrics.recovery_count = recovery_count_;\n\n        // Energy efficiency could be calculated based on actuator commands\n        metrics.energy_efficiency = calculateEnergyEfficiency();\n\n        // Obstacle avoidance quality based on minimum distances to obstacles\n        metrics.obstacle_avoidance_quality = calculateObstacleAvoidanceQuality();\n\n        return metrics;\n    }\n\n    void printMetrics(const NavigationMetrics& metrics)\n    {\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "Navigation Performance Metrics:");\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Success Rate: %.2f", metrics.success_rate);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Avg Time to Goal: %.2f s", metrics.average_time_to_goal);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Path Efficiency: %.2f", metrics.path_efficiency);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Avg Velocity: %.2f m/s", metrics.average_velocity);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Collisions: %d", metrics.collision_count);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Oscillations: %d", metrics.oscillation_count);\n        RCLCPP_INFO(rclcpp::get_logger("navigation_eval"),\n            "  Recoveries: %d", metrics.recovery_count);\n    }\n\nprivate:\n    double calculateEnergyEfficiency()\n    {\n        // Placeholder for energy efficiency calculation\n        // This would consider motor commands, robot dynamics, etc.\n        return 1.0;  // Perfect efficiency for now\n    }\n\n    double calculateObstacleAvoidanceQuality()\n    {\n        // Placeholder for obstacle avoidance quality\n        // This would consider minimum distances to obstacles during navigation\n        return 1.0;  // Perfect avoidance for now\n    }\n\n    std::chrono::high_resolution_clock::time_point trial_start_time_;\n    geometry_msgs::msg::Pose start_pose_;\n    geometry_msgs::msg::Pose goal_pose_;\n    geometry_msgs::msg::Pose previous_pose_;\n    double path_length_;\n    int collision_count_;\n    int oscillation_count_;\n    int recovery_count_;\n\n    const double oscillation_threshold_ = 0.5;  // rad/s\n};\n'})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(e.h3,{id:"1-multi-layered-safety-system",children:"1. Multi-Layered Safety System"}),"\n",(0,s.jsx)(e.p,{children:"Implement multiple layers of safety:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Perception-based obstacle detection"}),"\n",(0,s.jsx)(e.li,{children:"Costmap-based obstacle representation"}),"\n",(0,s.jsx)(e.li,{children:"Collision avoidance algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Emergency stop mechanisms"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-parameter-tuning",children:"2. Parameter Tuning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use systematic parameter tuning methods"}),"\n",(0,s.jsx)(e.li,{children:"Test in simulation before real robot deployment"}),"\n",(0,s.jsx)(e.li,{children:"Monitor performance metrics continuously"}),"\n",(0,s.jsx)(e.li,{children:"Adapt parameters based on environment conditions"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-humanoid-specific-considerations",children:"3. Humanoid-Specific Considerations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Balance maintenance during navigation"}),"\n",(0,s.jsx)(e.li,{children:"Step planning for bipedal locomotion"}),"\n",(0,s.jsx)(e.li,{children:"Fall prevention mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Dynamic stability during turning"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"exercise",children:"Exercise"}),"\n",(0,s.jsx)(e.p,{children:"Create a complete navigation system that includes:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Global path planning with A* algorithm adapted for humanoid robots"}),"\n",(0,s.jsx)(e.li,{children:"Local path planning with obstacle avoidance using DWA"}),"\n",(0,s.jsx)(e.li,{children:"Integration with perception data from Isaac ROS"}),"\n",(0,s.jsx)(e.li,{children:"Step planning for bipedal locomotion"}),"\n",(0,s.jsx)(e.li,{children:"Recovery behaviors for humanoid robots"}),"\n",(0,s.jsx)(e.li,{children:"Performance evaluation metrics"}),"\n",(0,s.jsx)(e.li,{children:"Isaac Sim integration for testing"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Test your system in various scenarios including:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Navigation around static obstacles"}),"\n",(0,s.jsx)(e.li,{children:"Dynamic obstacle avoidance"}),"\n",(0,s.jsx)(e.li,{children:"Stair climbing (if applicable)"}),"\n",(0,s.jsx)(e.li,{children:"Tight spaces navigation"}),"\n",(0,s.jsx)(e.li,{children:"Multi-goal navigation tasks"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Evaluate the system's performance using the metrics discussed in this section."})]})}function _(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);