"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[845],{1184:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Sensor simulation is a critical component of digital twin technology, enabling robots to perceive their virtual environment just as they would in the real world. This section covers simulating various types of sensors in both Gazebo and Unity environments.","source":"@site/docs/module-2-digital-twin/sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Practical Exercises - Digital Twin Simulation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/practical-exercises"},"next":{"title":"Unity Scene Setup and Physics Configuration","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-2-digital-twin/unity-scene-setup"}}');var s=i(4848),t=i(8453);const r={},o="Sensor Simulation",l={},d=[{value:"Overview of Sensor Simulation",id:"overview-of-sensor-simulation",level:2},{value:"Types of Sensors",id:"types-of-sensors",level:2},{value:"Range Sensors",id:"range-sensors",level:3},{value:"Vision Sensors",id:"vision-sensors",level:3},{value:"Inertial Sensors",id:"inertial-sensors",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Sensor Simulation in Gazebo",id:"sensor-simulation-in-gazebo",level:2},{value:"Camera Simulation",id:"camera-simulation",level:3},{value:"LIDAR Simulation",id:"lidar-simulation",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:3},{value:"IMU Simulation",id:"imu-simulation",level:3},{value:"Sensor Simulation in Unity",id:"sensor-simulation-in-unity",level:2},{value:"Camera Simulation with Perception Package",id:"camera-simulation-with-perception-package",level:3},{value:"LIDAR Simulation in Unity",id:"lidar-simulation-in-unity",level:3},{value:"IMU Simulation in Unity",id:"imu-simulation-in-unity",level:3},{value:"Sensor Fusion and Calibration",id:"sensor-fusion-and-calibration",level:2},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:3},{value:"Sensor Calibration",id:"sensor-calibration",level:3},{value:"Sensor Noise and Uncertainty",id:"sensor-noise-and-uncertainty",level:2},{value:"Adding Realistic Noise",id:"adding-realistic-noise",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Realism vs. Performance",id:"realism-vs-performance",level:3},{value:"Validation",id:"validation",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Sensor Data Quality",id:"sensor-data-quality",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"Exercise",id:"exercise",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"sensor-simulation",children:"Sensor Simulation"})}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is a critical component of digital twin technology, enabling robots to perceive their virtual environment just as they would in the real world. This section covers simulating various types of sensors in both Gazebo and Unity environments."}),"\n",(0,s.jsx)(e.h2,{id:"overview-of-sensor-simulation",children:"Overview of Sensor Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Robotic sensors provide the robot with information about its environment and internal state. In simulation, we must accurately model:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Physical sensing"}),": How the sensor would detect real-world phenomena"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Noise characteristics"}),": Real sensors have inherent noise and uncertainty"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Latency"}),": Processing delays in real sensor systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Field of view"}),": Physical limitations of sensor coverage"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Range limitations"}),": Maximum and minimum detection distances"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"types-of-sensors",children:"Types of Sensors"}),"\n",(0,s.jsx)(e.h3,{id:"range-sensors",children:"Range Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LIDAR"}),": 2D/3D laser range finders"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Ultrasonic"}),": Sound-based distance measurement"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Infrared"}),": Infrared-based proximity detection"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"vision-sensors",children:"Vision Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cameras"}),": RGB, depth, thermal imaging"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Stereo cameras"}),": 3D vision capabilities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Event cameras"}),": High-speed dynamic vision"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"inertial-sensors",children:"Inertial Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"IMU"}),": Inertial measurement units"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Accelerometers"}),": Linear acceleration"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Gyroscopes"}),": Angular velocity"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Magnetometers"}),": Magnetic field detection"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Force sensors"}),": Linear forces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Torque sensors"}),": Rotational forces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FT sensors"}),": Combined force/torque measurement"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"sensor-simulation-in-gazebo",children:"Sensor Simulation in Gazebo"}),"\n",(0,s.jsx)(e.h3,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Gazebo provides realistic camera simulation through the gazebo_ros_camera plugin:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<link name="camera_link">\n  <visual>\n    <geometry>\n      <box size="0.05 0.05 0.05"/>\n    </geometry>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.05 0.05 0.05"/>\n    </geometry>\n  </collision>\n</link>\n\n<gazebo reference="camera_link">\n  <sensor name="camera" type="camera">\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <frame_name>camera_link</frame_name>\n      <topic_name>image_raw</topic_name>\n      <camera_info_topic_name>camera_info</camera_info_topic_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"lidar-simulation",children:"LIDAR Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Simulate 2D and 3D LIDAR sensors:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="lidar_link">\n  <sensor name="lidar" type="ray">\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle> \x3c!-- -\u03c0 radians --\x3e\n          <max_angle>3.14159</max_angle>   \x3c!-- \u03c0 radians --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>laser</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="depth_camera_link">\n  <sensor name="depth_camera" type="depth">\n    <update_rate>30</update_rate>\n    <camera name="depth_cam">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10</far>\n      </clip>\n    </camera>\n    <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <baseline>0.2</baseline>\n      <alwaysOn>true</alwaysOn>\n      <updateRate>10.0</updateRate>\n      <cameraName>depth_camera</cameraName>\n      <imageTopicName>/rgb/image_raw</imageTopicName>\n      <depthImageTopicName>/depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>/depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>/rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>/depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>depth_camera_optical_frame</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.5</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>2e-4</stddev>\n            <bias_mean>0.0000075</bias_mean>\n            <bias_stddev>0.0000008</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>1.7e-2</stddev>\n            <bias_mean>0.1</bias_mean>\n            <bias_stddev>0.001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-simulation-in-unity",children:"Sensor Simulation in Unity"}),"\n",(0,s.jsx)(e.h3,{id:"camera-simulation-with-perception-package",children:"Camera Simulation with Perception Package"}),"\n",(0,s.jsx)(e.p,{children:"Unity's Perception package provides advanced camera simulation:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Perception.GroundTruth;\nusing Unity.Simulation;\n\npublic class CameraSensorSetup : MonoBehaviour\n{\n    [Header("Camera Properties")]\n    [SerializeField] private int width = 640;\n    [SerializeField] private int height = 480;\n    [SerializeField] private float fieldOfView = 60f;\n    [SerializeField] private float nearClip = 0.1f;\n    [SerializeField] private float farClip = 100f;\n\n    [Header("Sensor Properties")]\n    [SerializeField] private string sensorId = "camera_0";\n    [SerializeField] private string rosTopic = "/camera/image_raw";\n\n    private Camera cam;\n    private SyntheticCameraData syntheticCamera;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        if (cam == null)\n        {\n            cam = gameObject.AddComponent<Camera>();\n        }\n\n        ConfigureCamera();\n        SetupSyntheticCamera();\n    }\n\n    void ConfigureCamera()\n    {\n        cam.fieldOfView = fieldOfView;\n        cam.nearClipPlane = nearClip;\n        cam.farClipPlane = farClip;\n        cam.targetTexture = new RenderTexture(width, height, 24);\n    }\n\n    void SetupSyntheticCamera()\n    {\n        syntheticCamera = GetComponent<SyntheticCameraData>();\n        if (syntheticCamera == null)\n        {\n            syntheticCamera = gameObject.AddComponent<SyntheticCameraData>();\n        }\n\n        syntheticCamera.camera = cam;\n        syntheticCamera.sensorId = sensorId;\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"lidar-simulation-in-unity",children:"LIDAR Simulation in Unity"}),"\n",(0,s.jsx)(e.p,{children:"Create a LIDAR sensor using raycasting:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'using System.Collections.Generic;\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class UnityLidarSensor : MonoBehaviour\n{\n    [Header("LIDAR Properties")]\n    [SerializeField] private int horizontalRays = 360;\n    [SerializeField] private int verticalRays = 1;\n    [SerializeField] private float maxDistance = 10f;\n    [SerializeField] private float minDistance = 0.1f;\n    [SerializeField] private string topicName = "/scan";\n    [SerializeField] private float updateRate = 10f; // Hz\n\n    private float[] ranges;\n    private ROSConnection ros;\n    private float updateInterval;\n    private float lastUpdateTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        updateInterval = 1.0f / updateRate;\n        lastUpdateTime = 0;\n\n        // Initialize ranges array\n        ranges = new float[horizontalRays * verticalRays];\n    }\n\n    void Update()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            UpdateLidarScan();\n            PublishScan();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    void UpdateLidarScan()\n    {\n        for (int v = 0; v < verticalRays; v++)\n        {\n            float vAngle = (v - (verticalRays - 1) / 2.0f) * 0.1f; // Vertical spread\n\n            for (int h = 0; h < horizontalRays; h++)\n            {\n                float hAngle = (h * 2 * Mathf.PI) / horizontalRays;\n\n                Vector3 direction = new Vector3(\n                    Mathf.Cos(vAngle) * Mathf.Cos(hAngle),\n                    Mathf.Sin(vAngle),\n                    Mathf.Cos(vAngle) * Mathf.Sin(hAngle)\n                );\n\n                if (Physics.Raycast(transform.position, direction, out RaycastHit hit, maxDistance))\n                {\n                    ranges[v * horizontalRays + h] = hit.distance;\n                }\n                else\n                {\n                    ranges[v * horizontalRays + h] = float.PositiveInfinity;\n                }\n            }\n        }\n    }\n\n    void PublishScan()\n    {\n        var laserScan = new LaserScanMsg\n        {\n            header = new std_msgs.HeaderMsg\n            {\n                stamp = new builtin_interfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = transform.name\n            },\n            angle_min = -Mathf.PI,\n            angle_max = Mathf.PI,\n            angle_increment = (2 * Mathf.PI) / horizontalRays,\n            time_increment = 0,\n            scan_time = 1.0f / updateRate,\n            range_min = minDistance,\n            range_max = maxDistance,\n            ranges = ranges\n        };\n\n        ros.Publish(topicName, laserScan);\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"imu-simulation-in-unity",children:"IMU Simulation in Unity"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class UnityImuSensor : MonoBehaviour\n{\n    [Header("IMU Properties")]\n    [SerializeField] private string topicName = "/imu/data";\n    [SerializeField] private float updateRate = 100f; // Hz\n    [SerializeField] private float noiseLevel = 0.01f;\n\n    private ROSConnection ros;\n    private float updateInterval;\n    private float lastUpdateTime;\n    private Rigidbody attachedRigidbody;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        updateInterval = 1.0f / updateRate;\n        lastUpdateTime = 0;\n\n        // Try to find attached rigidbody\n        attachedRigidbody = GetComponent<Rigidbody>();\n        if (attachedRigidbody == null)\n        {\n            attachedRigidbody = GetComponentInParent<Rigidbody>();\n        }\n    }\n\n    void Update()\n    {\n        if (Time.time - lastUpdateTime >= updateInterval)\n        {\n            PublishImuData();\n            lastUpdateTime = Time.time;\n        }\n    }\n\n    void PublishImuData()\n    {\n        var imuMsg = new ImuMsg\n        {\n            header = new std_msgs.HeaderMsg\n            {\n                stamp = new builtin_interfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = transform.name\n            }\n        };\n\n        // Set orientation (from Unity rotation to quaternion)\n        Quaternion unityRotation = transform.rotation;\n        // Convert Unity coordinate system to ROS coordinate system\n        imuMsg.orientation = new geometry_msgs.QuaternionMsg\n        {\n            x = unityRotation.x,\n            y = unityRotation.y,\n            z = unityRotation.z,\n            w = unityRotation.w\n        };\n\n        // Set angular velocity\n        if (attachedRigidbody != null)\n        {\n            Vector3 angularVel = attachedRigidbody.angularVelocity;\n            imuMsg.angular_velocity = new geometry_msgs.Vector3Msg\n            {\n                x = angularVel.x + Random.Range(-noiseLevel, noiseLevel),\n                y = angularVel.y + Random.Range(-noiseLevel, noiseLevel),\n                z = angularVel.z + Random.Range(-noiseLevel, noiseLevel)\n            };\n\n            // Set linear acceleration\n            Vector3 linearAcc = attachedRigidbody.velocity / Time.fixedDeltaTime;\n            imuMsg.linear_acceleration = new geometry_msgs.Vector3Msg\n            {\n                x = linearAcc.x + Random.Range(-noiseLevel, noiseLevel),\n                y = linearAcc.y + Random.Range(-noiseLevel, noiseLevel),\n                z = linearAcc.z + Random.Range(-noiseLevel, noiseLevel)\n            };\n        }\n\n        ros.Publish(topicName, imuMsg);\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-fusion-and-calibration",children:"Sensor Fusion and Calibration"}),"\n",(0,s.jsx)(e.h3,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,s.jsx)(e.p,{children:"Combine data from multiple sensors for enhanced perception:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:"using System.Collections.Generic;\nusing UnityEngine;\n\npublic class SensorFusion : MonoBehaviour\n{\n    [SerializeField] private List<GameObject> sensors;\n    private Dictionary<string, object> sensorData;\n\n    void Start()\n    {\n        sensorData = new Dictionary<string, object>();\n    }\n\n    void Update()\n    {\n        // Collect data from all sensors\n        foreach (var sensor in sensors)\n        {\n            ISensorDataProvider provider = sensor.GetComponent<ISensorDataProvider>();\n            if (provider != null)\n            {\n                sensorData[provider.GetSensorId()] = provider.GetData();\n            }\n        }\n\n        // Process fused sensor data\n        ProcessFusedData();\n    }\n\n    void ProcessFusedData()\n    {\n        // Implement sensor fusion algorithms (Kalman filters, particle filters, etc.)\n        // Combine data from different sensors for better accuracy\n    }\n}\n\npublic interface ISensorDataProvider\n{\n    string GetSensorId();\n    object GetData();\n}\n"})}),"\n",(0,s.jsx)(e.h3,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,s.jsx)(e.p,{children:"Simulate sensor calibration procedures:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class SensorCalibration : MonoBehaviour\n{\n    [Header("Calibration Parameters")]\n    [SerializeField] private float calibrationDuration = 5.0f;\n    [SerializeField] private float calibrationInterval = 0.1f;\n\n    private bool isCalibrating = false;\n    private float calibrationStartTime = 0f;\n\n    public void StartCalibration()\n    {\n        isCalibrating = true;\n        calibrationStartTime = Time.time;\n    }\n\n    void Update()\n    {\n        if (isCalibrating)\n        {\n            if (Time.time - calibrationStartTime >= calibrationDuration)\n            {\n                CompleteCalibration();\n            }\n            else\n            {\n                PerformCalibrationStep();\n            }\n        }\n    }\n\n    void PerformCalibrationStep()\n    {\n        // Perform calibration calculations\n        // Adjust sensor parameters based on calibration data\n    }\n\n    void CompleteCalibration()\n    {\n        isCalibrating = false;\n        Debug.Log("Calibration completed");\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-noise-and-uncertainty",children:"Sensor Noise and Uncertainty"}),"\n",(0,s.jsx)(e.h3,{id:"adding-realistic-noise",children:"Adding Realistic Noise"}),"\n",(0,s.jsx)(e.p,{children:"Real sensors have inherent noise and uncertainty:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class SensorNoise : MonoBehaviour\n{\n    [Header("Noise Parameters")]\n    [SerializeField] private float gaussianNoiseStdDev = 0.01f;\n    [SerializeField] private float bias = 0.0f;\n    [SerializeField] private float driftRate = 0.001f;\n\n    private float currentBias = 0f;\n\n    public float ApplyNoise(float rawValue)\n    {\n        // Add Gaussian noise\n        float gaussianNoise = RandomGaussian() * gaussianNoiseStdDev;\n\n        // Add bias\n        float biasedValue = rawValue + bias + currentBias;\n\n        // Add noise\n        float noisyValue = biasedValue + gaussianNoise;\n\n        // Update drift\n        currentBias += Random.Range(-driftRate, driftRate) * Time.deltaTime;\n\n        return noisyValue;\n    }\n\n    float RandomGaussian()\n    {\n        // Box-Muller transform for Gaussian random numbers\n        float u1 = Random.value;\n        float u2 = Random.value;\n        return Mathf.Sqrt(-2.0f * Mathf.Log(u1)) * Mathf.Cos(2.0f * Mathf.PI * u2);\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use appropriate update rates for each sensor type"}),"\n",(0,s.jsx)(e.li,{children:"Implement level-of-detail for sensor processing"}),"\n",(0,s.jsx)(e.li,{children:"Use occlusion culling for vision sensors"}),"\n",(0,s.jsx)(e.li,{children:"Cache expensive calculations when possible"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"realism-vs-performance",children:"Realism vs. Performance"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Balance physical accuracy with simulation speed"}),"\n",(0,s.jsx)(e.li,{children:"Use simplified models for distant objects"}),"\n",(0,s.jsx)(e.li,{children:"Implement adaptive resolution based on importance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"validation",children:"Validation"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Compare simulation results with real sensor data"}),"\n",(0,s.jsx)(e.li,{children:"Validate noise characteristics match real sensors"}),"\n",(0,s.jsx)(e.li,{children:"Test edge cases and failure modes"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,s.jsx)(e.h3,{id:"sensor-data-quality",children:"Sensor Data Quality"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Check coordinate frame alignment between sensors"}),"\n",(0,s.jsx)(e.li,{children:"Verify proper TF transforms are published"}),"\n",(0,s.jsx)(e.li,{children:"Ensure sensor mounting positions are accurate"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reduce ray count for LIDAR sensors if performance is poor"}),"\n",(0,s.jsx)(e.li,{children:"Use lower resolution cameras for faster processing"}),"\n",(0,s.jsx)(e.li,{children:"Implement sensor frustum culling"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"exercise",children:"Exercise"}),"\n",(0,s.jsx)(e.p,{children:"Create a complete sensor simulation setup for your humanoid robot that includes:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"A RGB camera with realistic noise characteristics"}),"\n",(0,s.jsx)(e.li,{children:"A 2D LIDAR sensor for navigation"}),"\n",(0,s.jsx)(e.li,{children:"An IMU for orientation and motion sensing"}),"\n",(0,s.jsx)(e.li,{children:"A fusion algorithm that combines sensor data for improved accuracy"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Test the sensor setup in various environments and validate that the simulated data matches expected real-world sensor behavior."})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>o});var a=i(6540);const s={},t=a.createContext(s);function r(n){const e=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),a.createElement(t.Provider,{value:e},n.children)}}}]);