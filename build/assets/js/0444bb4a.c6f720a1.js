"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[110],{6580:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-4-vla/whisper-speech","title":"Whisper Speech Processing","description":"OpenAI\'s Whisper model is a state-of-the-art automatic speech recognition (ASR) system that can transcribe speech to text with remarkable accuracy. In this section, we\'ll explore how to integrate Whisper into your humanoid robot\'s communication system.","source":"@site/docs/module-4-vla/whisper-speech.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper-speech","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/whisper-speech","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"VSLAM and Navigation","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/vslam-navigation"}}');var r=i(4848),o=i(8453);const a={},t="Whisper Speech Processing",l={},d=[{value:"Introduction to Whisper",id:"introduction-to-whisper",level:2},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"Whisper Integration with Robotics",id:"whisper-integration-with-robotics",level:2},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Basic Whisper Usage",id:"basic-whisper-usage",level:3},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Whisper with ROS 2 Integration",id:"whisper-with-ros-2-integration",level:2},{value:"Creating a Whisper ROS 2 Node",id:"creating-a-whisper-ros-2-node",level:3},{value:"Launch File for Whisper Node",id:"launch-file-for-whisper-node",level:3},{value:"Advanced Whisper Features",id:"advanced-whisper-features",level:2},{value:"Language Detection and Multilingual Support",id:"language-detection-and-multilingual-support",level:3},{value:"Improved Real-time Processing with VAD (Voice Activity Detection)",id:"improved-real-time-processing-with-vad-voice-activity-detection",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Using Local Whisper Models",id:"using-local-whisper-models",level:3},{value:"Quantization for Better Performance",id:"quantization-for-better-performance",level:3},{value:"Error Handling and Robustness",id:"error-handling-and-robustness",level:2},{value:"Handling Different Audio Formats",id:"handling-different-audio-formats",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. Audio Quality Issues",id:"1-audio-quality-issues",level:3},{value:"2. Performance Issues",id:"2-performance-issues",level:3},{value:"3. Memory Issues",id:"3-memory-issues",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Audio Preprocessing",id:"1-audio-preprocessing",level:3},{value:"2. Model Selection",id:"2-model-selection",level:3},{value:"3. Integration",id:"3-integration",level:3},{value:"Exercise",id:"exercise",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"whisper-speech-processing",children:"Whisper Speech Processing"})}),"\n",(0,r.jsx)(n.p,{children:"OpenAI's Whisper model is a state-of-the-art automatic speech recognition (ASR) system that can transcribe speech to text with remarkable accuracy. In this section, we'll explore how to integrate Whisper into your humanoid robot's communication system."}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-whisper",children:"Introduction to Whisper"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model trained on 680,000 hours of multilingual and multitask supervised data. It demonstrates strong performance in:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converting speech to text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech Translation"}),": Translating speech from one language to another"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language Identification"}),": Determining the spoken language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Activity Detection"}),": Identifying when speech occurs"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,r.jsx)(n.p,{children:"There are several Whisper model variants with different sizes and capabilities:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tiny"}),": Fastest, smallest (39M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"base"}),": Small (74M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"small"}),": Medium (244M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"medium"}),": Large (769M parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"large"}),": Largest, most accurate (1550M parameters)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For robotics applications, the choice depends on:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy requirements"}),": Larger models provide better accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational resources"}),": Smaller models run faster with less memory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency requirements"}),": Real-time applications may need faster models"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"whisper-integration-with-robotics",children:"Whisper Integration with Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,r.jsx)(n.p,{children:"First, install the required dependencies:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\npip install sounddevice  # For audio input\npip install pyaudio      # Alternative audio input\npip install transformers # For LLM integration\n"})}),"\n",(0,r.jsx)(n.h3,{id:"basic-whisper-usage",children:"Basic Whisper Usage"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\n\n# Load the Whisper model\nmodel = whisper.load_model("small")  # Choose tiny, base, small, medium, or large\n\n# Transcribe audio\nresult = model.transcribe("path/to/audio.wav")\nprint(result["text"])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,r.jsx)(n.p,{children:"For real-time speech processing in robotics, we need to capture and process audio streams:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import whisper\nimport numpy as np\nimport sounddevice as sd\nimport queue\nimport threading\nimport time\n\nclass RealTimeWhisper:\n    def __init__(self, model_size="small"):\n        # Load Whisper model\n        self.model = whisper.load_model(model_size)\n\n        # Audio parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.chunk_duration = 1.0  # Process 1-second chunks\n        self.chunk_size = int(self.sample_rate * self.chunk_duration)\n\n        # Audio buffer\n        self.audio_queue = queue.Queue()\n        self.transcript_queue = queue.Queue()\n\n        # Flags\n        self.recording = False\n\n    def audio_callback(self, indata, frames, time, status):\n        """Callback for audio input"""\n        if status:\n            print(status)\n        # Add audio data to queue\n        self.audio_queue.put(indata[:, 0].copy())\n\n    def start_recording(self):\n        """Start recording audio"""\n        self.recording = True\n\n        # Start audio stream\n        self.stream = sd.InputStream(\n            samplerate=self.sample_rate,\n            blocksize=self.chunk_size,\n            channels=1,\n            dtype=\'float32\',\n            callback=self.audio_callback\n        )\n        self.stream.start()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.start()\n\n    def stop_recording(self):\n        """Stop recording audio"""\n        self.recording = False\n        self.stream.stop()\n        self.stream.close()\n\n    def process_audio(self):\n        """Process audio chunks in a separate thread"""\n        audio_buffer = np.array([])\n\n        while self.recording:\n            try:\n                # Get audio chunk\n                chunk = self.audio_queue.get(timeout=0.1)\n\n                # Add to buffer\n                audio_buffer = np.concatenate([audio_buffer, chunk])\n\n                # Process when we have enough audio\n                if len(audio_buffer) >= self.chunk_size:\n                    # Process the audio\n                    transcript = self.transcribe_chunk(audio_buffer)\n\n                    # Add to transcript queue\n                    if transcript.strip():  # Only add non-empty transcripts\n                        self.transcript_queue.put(transcript)\n\n                    # Keep remaining audio in buffer\n                    audio_buffer = audio_buffer[self.chunk_size:]\n\n            except queue.Empty:\n                continue\n\n    def transcribe_chunk(self, audio_chunk):\n        """Transcribe a chunk of audio"""\n        # Convert to tensor\n        audio_tensor = torch.from_numpy(audio_chunk).float()\n\n        # Transcribe\n        result = self.model.transcribe(audio_tensor.numpy())\n\n        return result["text"]\n\n    def get_transcript(self):\n        """Get next transcript from queue"""\n        try:\n            return self.transcript_queue.get_nowait()\n        except queue.Empty:\n            return None\n\n# Usage example\nwhisper_robot = RealTimeWhisper(model_size="small")\nwhisper_robot.start_recording()\n\ntry:\n    while True:\n        transcript = whisper_robot.get_transcript()\n        if transcript:\n            print(f"Robot heard: {transcript}")\n            # Process the transcript for robot actions\n\n        time.sleep(0.1)\nexcept KeyboardInterrupt:\n    whisper_robot.stop_recording()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"whisper-with-ros-2-integration",children:"Whisper with ROS 2 Integration"}),"\n",(0,r.jsx)(n.h3,{id:"creating-a-whisper-ros-2-node",children:"Creating a Whisper ROS 2 Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nfrom io import BytesIO\nimport wave\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Load Whisper model\n        self.model_size = self.declare_parameter('model_size', 'small').get_parameter_value().string_value\n        self.model = whisper.load_model(self.model_size)\n\n        # Subscribe to audio data\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10\n        )\n\n        # Publish transcriptions\n        self.transcript_pub = self.create_publisher(\n            String,\n            'speech_transcription',\n            10\n        )\n\n        self.get_logger().info(f'Whisper node initialized with {self.model_size} model')\n\n    def audio_callback(self, msg):\n        \"\"\"Process incoming audio data\"\"\"\n        try:\n            # Convert audio data to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Transcribe the audio\n            result = self.model.transcribe(audio_data)\n\n            # Publish transcription\n            transcript_msg = String()\n            transcript_msg.data = result[\"text\"]\n            self.transcript_pub.publish(transcript_msg)\n\n            self.get_logger().info(f'Transcribed: {result[\"text\"]}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperNode()\n\n    try:\n        rclpy.spin(whisper_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        whisper_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"launch-file-for-whisper-node",children:"Launch File for Whisper Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"\x3c!-- launch/whisper_node.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='robot_voice_interface',\n            executable='whisper_node',\n            name='whisper_node',\n            parameters=[\n                {'model_size': 'small'}  # Choose tiny, base, small, medium, or large\n            ],\n            remappings=[\n                ('/audio_input', '/microphone/audio_raw'),\n                ('/speech_transcription', '/voice_commands')\n            ]\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-whisper-features",children:"Advanced Whisper Features"}),"\n",(0,r.jsx)(n.h3,{id:"language-detection-and-multilingual-support",children:"Language Detection and Multilingual Support"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def detect_language_and_transcribe(self, audio_data):\n    """Detect language and transcribe accordingly"""\n    # Detect language\n    audio_tensor = torch.from_numpy(audio_data).float()\n    mel = whisper.log_mel_spectrogram(audio_tensor)\n\n    # Detect language\n    _, probs = self.model.detect_language(mel[:1])\n    detected_lang = max(probs, key=probs.get)\n\n    # Transcribe with detected language\n    result = self.model.transcribe(audio_data, language=detected_lang)\n\n    return result["text"], detected_lang\n'})}),"\n",(0,r.jsx)(n.h3,{id:"improved-real-time-processing-with-vad-voice-activity-detection",children:"Improved Real-time Processing with VAD (Voice Activity Detection)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import webrtcvad  # pip install webrtcvad\n\nclass SmartWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'smart_whisper_node\')\n\n        # Load Whisper model\n        self.model = whisper.load_model("small")\n\n        # Voice activity detection\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(1)  # Aggressiveness mode (0-3)\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Speech detection parameters\n        self.speech_buffer = []\n        self.silence_threshold = 50  # frames of silence to trigger processing\n        self.silence_count = 0\n        self.is_speaking = False\n\n        # ROS 2 setup\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'audio_input\',\n            self.smart_audio_callback,\n            10\n        )\n\n        self.transcript_pub = self.create_publisher(String, \'speech_transcription\', 10)\n\n    def smart_audio_callback(self, msg):\n        """Process audio with voice activity detection"""\n        # Convert to 16-bit PCM for VAD\n        audio_int16 = np.frombuffer(msg.data, dtype=np.int16)\n\n        # Process in frames\n        for i in range(0, len(audio_int16), self.frame_size):\n            frame = audio_int16[i:i+self.frame_size]\n\n            # Pad frame if necessary\n            if len(frame) < self.frame_size:\n                frame = np.pad(frame, (0, self.frame_size - len(frame)), \'constant\')\n\n            # Check for voice activity\n            is_speech = self.vad.is_speech(frame.tobytes(), self.sample_rate)\n\n            if is_speech:\n                # Add to speech buffer\n                self.speech_buffer.extend(frame)\n                self.silence_count = 0\n                self.is_speaking = True\n            else:\n                # Add to silence counter\n                self.silence_count += 1\n\n                if self.is_speaking and self.silence_count > self.silence_threshold:\n                    # End of speech detected, process the buffer\n                    self.process_speech_buffer()\n                    self.is_speaking = False\n\n    def process_speech_buffer(self):\n        """Process accumulated speech buffer"""\n        if len(self.speech_buffer) > 0:\n            # Convert to float32\n            audio_float32 = np.array(self.speech_buffer, dtype=np.float32) / 32768.0\n\n            # Transcribe\n            result = self.model.transcribe(audio_float32)\n\n            # Publish if we have a meaningful result\n            if result["text"].strip():\n                transcript_msg = String()\n                transcript_msg.data = result["text"]\n                self.transcript_pub.publish(transcript_msg)\n\n                self.get_logger().info(f\'Speech detected: {result["text"]}\')\n\n            # Clear buffer\n            self.speech_buffer = []\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"using-local-whisper-models",children:"Using Local Whisper Models"}),"\n",(0,r.jsx)(n.p,{children:"For better performance and privacy, use local models:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Download model to local directory\nimport os\nfrom whisper import _download, _MODELS\n\ndef download_whisper_model(model_size, download_root=None):\n    """Download Whisper model to local directory"""\n    if download_root is None:\n        download_root = os.path.expanduser("~/.cache/whisper")\n\n    model_url = _MODELS[model_size]\n    return _download(model_url, download_root, False)\n\n# Use local model\nmodel_path = download_whisper_model("small")\nmodel = whisper.load_model(model_path)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"quantization-for-better-performance",children:"Quantization for Better Performance"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Load quantized model for better performance\nmodel = whisper.load_model("small", device="cuda", in_memory=True)\n\n# Or use CPU with FP16 for better performance\nmodel = whisper.load_model("small", device="cpu", fp16=True)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"error-handling-and-robustness",children:"Error Handling and Robustness"}),"\n",(0,r.jsx)(n.h3,{id:"handling-different-audio-formats",children:"Handling Different Audio Formats"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def process_audio_with_format_handling(self, audio_msg):\n    """Handle different audio formats"""\n    try:\n        # Convert different sample rates to 16kHz\n        audio_data = self.convert_audio_format(audio_msg)\n\n        # Normalize audio\n        audio_data = self.normalize_audio(audio_data)\n\n        # Transcribe\n        result = self.model.transcribe(audio_data)\n\n        return result["text"]\n    except Exception as e:\n        self.get_logger().error(f\'Audio processing error: {str(e)}\')\n        return ""\n\ndef convert_audio_format(self, audio_msg):\n    """Convert audio to required format"""\n    # Convert to numpy array\n    if audio_msg.encoding == \'PCM_16\':\n        audio_np = np.frombuffer(audio_msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n    elif audio_msg.encoding == \'PCM_32\':\n        audio_np = np.frombuffer(audio_msg.data, dtype=np.int32).astype(np.float32) / 2147483648.0\n    else:\n        raise ValueError(f"Unsupported audio encoding: {audio_msg.encoding}")\n\n    # Resample if necessary\n    if audio_msg.rate != 16000:\n        import librosa\n        audio_np = librosa.resample(audio_np, orig_sr=audio_msg.rate, target_sr=16000)\n\n    return audio_np\n\ndef normalize_audio(self, audio_data):\n    """Normalize audio to prevent clipping"""\n    max_val = np.max(np.abs(audio_data))\n    if max_val > 1.0:\n        audio_data = audio_data / max_val\n    return audio_data\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,r.jsx)(n.h3,{id:"1-audio-quality-issues",children:"1. Audio Quality Issues"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Poor transcription accuracy\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use noise reduction preprocessing"}),"\n",(0,r.jsx)(n.li,{children:"Ensure proper microphone positioning"}),"\n",(0,r.jsx)(n.li,{children:"Check audio input levels"}),"\n",(0,r.jsx)(n.li,{children:"Use directional microphones"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-performance-issues",children:"2. Performance Issues"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": Slow processing or high latency\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use smaller Whisper models"}),"\n",(0,r.jsx)(n.li,{children:"Optimize audio chunk sizes"}),"\n",(0,r.jsx)(n.li,{children:"Use GPU acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Implement audio buffering"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-memory-issues",children:"3. Memory Issues"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem"}),": High memory consumption\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use CPU instead of GPU for smaller models"}),"\n",(0,r.jsx)(n.li,{children:"Process audio in smaller chunks"}),"\n",(0,r.jsx)(n.li,{children:"Implement memory cleanup"}),"\n",(0,r.jsx)(n.li,{children:"Use quantized models"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"1-audio-preprocessing",children:"1. Audio Preprocessing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apply noise reduction filters"}),"\n",(0,r.jsx)(n.li,{children:"Normalize audio levels"}),"\n",(0,r.jsx)(n.li,{children:"Use appropriate sample rates"}),"\n",(0,r.jsx)(n.li,{children:"Implement silence detection"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-model-selection",children:"2. Model Selection"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Choose model size based on accuracy requirements"}),"\n",(0,r.jsx)(n.li,{children:"Consider computational constraints"}),"\n",(0,r.jsx)(n.li,{children:"Test with domain-specific audio"}),"\n",(0,r.jsx)(n.li,{children:"Use appropriate languages"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-integration",children:"3. Integration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Implement proper error handling"}),"\n",(0,r.jsx)(n.li,{children:"Use appropriate ROS 2 QoS settings"}),"\n",(0,r.jsx)(n.li,{children:"Implement buffering for smooth operation"}),"\n",(0,r.jsx)(n.li,{children:"Monitor performance metrics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,r.jsx)(n.p,{children:"Create a complete Whisper integration for your humanoid robot that includes:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time audio capture from the robot's microphone"}),"\n",(0,r.jsx)(n.li,{children:"Whisper-based speech-to-text processing"}),"\n",(0,r.jsx)(n.li,{children:"Integration with ROS 2 for message passing"}),"\n",(0,r.jsx)(n.li,{children:"Voice activity detection to reduce processing overhead"}),"\n",(0,r.jsx)(n.li,{children:"Performance optimization for real-time operation"}),"\n",(0,r.jsx)(n.li,{children:"Error handling for various audio conditions"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Test your system with various commands and evaluate the accuracy and response time."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>t});var s=i(6540);const r={},o=s.createContext(r);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);